{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "01b2d54a",
        "outputId": "0663608c-d054-4195-81bd-63a3692d5f48"
      },
      "source": [
        "!pip install face-alignment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: face-alignment in /usr/local/lib/python3.12/dist-packages (1.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from face-alignment) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from face-alignment) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.12/dist-packages (from face-alignment) (1.16.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from face-alignment) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from face-alignment) (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from face-alignment) (4.67.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from face-alignment) (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->face-alignment) (0.43.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->face-alignment) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->face-alignment) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchaudio opencv-python numpy face-alignment moviepy tqdm"
      ],
      "metadata": {
        "id": "wXZvLxCP__cV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1b3813ec-b1e2-4001-e4fa-3ebd7aef0c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: face-alignment in /usr/local/lib/python3.12/dist-packages (1.4.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.12/dist-packages (from face-alignment) (1.16.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from face-alignment) (0.25.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from face-alignment) (0.60.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->face-alignment) (0.43.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "RAVDESS Dataset Preprocessing and PyTorch Dataset\n",
        "Prepares audio-visual emotion data for talking face generation research.\n",
        "\n",
        "Directory Structure:\n",
        "    raw_data/\n",
        "    ├── Actor_01/\n",
        "    │   ├── 01-01-01-01-01-01-01.mp4\n",
        "    │   └── ...\n",
        "    ├── Actor_02/\n",
        "    └── ...\n",
        "\n",
        "Author: Cross-Modal Emotion Synchronization Research\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Union, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from moviepy.editor import VideoFileClip\n",
        "import face_alignment\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class PreprocessingConfig:\n",
        "    \"\"\"Configuration for RAVDESS preprocessing.\"\"\"\n",
        "    target_sample_rate: int = 16000\n",
        "    video_height: int = 224\n",
        "    video_width: int = 224\n",
        "    num_frames_per_clip: int = 32\n",
        "    min_face_detection_confidence: float = 0.5\n",
        "    face_padding_multiplier: float = 1.2\n",
        "    min_face_padding_pixels: int = 40\n",
        "    num_facial_landmarks: int = 68\n",
        "\n",
        "\n",
        "EMOTION_CODES = {\n",
        "    '01': 'neutral',\n",
        "    '02': 'calm',\n",
        "    '03': 'happy',\n",
        "    '04': 'sad',\n",
        "    '05': 'angry',\n",
        "    '06': 'fearful',\n",
        "    '07': 'disgust',\n",
        "    '08': 'surprised'\n",
        "}\n",
        "\n",
        "EMOTION_TO_INDEX = {\n",
        "    'neutral': 0,\n",
        "    'calm': 1,\n",
        "    'happy': 2,\n",
        "    'sad': 3,\n",
        "    'angry': 4,\n",
        "    'fearful': 5,\n",
        "    'disgust': 6,\n",
        "    'surprised': 7\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Utility Functions\n",
        "# ============================================================================\n",
        "\n",
        "def sample_frame_indices(\n",
        "    total_frames: int,\n",
        "    target_num_frames: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Uniformly sample target_num_frames indices from the total available frames.\n",
        "\n",
        "    Args:\n",
        "        total_frames: Total number of frames in the video\n",
        "        target_num_frames: Desired number of frames to sample\n",
        "\n",
        "    Returns:\n",
        "        Array of frame indices to extract\n",
        "    \"\"\"\n",
        "    if total_frames <= 0:\n",
        "        warnings.warn(\"Total frames <= 0, returning zero indices\")\n",
        "        return np.zeros(target_num_frames, dtype=int)\n",
        "\n",
        "    if total_frames <= target_num_frames:\n",
        "        # Pad with last frame if insufficient frames\n",
        "        indices = np.arange(total_frames)\n",
        "        padding = np.full(\n",
        "            target_num_frames - total_frames,\n",
        "            total_frames - 1,\n",
        "            dtype=int\n",
        "        )\n",
        "        return np.concatenate([indices, padding])\n",
        "\n",
        "    # Uniform sampling\n",
        "    return np.linspace(\n",
        "        0,\n",
        "        total_frames - 1,\n",
        "        target_num_frames,\n",
        "        dtype=int\n",
        "    )\n",
        "\n",
        "\n",
        "def ensure_directory_exists(path: Union[str, Path]) -> Path:\n",
        "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
        "    path = Path(path)\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "\n",
        "def validate_video_has_audio(video_path: Path) -> bool:\n",
        "    \"\"\"\n",
        "    Check if video file contains an audio track.\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to video file\n",
        "\n",
        "    Returns:\n",
        "        True if audio track exists, False otherwise\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If video cannot be opened or has no audio\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with VideoFileClip(str(video_path)) as clip:\n",
        "            if clip.audio is None:\n",
        "                raise ValueError(\n",
        "                    f\"Video file has no audio track: {video_path.name}\"\n",
        "                )\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        raise ValueError(\n",
        "            f\"Cannot validate audio in {video_path.name}: {str(e)}\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RAVDESS Filename Parser\n",
        "# ============================================================================\n",
        "\n",
        "class RAVDESSFilenameParser:\n",
        "    \"\"\"\n",
        "    Parse RAVDESS filename format:\n",
        "    Modality-VocalChannel-Emotion-Intensity-Statement-Repetition-Actor.mp4\n",
        "\n",
        "    Example: 03-01-06-01-02-01-12.mp4\n",
        "    \"\"\"\n",
        "\n",
        "    EXPECTED_PARTS = 7\n",
        "    MODALITY_CODES = {\n",
        "        '01': 'audio_video',\n",
        "        '02': 'video_only',\n",
        "        '03': 'audio_only'\n",
        "    }\n",
        "    VOCAL_CHANNEL_CODES = {\n",
        "        '01': 'speech',\n",
        "        '02': 'song'\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def parse(filename: Union[str, Path]) -> Dict:\n",
        "        \"\"\"\n",
        "        Parse RAVDESS filename into structured metadata.\n",
        "\n",
        "        Args:\n",
        "            filename: RAVDESS format filename\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing parsed metadata\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If filename format is invalid\n",
        "        \"\"\"\n",
        "        filename = Path(filename)\n",
        "        parts = filename.stem.split('-')\n",
        "\n",
        "        if len(parts) != RAVDESSFilenameParser.EXPECTED_PARTS:\n",
        "            raise ValueError(\n",
        "                f\"Invalid RAVDESS filename format: {filename.name}. \"\n",
        "                f\"Expected {RAVDESSFilenameParser.EXPECTED_PARTS} parts, \"\n",
        "                f\"got {len(parts)}\"\n",
        "            )\n",
        "\n",
        "        modality_code = parts[0]\n",
        "        vocal_channel_code = parts[1]\n",
        "        emotion_code = parts[2]\n",
        "        intensity = int(parts[3])\n",
        "        statement = int(parts[4])\n",
        "        repetition = int(parts[5])\n",
        "        actor_id = int(parts[6])\n",
        "\n",
        "        # Determine gender (odd = male, even = female in RAVDESS)\n",
        "        gender = 'female' if actor_id % 2 == 0 else 'male'\n",
        "\n",
        "        # Get descriptive names\n",
        "        modality = RAVDESSFilenameParser.MODALITY_CODES.get(\n",
        "            modality_code,\n",
        "            'unknown'\n",
        "        )\n",
        "        vocal_channel = RAVDESSFilenameParser.VOCAL_CHANNEL_CODES.get(\n",
        "            vocal_channel_code,\n",
        "            'unknown'\n",
        "        )\n",
        "        emotion = EMOTION_CODES.get(emotion_code, 'unknown')\n",
        "\n",
        "        return {\n",
        "            'modality': modality,\n",
        "            'modality_code': modality_code,\n",
        "            'vocal_channel': vocal_channel,\n",
        "            'vocal_channel_code': vocal_channel_code,\n",
        "            'emotion': emotion,\n",
        "            'emotion_code': emotion_code,\n",
        "            'intensity': intensity,\n",
        "            'statement': statement,\n",
        "            'repetition': repetition,\n",
        "            'actor_id': actor_id,\n",
        "            'gender': gender,\n",
        "            'filename': filename.name\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Audio Processor\n",
        "# ============================================================================\n",
        "\n",
        "class AudioProcessor:\n",
        "    \"\"\"Extract and preprocess audio from video files.\"\"\"\n",
        "\n",
        "    def __init__(self, target_sample_rate: int = 16000):\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "\n",
        "    def extract_audio_from_video(\n",
        "        self,\n",
        "        video_path: Path,\n",
        "        output_audio_path: Path\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract audio track from video and save as WAV.\n",
        "\n",
        "        Args:\n",
        "            video_path: Path to input video file\n",
        "            output_audio_path: Path to save extracted audio\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with audio metadata (duration, sample_rate, channels)\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If video has no audio track\n",
        "        \"\"\"\n",
        "        # Validate audio exists\n",
        "        validate_video_has_audio(video_path)\n",
        "\n",
        "        # Extract audio using MoviePy\n",
        "        with VideoFileClip(str(video_path)) as video_clip:\n",
        "            temp_audio_path = output_audio_path.with_suffix(\".temp.wav\")\n",
        "\n",
        "            video_clip.audio.write_audiofile(\n",
        "                str(temp_audio_path),\n",
        "                fps=self.target_sample_rate,\n",
        "                nbytes=2,\n",
        "                codec=\"pcm_s16le\",\n",
        "                verbose=False,\n",
        "                logger=None\n",
        "            )\n",
        "\n",
        "        # Load and normalize audio\n",
        "        waveform, sample_rate = torchaudio.load(str(temp_audio_path))\n",
        "\n",
        "        # Resample if needed\n",
        "        if sample_rate != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(\n",
        "                sample_rate,\n",
        "                self.target_sample_rate\n",
        "            )\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Convert to mono\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Save processed audio\n",
        "        torchaudio.save(\n",
        "            str(output_audio_path),\n",
        "            waveform,\n",
        "            self.target_sample_rate\n",
        "        )\n",
        "\n",
        "        # Clean up temp file\n",
        "        temp_audio_path.unlink(missing_ok=True)\n",
        "\n",
        "        return {\n",
        "            'duration_seconds': waveform.shape[1] / self.target_sample_rate,\n",
        "            'sample_rate': self.target_sample_rate,\n",
        "            'num_channels': waveform.shape[0],\n",
        "            'num_samples': waveform.shape[1]\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Video Processor with Face Alignment\n",
        "# ============================================================================\n",
        "\n",
        "class VideoFaceProcessor:\n",
        "    \"\"\"Process video frames with face detection and alignment.\"\"\"\n",
        "\n",
        "    def __init__(self, config: PreprocessingConfig):\n",
        "        self.config = config\n",
        "        self.video_size = (config.video_width, config.video_height)\n",
        "\n",
        "        # Initialize face alignment model\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.face_aligner = face_alignment.FaceAlignment(\n",
        "            face_alignment.LandmarksType.TWO_D,\n",
        "            flip_input=False,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    def _compute_face_crop_region(\n",
        "        self,\n",
        "        frame_shape: Tuple[int, int],\n",
        "        landmarks: np.ndarray\n",
        "    ) -> Tuple[int, int, int, int]:\n",
        "        \"\"\"\n",
        "        Compute face crop region with dynamic padding based on inter-ocular distance.\n",
        "\n",
        "        Args:\n",
        "            frame_shape: (height, width) of the original frame\n",
        "            landmarks: (68, 2) array of facial landmarks\n",
        "\n",
        "        Returns:\n",
        "            (x_min, y_min, x_max, y_max) crop coordinates\n",
        "        \"\"\"\n",
        "        frame_height, frame_width = frame_shape\n",
        "\n",
        "        # Get bounding box from landmarks\n",
        "        x_coords, y_coords = landmarks[:, 0], landmarks[:, 1]\n",
        "        x_min, x_max = int(x_coords.min()), int(x_coords.max())\n",
        "        y_min, y_max = int(y_coords.min()), int(y_coords.max())\n",
        "\n",
        "        # Calculate inter-ocular distance for adaptive padding\n",
        "        left_eye_center = landmarks[36:42].mean(axis=0)  # Left eye landmarks\n",
        "        right_eye_center = landmarks[42:48].mean(axis=0)  # Right eye landmarks\n",
        "        inter_ocular_distance = np.linalg.norm(\n",
        "            left_eye_center - right_eye_center\n",
        "        )\n",
        "\n",
        "        # Dynamic padding: larger faces get more padding\n",
        "        padding = int(max(\n",
        "            self.config.min_face_padding_pixels,\n",
        "            self.config.face_padding_multiplier * inter_ocular_distance\n",
        "        ))\n",
        "\n",
        "        # Apply padding with boundary checks\n",
        "        x_min = max(0, x_min - padding)\n",
        "        y_min = max(0, y_min - padding)\n",
        "        x_max = min(frame_width, x_max + padding)\n",
        "        y_max = min(frame_height, y_max + padding)\n",
        "\n",
        "        return x_min, y_min, x_max, y_max\n",
        "\n",
        "    def _fallback_center_crop(self, frame: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Fallback center crop when face detection fails.\n",
        "\n",
        "        Args:\n",
        "            frame: RGB frame\n",
        "\n",
        "        Returns:\n",
        "            Center-cropped and resized face region\n",
        "        \"\"\"\n",
        "        height, width = frame.shape[:2]\n",
        "        crop_width, crop_height = self.video_size\n",
        "\n",
        "        # Center coordinates\n",
        "        center_x, center_y = width // 2, height // 2\n",
        "        half_crop_width = crop_width // 2\n",
        "        half_crop_height = crop_height // 2\n",
        "\n",
        "        # Calculate crop bounds\n",
        "        x_min = max(0, center_x - half_crop_width)\n",
        "        y_min = max(0, center_y - half_crop_height)\n",
        "        x_max = min(width, center_x + half_crop_width)\n",
        "        y_max = min(height, center_y + half_crop_height)\n",
        "\n",
        "        cropped = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "        if cropped.size == 0:\n",
        "            # Emergency fallback: return resized full frame\n",
        "            cropped = frame\n",
        "\n",
        "        return cv2.resize(cropped, self.video_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    def detect_and_crop_face(\n",
        "        self,\n",
        "        frame_rgb: np.ndarray\n",
        "    ) -> Tuple[np.ndarray, bool]:\n",
        "        \"\"\"\n",
        "        Detect face landmarks and crop face region.\n",
        "\n",
        "        Args:\n",
        "            frame_rgb: RGB frame (H, W, 3)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (cropped_face, detection_success)\n",
        "            - cropped_face: Resized face crop (H, W, 3)\n",
        "            - detection_success: True if face was detected\n",
        "        \"\"\"\n",
        "        # Attempt face detection\n",
        "        detected_landmarks = self.face_aligner.get_landmarks(frame_rgb)\n",
        "\n",
        "        if not detected_landmarks:\n",
        "            # No face detected - use fallback\n",
        "            cropped_face = self._fallback_center_crop(frame_rgb)\n",
        "            return cropped_face, False\n",
        "\n",
        "        # Use first detected face\n",
        "        landmarks = detected_landmarks[0]\n",
        "\n",
        "        if landmarks.shape[0] != self.config.num_facial_landmarks:\n",
        "            warnings.warn(\n",
        "                f\"Expected {self.config.num_facial_landmarks} landmarks, \"\n",
        "                f\"got {landmarks.shape[0]}. Using fallback.\"\n",
        "            )\n",
        "            cropped_face = self._fallback_center_crop(frame_rgb)\n",
        "            return cropped_face, False\n",
        "\n",
        "        # Compute crop region\n",
        "        x_min, y_min, x_max, y_max = self._compute_face_crop_region(\n",
        "            frame_rgb.shape[:2],\n",
        "            landmarks\n",
        "        )\n",
        "\n",
        "        # Crop and resize\n",
        "        face_region = frame_rgb[y_min:y_max, x_min:x_max]\n",
        "\n",
        "        if face_region.size == 0:\n",
        "            cropped_face = self._fallback_center_crop(frame_rgb)\n",
        "            return cropped_face, False\n",
        "\n",
        "        cropped_face = cv2.resize(\n",
        "            face_region,\n",
        "            self.video_size,\n",
        "            interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "\n",
        "        return cropped_face, True\n",
        "\n",
        "    def process_video_to_frames(\n",
        "        self,\n",
        "        video_path: Path,\n",
        "        output_frames_dir: Path\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract, detect faces, and save uniformly sampled frames.\n",
        "\n",
        "        Args:\n",
        "            video_path: Path to input video\n",
        "            output_frames_dir: Directory to save frame arrays\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with processing statistics\n",
        "        \"\"\"\n",
        "        ensure_directory_exists(output_frames_dir)\n",
        "\n",
        "        # Open video\n",
        "        video_capture = cv2.VideoCapture(str(video_path))\n",
        "        if not video_capture.isOpened():\n",
        "            raise ValueError(f\"Cannot open video: {video_path}\")\n",
        "\n",
        "        # Get video properties\n",
        "        total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        original_fps = video_capture.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "\n",
        "        # Determine frames to sample\n",
        "        frame_indices = sample_frame_indices(\n",
        "            total_frames,\n",
        "            self.config.num_frames_per_clip\n",
        "        )\n",
        "\n",
        "        num_successful_detections = 0\n",
        "        frames_saved = 0\n",
        "\n",
        "        for output_index, source_frame_index in enumerate(frame_indices):\n",
        "            # Seek to frame\n",
        "            video_capture.set(cv2.CAP_PROP_POS_FRAMES, int(source_frame_index))\n",
        "            success, frame_bgr = video_capture.read()\n",
        "\n",
        "            if not success:\n",
        "                # Frame read failed - use previous frame or black frame\n",
        "                if output_index > 0:\n",
        "                    previous_frame = np.load(\n",
        "                        output_frames_dir / f\"frame_{output_index-1:05d}.npy\"\n",
        "                    )\n",
        "                    np.save(\n",
        "                        output_frames_dir / f\"frame_{output_index:05d}.npy\",\n",
        "                        previous_frame\n",
        "                    )\n",
        "                else:\n",
        "                    # First frame failed - save black frame\n",
        "                    black_frame = np.zeros(\n",
        "                        (*self.video_size[::-1], 3),\n",
        "                        dtype=np.float32\n",
        "                    )\n",
        "                    np.save(\n",
        "                        output_frames_dir / f\"frame_{output_index:05d}.npy\",\n",
        "                        black_frame\n",
        "                    )\n",
        "                frames_saved += 1\n",
        "                continue\n",
        "\n",
        "            # Convert BGR to RGB\n",
        "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Detect and crop face\n",
        "            face_crop, detection_success = self.detect_and_crop_face(frame_rgb)\n",
        "\n",
        "            if detection_success:\n",
        "                num_successful_detections += 1\n",
        "\n",
        "            # Normalize to [0, 1]\n",
        "            face_normalized = face_crop.astype(np.float32) / 255.0\n",
        "\n",
        "            # Save as numpy array\n",
        "            np.save(\n",
        "                output_frames_dir / f\"frame_{output_index:05d}.npy\",\n",
        "                face_normalized\n",
        "            )\n",
        "            frames_saved += 1\n",
        "\n",
        "        video_capture.release()\n",
        "\n",
        "        return {\n",
        "            'num_frames_saved': frames_saved,\n",
        "            'num_successful_face_detections': num_successful_detections,\n",
        "            'face_detection_rate': (\n",
        "                num_successful_detections / len(frame_indices)\n",
        "                if len(frame_indices) > 0 else 0.0\n",
        "            ),\n",
        "            'original_fps': float(original_fps),\n",
        "            'frames_directory': str(output_frames_dir)\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Preprocessor\n",
        "# ============================================================================\n",
        "\n",
        "class RAVDESSPreprocessor:\n",
        "    \"\"\"\n",
        "    Main preprocessor for RAVDESS dataset.\n",
        "    Extracts audio and video with face alignment.\n",
        "\n",
        "    Expected directory structure:\n",
        "        raw_data/\n",
        "        ├── Actor_01/\n",
        "        │   ├── 01-01-01-01-01-01-01.mp4\n",
        "        │   └── ...\n",
        "        ├── Actor_02/\n",
        "        └── ...\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_root: Union[str, Path],\n",
        "        output_root: Union[str, Path],\n",
        "        config: Optional[PreprocessingConfig] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_root: Root directory containing Actor_XX folders\n",
        "            output_root: Directory to save processed data\n",
        "            config: Preprocessing configuration (uses defaults if None)\n",
        "        \"\"\"\n",
        "        self.dataset_root = Path(dataset_root)\n",
        "        self.output_root = Path(output_root)\n",
        "        self.config = config or PreprocessingConfig()\n",
        "\n",
        "        # Validate dataset structure\n",
        "        self._validate_dataset_structure()\n",
        "\n",
        "        # Initialize processors\n",
        "        self.audio_processor = AudioProcessor(self.config.target_sample_rate)\n",
        "        self.video_processor = VideoFaceProcessor(self.config)\n",
        "\n",
        "        # Create output directories\n",
        "        self.audio_dir = ensure_directory_exists(self.output_root / 'audio')\n",
        "        self.frames_dir = ensure_directory_exists(self.output_root / 'frames')\n",
        "\n",
        "    def _validate_dataset_structure(self):\n",
        "        \"\"\"Validate that the dataset has Actor_XX folders.\"\"\"\n",
        "        actor_folders = list(self.dataset_root.glob('Actor_*'))\n",
        "\n",
        "        if not actor_folders:\n",
        "            raise ValueError(\n",
        "                f\"No Actor_XX folders found in {self.dataset_root}.\\n\"\n",
        "                f\"Expected structure: {self.dataset_root}/Actor_01/, Actor_02/, etc.\"\n",
        "            )\n",
        "\n",
        "        print(f\"Found {len(actor_folders)} actor folders in {self.dataset_root}\")\n",
        "\n",
        "        # Check for videos in actor folders\n",
        "        total_videos = sum(\n",
        "            len(list(actor_folder.glob('*.mp4')))\n",
        "            for actor_folder in actor_folders\n",
        "        )\n",
        "\n",
        "        if total_videos == 0:\n",
        "            raise ValueError(\n",
        "                f\"No .mp4 files found in Actor folders under {self.dataset_root}\"\n",
        "            )\n",
        "\n",
        "        print(f\"Found {total_videos} total video files across all actors\")\n",
        "\n",
        "    def get_all_video_files(self) -> List[Path]:\n",
        "        \"\"\"\n",
        "        Get all video files from Actor folders.\n",
        "\n",
        "        Returns:\n",
        "            List of paths to video files\n",
        "        \"\"\"\n",
        "        video_files = []\n",
        "        actor_folders = sorted(self.dataset_root.glob('Actor_*'))\n",
        "\n",
        "        for actor_folder in actor_folders:\n",
        "            actor_videos = list(actor_folder.glob('*.mp4'))\n",
        "            video_files.extend(actor_videos)\n",
        "\n",
        "        return sorted(video_files)\n",
        "\n",
        "    def process_single_video(\n",
        "        self,\n",
        "        video_path: Path\n",
        "    ) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Process a single RAVDESS video file.\n",
        "\n",
        "        Args:\n",
        "            video_path: Path to video file (e.g., Actor_01/01-01-01-01-01-01-01.mp4)\n",
        "\n",
        "        Returns:\n",
        "            Metadata dictionary or None if processing failed\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Parse filename\n",
        "            file_metadata = RAVDESSFilenameParser.parse(video_path)\n",
        "            sample_id = video_path.stem\n",
        "\n",
        "            # Add actor folder info\n",
        "            actor_folder = video_path.parent.name\n",
        "            file_metadata['actor_folder'] = actor_folder\n",
        "\n",
        "            # Process audio\n",
        "            audio_output_path = self.audio_dir / f\"{sample_id}.wav\"\n",
        "            if not audio_output_path.exists():\n",
        "                audio_metadata = self.audio_processor.extract_audio_from_video(\n",
        "                    video_path,\n",
        "                    audio_output_path\n",
        "                )\n",
        "            else:\n",
        "                # Load existing audio for metadata\n",
        "                waveform, sample_rate = torchaudio.load(str(audio_output_path))\n",
        "                audio_metadata = {\n",
        "                    'duration_seconds': waveform.shape[1] / sample_rate,\n",
        "                    'sample_rate': sample_rate,\n",
        "                    'num_channels': waveform.shape[0],\n",
        "                    'num_samples': waveform.shape[1]\n",
        "                }\n",
        "\n",
        "            # Process video\n",
        "            video_frames_output_dir = self.frames_dir / sample_id\n",
        "            existing_frames = list(video_frames_output_dir.glob('frame_*.npy'))\n",
        "\n",
        "            if len(existing_frames) != self.config.num_frames_per_clip:\n",
        "                video_metadata = self.video_processor.process_video_to_frames(\n",
        "                    video_path,\n",
        "                    video_frames_output_dir\n",
        "                )\n",
        "            else:\n",
        "                # Frames already processed\n",
        "                video_metadata = {\n",
        "                    'num_frames_saved': self.config.num_frames_per_clip,\n",
        "                    'num_successful_face_detections': len(existing_frames),\n",
        "                    'face_detection_rate': 1.0,\n",
        "                    'original_fps': 25.0,\n",
        "                    'frames_directory': str(video_frames_output_dir)\n",
        "                }\n",
        "\n",
        "            # Combine all metadata\n",
        "            complete_metadata = {\n",
        "                'sample_id': sample_id,\n",
        "                'original_video_path': str(video_path),\n",
        "                'audio_path': str(audio_output_path),\n",
        "                'video_frames_directory': video_metadata['frames_directory'],\n",
        "                'num_frames': video_metadata['num_frames_saved'],\n",
        "                'audio_duration_seconds': audio_metadata['duration_seconds'],\n",
        "                'target_num_frames': self.config.num_frames_per_clip,\n",
        "                'video_size': [self.config.video_width, self.config.video_height],\n",
        "                'face_detection_rate': video_metadata['face_detection_rate'],\n",
        "                **file_metadata\n",
        "            }\n",
        "\n",
        "            return complete_metadata\n",
        "\n",
        "        except Exception as error:\n",
        "            warnings.warn(\n",
        "                f\"Failed to process {video_path.name}: {str(error)}\"\n",
        "            )\n",
        "            return None\n",
        "\n",
        "    def process_dataset(\n",
        "        self,\n",
        "        modality_filter: str = 'audio_video',\n",
        "        vocal_channel_filter: str = 'speech'\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Process entire RAVDESS dataset from Actor folders.\n",
        "\n",
        "        Args:\n",
        "            modality_filter: 'audio_video', 'video_only', or 'audio_only'\n",
        "            vocal_channel_filter: 'speech' or 'song'\n",
        "\n",
        "        Returns:\n",
        "            List of metadata dictionaries for all processed samples\n",
        "        \"\"\"\n",
        "        # Get all video files from Actor folders\n",
        "        video_files = self.get_all_video_files()\n",
        "        print(f\"\\nFound {len(video_files)} video files across all actor folders\")\n",
        "\n",
        "        processed_metadata = []\n",
        "        skipped_modality = 0\n",
        "        skipped_vocal = 0\n",
        "        failed = 0\n",
        "\n",
        "        for video_path in tqdm(video_files, desc=\"Processing RAVDESS videos\"):\n",
        "            # Process video\n",
        "            metadata = self.process_single_video(video_path)\n",
        "\n",
        "            if metadata is None:\n",
        "                failed += 1\n",
        "                continue\n",
        "\n",
        "            # Apply modality filter\n",
        "            if metadata['modality'] != modality_filter:\n",
        "                skipped_modality += 1\n",
        "                continue\n",
        "\n",
        "            # Apply vocal channel filter\n",
        "            if metadata['vocal_channel'] != vocal_channel_filter:\n",
        "                skipped_vocal += 1\n",
        "                continue\n",
        "\n",
        "            processed_metadata.append(metadata)\n",
        "\n",
        "        # Save metadata\n",
        "        metadata_output_path = self.output_root / 'metadata.json'\n",
        "        with open(metadata_output_path, 'w', encoding='utf-8') as file:\n",
        "            json.dump(processed_metadata, file, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Processing Summary\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Total videos found:          {len(video_files)}\")\n",
        "        print(f\"Successfully processed:      {len(processed_metadata)}\")\n",
        "        print(f\"Skipped (modality filter):   {skipped_modality}\")\n",
        "        print(f\"Skipped (vocal filter):      {skipped_vocal}\")\n",
        "        print(f\"Failed:                      {failed}\")\n",
        "        print(f\"\\nMetadata saved to: {metadata_output_path}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return processed_metadata\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PyTorch Dataset\n",
        "# ============================================================================\n",
        "\n",
        "class RAVDESSEmotionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for RAVDESS emotion recognition.\n",
        "\n",
        "    Returns batches containing:\n",
        "        - audio: 1D waveform tensor (variable length)\n",
        "        - video: (T, C, H, W) tensor normalized to [0, 1]\n",
        "        - emotion_label: Integer in [0, 7]\n",
        "        - sample_id: String identifier\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_path: Union[str, Path],\n",
        "        modality: str = 'both'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            metadata_path: Path to metadata.json from preprocessor\n",
        "            modality: 'audio', 'video', or 'both'\n",
        "        \"\"\"\n",
        "        self.modality = modality\n",
        "        self.metadata_path = Path(metadata_path)\n",
        "\n",
        "        # Load metadata\n",
        "        with open(self.metadata_path, 'r', encoding='utf-8') as file:\n",
        "            self.samples = json.load(file)\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found in {metadata_path}\")\n",
        "\n",
        "        # Extract configuration from first sample\n",
        "        first_sample = self.samples[0]\n",
        "        self.num_frames = first_sample['target_num_frames']\n",
        "        self.video_height = first_sample['video_size'][1]\n",
        "        self.video_width = first_sample['video_size'][0]\n",
        "\n",
        "        print(\n",
        "            f\"Loaded {len(self.samples)} samples from {metadata_path}\\n\"\n",
        "            f\"Configuration: {self.num_frames} frames, \"\n",
        "            f\"{self.video_width}x{self.video_height} resolution\"\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _load_audio(self, audio_path: str) -> torch.Tensor:\n",
        "        \"\"\"Load audio waveform.\"\"\"\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # Ensure 16kHz mono\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        return waveform.squeeze(0)  # Shape: [num_samples]\n",
        "\n",
        "    def _load_video(self, frames_directory: str) -> torch.Tensor:\n",
        "        \"\"\"Load video frames as (T, C, H, W) tensor.\"\"\"\n",
        "        frames_dir = Path(frames_directory)\n",
        "        frame_files = sorted(frames_dir.glob('frame_*.npy'))\n",
        "\n",
        "        if len(frame_files) == 0:\n",
        "            warnings.warn(f\"No frames found in {frames_directory}\")\n",
        "            return torch.zeros(\n",
        "                (self.num_frames, 3, self.video_height, self.video_width),\n",
        "                dtype=torch.float32\n",
        "            )\n",
        "\n",
        "        # Ensure correct number of frames\n",
        "        if len(frame_files) != self.num_frames:\n",
        "            indices = sample_frame_indices(len(frame_files), self.num_frames)\n",
        "            frame_files = [frame_files[idx] for idx in indices]\n",
        "\n",
        "        # Load frames\n",
        "        loaded_frames = []\n",
        "        for frame_path in frame_files:\n",
        "            frame_array = np.load(frame_path)  # (H, W, C) in [0, 1]\n",
        "\n",
        "            # Validate shape\n",
        "            if frame_array.ndim != 3 or frame_array.shape[2] != 3:\n",
        "                warnings.warn(f\"Malformed frame: {frame_path}\")\n",
        "                frame_array = np.zeros(\n",
        "                    (self.video_height, self.video_width, 3),\n",
        "                    dtype=np.float32\n",
        "                )\n",
        "\n",
        "            loaded_frames.append(frame_array)\n",
        "\n",
        "        # Stack and convert to tensor: (T, H, W, C) -> (T, C, H, W)\n",
        "        frames_array = np.stack(loaded_frames, axis=0)\n",
        "        frames_tensor = torch.from_numpy(frames_array).permute(0, 3, 1, 2)\n",
        "\n",
        "        return frames_tensor.contiguous().float()\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict:\n",
        "        \"\"\"Get a single sample.\"\"\"\n",
        "        sample = self.samples[index]\n",
        "\n",
        "        output = {\n",
        "            'sample_id': sample['sample_id'],\n",
        "            'emotion_label': EMOTION_TO_INDEX[sample['emotion']]\n",
        "        }\n",
        "\n",
        "        # Load audio if requested\n",
        "        if self.modality in ('audio', 'both'):\n",
        "            output['audio'] = self._load_audio(sample['audio_path'])\n",
        "\n",
        "        # Load video if requested\n",
        "        if self.modality in ('video', 'both'):\n",
        "            output['video'] = self._load_video(sample['video_frames_directory'])\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def collate_batch(batch: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader.\n",
        "\n",
        "    Videos are stacked into (B, T, C, H, W).\n",
        "    Audio remains as list due to variable lengths.\n",
        "    \"\"\"\n",
        "    sample_ids = [item['sample_id'] for item in batch]\n",
        "    emotion_labels = torch.tensor(\n",
        "        [item['emotion_label'] for item in batch],\n",
        "        dtype=torch.long\n",
        "    )\n",
        "\n",
        "    collated = {\n",
        "        'sample_id': sample_ids,\n",
        "        'emotion_label': emotion_labels\n",
        "    }\n",
        "\n",
        "    # Collate audio (keep as list - processors will pad later)\n",
        "    if 'audio' in batch[0]:\n",
        "        collated['audio'] = [item['audio'] for item in batch]\n",
        "\n",
        "    # Collate video (stack into batch)\n",
        "    if 'video' in batch[0]:\n",
        "        videos = [item['video'] for item in batch]\n",
        "        collated['video'] = torch.stack(videos, dim=0)  # (B, T, C, H, W)\n",
        "\n",
        "    return collated\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Validation Functions\n",
        "# ============================================================================\n",
        "\n",
        "def validate_processed_dataset(output_root: Path):\n",
        "    \"\"\"Run comprehensive sanity checks on processed data.\"\"\"\n",
        "    metadata_path = output_root / 'metadata.json'\n",
        "\n",
        "    if not metadata_path.exists():\n",
        "        raise FileNotFoundError(f\"Metadata not found: {metadata_path}\")\n",
        "\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        samples = json.load(f)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Dataset Validation Report\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Basic statistics\n",
        "    print(f\"\\nTotal samples: {len(samples)}\")\n",
        "\n",
        "    # Actor distribution\n",
        "    actor_counts = {}\n",
        "    for sample in samples:\n",
        "        actor_id = sample['actor_id']\n",
        "        actor_counts[actor_id] = actor_counts.get(actor_id, 0) + 1\n",
        "\n",
        "    print(f\"\\nActors represented: {len(actor_counts)}\")\n",
        "    print(f\"Samples per actor: {len(samples) / len(actor_counts):.1f} average\")\n",
        "\n",
        "    # Gender distribution\n",
        "    gender_counts = {}\n",
        "    for sample in samples:\n",
        "        gender = sample['gender']\n",
        "        gender_counts[gender] = gender_counts.get(gender, 0) + 1\n",
        "\n",
        "    print(\"\\nGender Distribution:\")\n",
        "    for gender, count in sorted(gender_counts.items()):\n",
        "        print(f\"  {gender}: {count} ({count/len(samples)*100:.1f}%)\")\n",
        "\n",
        "    # Emotion distribution\n",
        "    emotion_counts = {}\n",
        "    for sample in samples:\n",
        "        emotion = sample['emotion']\n",
        "        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
        "\n",
        "    print(\"\\nEmotion Distribution:\")\n",
        "    for emotion, count in sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {emotion:12s}: {count:3d} ({count/len(samples)*100:.1f}%)\")\n",
        "\n",
        "    # Intensity distribution\n",
        "    intensity_counts = {}\n",
        "    for sample in samples:\n",
        "        intensity = sample['intensity']\n",
        "        intensity_counts[intensity] = intensity_counts.get(intensity, 0) + 1\n",
        "\n",
        "    print(\"\\nIntensity Distribution:\")\n",
        "    for intensity, count in sorted(intensity_counts.items()):\n",
        "        intensity_name = \"normal\" if intensity == 1 else \"strong\"\n",
        "        print(f\"  {intensity_name}: {count} ({count/len(samples)*100:.1f}%)\")\n",
        "\n",
        "    # Face detection rates\n",
        "    detection_rates = [s['face_detection_rate'] for s in samples]\n",
        "    avg_rate = sum(detection_rates) / len(detection_rates)\n",
        "    min_rate = min(detection_rates)\n",
        "    max_rate = max(detection_rates)\n",
        "\n",
        "    print(f\"\\nFace Detection Statistics:\")\n",
        "    print(f\"  Average: {avg_rate:.2%}\")\n",
        "    print(f\"  Min: {min_rate:.2%}\")\n",
        "    print(f\"  Max: {max_rate:.2%}\")\n",
        "\n",
        "    # Find problematic samples\n",
        "    low_detection = [s for s in samples if s['face_detection_rate'] < 0.5]\n",
        "    if low_detection:\n",
        "        print(f\"\\n⚠️  {len(low_detection)} samples with <50% face detection:\")\n",
        "        for s in low_detection[:5]:\n",
        "            print(f\"  - {s['sample_id']} (Actor {s['actor_id']}): {s['face_detection_rate']:.2%}\")\n",
        "        if len(low_detection) > 5:\n",
        "            print(f\"  ... and {len(low_detection) - 5} more\")\n",
        "    else:\n",
        "        print(\"\\n✓ All samples have ≥50% face detection rate\")\n",
        "\n",
        "    # Audio duration statistics\n",
        "    durations = [s['audio_duration_seconds'] for s in samples]\n",
        "    avg_duration = sum(durations) / len(durations)\n",
        "    min_duration = min(durations)\n",
        "    max_duration = max(durations)\n",
        "\n",
        "    print(f\"\\nAudio Duration Statistics:\")\n",
        "    print(f\"  Average: {avg_duration:.2f}s\")\n",
        "    print(f\"  Min: {min_duration:.2f}s\")\n",
        "    print(f\"  Max: {max_duration:.2f}s\")\n",
        "\n",
        "    # Verify files exist\n",
        "    print(\"\\nVerifying file existence...\")\n",
        "    missing_audio = 0\n",
        "    missing_frames = 0\n",
        "\n",
        "    for sample in samples:\n",
        "        if not Path(sample['audio_path']).exists():\n",
        "            missing_audio += 1\n",
        "        if not Path(sample['video_frames_directory']).exists():\n",
        "            missing_frames += 1\n",
        "\n",
        "    if missing_audio == 0 and missing_frames == 0:\n",
        "        print(\"✓ All audio and frame files exist\")\n",
        "    else:\n",
        "        if missing_audio > 0:\n",
        "            print(f\"⚠️  {missing_audio} audio files missing\")\n",
        "        if missing_frames > 0:\n",
        "            print(f\"⚠️  {missing_frames} frame directories missing\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Example Usage\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    DATASET_ROOT = Path(\"/content/raw_data\")  # Contains Actor_01, Actor_02, etc.\n",
        "    OUTPUT_ROOT = Path(\"/content/processed_data\")\n",
        "\n",
        "    # Step 1: Preprocess dataset\n",
        "    print(\"=\" * 80)\n",
        "    print(\"RAVDESS Dataset Preprocessing Pipeline\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    preprocessor = RAVDESSPreprocessor(\n",
        "        dataset_root=DATASET_ROOT,\n",
        "        output_root=OUTPUT_ROOT,\n",
        "        config=PreprocessingConfig(\n",
        "            target_sample_rate=16000,\n",
        "            video_height=224,\n",
        "            video_width=224,\n",
        "            num_frames_per_clip=32\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Process only audio-video speech samples (recommended for your research)\n",
        "    metadata = preprocessor.process_dataset(\n",
        "        modality_filter='audio_video',  # Only AV files\n",
        "        vocal_channel_filter='speech'    # Only speech (not song)\n",
        "    )\n",
        "\n",
        "    # Step 2: Validate processed data\n",
        "    validate_processed_dataset(OUTPUT_ROOT)\n",
        "\n",
        "    # Step 3: Create PyTorch Dataset\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Creating PyTorch Dataset\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    dataset = RAVDESSEmotionDataset(\n",
        "        metadata_path=OUTPUT_ROOT / 'metadata.json',\n",
        "        modality='both'\n",
        "    )\n",
        "\n",
        "    # Inspect single sample\n",
        "    sample = dataset[0]\n",
        "    print(f\"\\nSample ID: {sample['sample_id']}\")\n",
        "    print(f\"Emotion Label: {sample['emotion_label']}\")\n",
        "    print(f\"Audio Shape: {sample['audio'].shape}\")\n",
        "    print(f\"Video Shape: {sample['video'].shape}\")\n",
        "\n",
        "    # Step 4: Create DataLoader\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Testing DataLoader\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=4,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_batch,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    batch = next(iter(dataloader))\n",
        "    print(f\"\\nBatch Size: {len(batch['sample_id'])}\")\n",
        "    print(f\"Emotion Labels: {batch['emotion_label']}\")\n",
        "    print(f\"Video Shape: {batch['video'].shape}\")  # (B, T, C, H, W)\n",
        "    print(f\"Audio: {len(batch['audio'])} waveforms (variable length)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"✓ Preprocessing and Dataset Creation Complete!\")\n",
        "    print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2JOnCtwFOA9",
        "outputId": "b41bec6a-11ed-4c35-e005-7c6ffc6aa493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "RAVDESS Dataset Preprocessing Pipeline\n",
            "================================================================================\n",
            "Found 6 actor folders in /content/raw_data\n",
            "Found 360 total video files across all actors\n",
            "\n",
            "Found 360 video files across all actor folders\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing RAVDESS videos:   0%|          | 0/360 [00:00<?, ?it/s]WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "\n",
            "Processing RAVDESS videos:  14%|█▍        | 51/360 [00:38<05:36,  1.09s/it] WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:247: UserWarning: torio.io._streaming_media_encoder.StreamingMediaEncoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n",
            "\n",
            "Processing RAVDESS videos: 100%|██████████| 360/360 [2:43:51<00:00, 27.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Processing Summary\n",
            "================================================================================\n",
            "Total videos found:          360\n",
            "Successfully processed:      360\n",
            "Skipped (modality filter):   0\n",
            "Skipped (vocal filter):      0\n",
            "Failed:                      0\n",
            "\n",
            "Metadata saved to: /content/processed_data/metadata.json\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Dataset Validation Report\n",
            "================================================================================\n",
            "\n",
            "Total samples: 360\n",
            "\n",
            "Actors represented: 6\n",
            "Samples per actor: 60.0 average\n",
            "\n",
            "Gender Distribution:\n",
            "  female: 180 (50.0%)\n",
            "  male: 180 (50.0%)\n",
            "\n",
            "Emotion Distribution:\n",
            "  calm        :  48 (13.3%)\n",
            "  happy       :  48 (13.3%)\n",
            "  sad         :  48 (13.3%)\n",
            "  angry       :  48 (13.3%)\n",
            "  fearful     :  48 (13.3%)\n",
            "  disgust     :  48 (13.3%)\n",
            "  surprised   :  48 (13.3%)\n",
            "  neutral     :  24 (6.7%)\n",
            "\n",
            "Intensity Distribution:\n",
            "  normal: 192 (53.3%)\n",
            "  strong: 168 (46.7%)\n",
            "\n",
            "Face Detection Statistics:\n",
            "  Average: 99.91%\n",
            "  Min: 96.88%\n",
            "  Max: 100.00%\n",
            "\n",
            "✓ All samples have ≥50% face detection rate\n",
            "\n",
            "Audio Duration Statistics:\n",
            "  Average: 3.80s\n",
            "  Min: 3.07s\n",
            "  Max: 5.27s\n",
            "\n",
            "Verifying file existence...\n",
            "✓ All audio and frame files exist\n",
            "\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Creating PyTorch Dataset\n",
            "================================================================================\n",
            "Loaded 360 samples from /content/processed_data/metadata.json\n",
            "Configuration: 32 frames, 224x224 resolution\n",
            "\n",
            "Sample ID: 01-01-01-01-01-01-19\n",
            "Emotion Label: 0\n",
            "Audio Shape: torch.Size([53920])\n",
            "Video Shape: torch.Size([32, 3, 224, 224])\n",
            "\n",
            "================================================================================\n",
            "Testing DataLoader\n",
            "================================================================================\n",
            "\n",
            "Batch Size: 4\n",
            "Emotion Labels: tensor([3, 6, 6, 5])\n",
            "Video Shape: torch.Size([4, 32, 3, 224, 224])\n",
            "Audio: 4 waveforms (variable length)\n",
            "\n",
            "================================================================================\n",
            "✓ Preprocessing and Dataset Creation Complete!\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}