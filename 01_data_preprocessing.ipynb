{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "01b2d54a"
      },
      "source": [
        "!pip install -q face-alignment torchaudio"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wXZvLxCP__cV"
      },
      "source": [
        "import json\n",
        "import subprocess\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import face_alignment\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    raw_dir: Path = Path(\"/content/raw_data\")\n",
        "    out_dir: Path = Path(\"/content/processed_data\")\n",
        "    sample_rate: int = 16000\n",
        "    frame_size: int = 224\n",
        "    n_frames: int = 32\n",
        "    face_pad_factor: float = 1.2\n",
        "    min_face_pad: int = 40\n",
        "    val_ratio: float = 0.15\n",
        "    test_ratio: float = 0.15\n",
        "    split_seed: int = 42\n",
        "\n",
        "\n",
        "EMOTIONS = {\n",
        "    \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
        "    \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\",\n",
        "}\n",
        "EMOTION_TO_IDX = {v: i for i, v in enumerate(EMOTIONS.values())}\n",
        "\n",
        "cfg = Config()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2JOnCtwFOA9",
        "outputId": "b6b56b20-c7ef-4a75-dfb7-3bf940b66f4d"
      },
      "source": [
        "def parse_ravdess(path: Path) -> dict:\n",
        "    \"\"\"Modality-Channel-Emotion-Intensity-Statement-Repetition-Actor.mp4\"\"\"\n",
        "    parts = path.stem.split(\"-\")\n",
        "    if len(parts) != 7:\n",
        "        return None\n",
        "    actor_id = int(parts[6])\n",
        "    emotion = EMOTIONS.get(parts[2], \"unknown\")\n",
        "    return {\n",
        "        \"video_path\": str(path),\n",
        "        \"sample_id\": path.stem,\n",
        "        \"modality\": parts[0],\n",
        "        \"channel\": parts[1],\n",
        "        \"emotion\": emotion,\n",
        "        \"emotion_idx\": EMOTION_TO_IDX.get(emotion, -1),\n",
        "        \"intensity\": int(parts[3]),\n",
        "        \"actor_id\": actor_id,\n",
        "        \"gender\": \"female\" if actor_id % 2 == 0 else \"male\",\n",
        "    }\n",
        "\n",
        "\n",
        "def collect_av_speech_samples(raw_dir: Path) -> list[dict]:\n",
        "    samples = []\n",
        "    for path in sorted(raw_dir.glob(\"Actor_*/*.mp4\")):\n",
        "        meta = parse_ravdess(path)\n",
        "        if meta and meta[\"modality\"] == \"01\" and meta[\"channel\"] == \"01\":\n",
        "            samples.append(meta)\n",
        "    return samples\n",
        "\n",
        "\n",
        "def assign_splits(samples: list[dict], val_ratio: float, test_ratio: float, seed: int):\n",
        "    actors = sorted({s[\"actor_id\"] for s in samples})\n",
        "    rng = np.random.RandomState(seed)\n",
        "    shuffled = list(actors)\n",
        "    rng.shuffle(shuffled)\n",
        "    n_test = max(1, round(len(shuffled) * test_ratio))\n",
        "    n_val = max(1, round(len(shuffled) * val_ratio))\n",
        "    test_set = set(shuffled[:n_test])\n",
        "    val_set = set(shuffled[n_test:n_test + n_val])\n",
        "    for s in samples:\n",
        "        if s[\"actor_id\"] in test_set:\n",
        "            s[\"split\"] = \"test\"\n",
        "        elif s[\"actor_id\"] in val_set:\n",
        "            s[\"split\"] = \"val\"\n",
        "        else:\n",
        "            s[\"split\"] = \"train\"\n",
        "\n",
        "\n",
        "samples = collect_av_speech_samples(cfg.raw_dir)\n",
        "assign_splits(samples, cfg.val_ratio, cfg.test_ratio, cfg.split_seed)\n",
        "\n",
        "print(f\"Total: {len(samples)} AV speech samples\")\n",
        "for split in (\"train\", \"val\", \"test\"):\n",
        "    n = sum(1 for s in samples if s[\"split\"] == split)\n",
        "    print(f\"  {split}: {n}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: 1380 AV speech samples\n",
            "  train: 1020\n",
            "  val: 180\n",
            "  test: 180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9c0GnGRsC6K",
        "outputId": "d8538f98-2c42-4d99-874e-a4afea049fb5"
      },
      "source": [
        "def extract_audio(video_path: str, out_path: Path, sr: int) -> float:\n",
        "    subprocess.run(\n",
        "        [\"ffmpeg\", \"-y\", \"-loglevel\", \"error\", \"-i\", video_path,\n",
        "         \"-ar\", str(sr), \"-ac\", \"1\", str(out_path)],\n",
        "        check=True,\n",
        "    )\n",
        "    info = torchaudio.info(str(out_path))\n",
        "    return info.num_frames / info.sample_rate\n",
        "\n",
        "\n",
        "audio_dir = cfg.out_dir / \"audio\"\n",
        "audio_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for s in tqdm(samples, desc=\"Extracting audio\"):\n",
        "    wav_path = audio_dir / f\"{s['sample_id']}.wav\"\n",
        "    s[\"audio_path\"] = str(wav_path)\n",
        "    if wav_path.exists():\n",
        "        info = torchaudio.info(str(wav_path))\n",
        "        s[\"duration\"] = info.num_frames / info.sample_rate\n",
        "        continue\n",
        "    try:\n",
        "        s[\"duration\"] = extract_audio(s[\"video_path\"], wav_path, cfg.sample_rate)\n",
        "    except Exception as e:\n",
        "        print(f\"FAILED {s['sample_id']}: {e}\")\n",
        "        s[\"duration\"] = 0.0"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting audio: 100%|██████████| 1380/1380 [03:02<00:00,  7.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG7Iy2VNsC6K",
        "outputId": "c6d6f6fb-b375-4eea-85e1-6b83aa16ead0"
      },
      "source": [
        "class FaceProcessor:\n",
        "    def __init__(self, size: int, pad_factor: float, min_pad: int):\n",
        "        self.size = size\n",
        "        self.pad_factor = pad_factor\n",
        "        self.min_pad = min_pad\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.fa = face_alignment.FaceAlignment(\n",
        "            face_alignment.LandmarksType.TWO_D, flip_input=False, device=device\n",
        "        )\n",
        "\n",
        "    def crop_face(self, frame: np.ndarray) -> tuple[np.ndarray, bool]:\n",
        "        landmarks = self.fa.get_landmarks(frame)\n",
        "        if not landmarks or landmarks[0].shape[0] != 68:\n",
        "            return self._center_crop(frame), False\n",
        "        lm = landmarks[0]\n",
        "        h, w = frame.shape[:2]\n",
        "        iod = np.linalg.norm(lm[36:42].mean(0) - lm[42:48].mean(0))\n",
        "        pad = int(max(self.min_pad, self.pad_factor * iod))\n",
        "        x0 = max(0, int(lm[:, 0].min()) - pad)\n",
        "        y0 = max(0, int(lm[:, 1].min()) - pad)\n",
        "        x1 = min(w, int(lm[:, 0].max()) + pad)\n",
        "        y1 = min(h, int(lm[:, 1].max()) + pad)\n",
        "        crop = frame[y0:y1, x0:x1]\n",
        "        if crop.size == 0:\n",
        "            return self._center_crop(frame), False\n",
        "        return cv2.resize(crop, (self.size, self.size)), True\n",
        "\n",
        "    def _center_crop(self, frame: np.ndarray) -> np.ndarray:\n",
        "        h, w = frame.shape[:2]\n",
        "        s = min(h, w)\n",
        "        y0, x0 = (h - s) // 2, (w - s) // 2\n",
        "        return cv2.resize(frame[y0:y0 + s, x0:x0 + s], (self.size, self.size))\n",
        "\n",
        "\n",
        "def sample_indices(total: int, n: int) -> np.ndarray:\n",
        "    if total <= 0:\n",
        "        return np.zeros(n, dtype=int)\n",
        "    if total <= n:\n",
        "        return np.pad(np.arange(total), (0, n - total), mode=\"edge\")\n",
        "    return np.linspace(0, total - 1, n, dtype=int)\n",
        "\n",
        "\n",
        "def process_video(path: str, fp: FaceProcessor, n_frames: int) -> tuple[np.ndarray, float]:\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    indices = sample_indices(total, n_frames)\n",
        "    frames, hits = [], 0\n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
        "        ok, bgr = cap.read()\n",
        "        if not ok:\n",
        "            frames.append(np.zeros((fp.size, fp.size, 3), dtype=np.uint8))\n",
        "            continue\n",
        "        face, detected = fp.crop_face(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))\n",
        "        hits += int(detected)\n",
        "        frames.append(face.astype(np.uint8))\n",
        "    cap.release()\n",
        "    return np.stack(frames), hits / max(len(indices), 1)\n",
        "\n",
        "\n",
        "fp = FaceProcessor(cfg.frame_size, cfg.face_pad_factor, cfg.min_face_pad)\n",
        "frames_dir = cfg.out_dir / \"frames\"\n",
        "frames_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for s in tqdm(samples, desc=\"Processing video\"):\n",
        "    npy_path = frames_dir / f\"{s['sample_id']}.npy\"\n",
        "    s[\"frames_path\"] = str(npy_path)\n",
        "    if npy_path.exists():\n",
        "        s[\"face_det_rate\"] = -1.0\n",
        "        continue\n",
        "    try:\n",
        "        frames, det_rate = process_video(s[\"video_path\"], fp, cfg.n_frames)\n",
        "        np.save(npy_path, frames)\n",
        "        s[\"face_det_rate\"] = det_rate\n",
        "    except Exception as e:\n",
        "        print(f\"FAILED {s['sample_id']}: {e}\")\n",
        "        s[\"face_det_rate\"] = 0.0"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing video: 100%|██████████| 1380/1380 [1:03:53<00:00,  2.78s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHNTr92dsC6L",
        "outputId": "a951a9f4-370d-44fa-990d-fdb950cc320e"
      },
      "source": [
        "meta_path = cfg.out_dir / \"metadata.json\"\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(samples, f, indent=2)\n",
        "print(f\"Saved {len(samples)} samples -> {meta_path}\\n\")\n",
        "\n",
        "for split in (\"train\", \"val\", \"test\"):\n",
        "    sub = [s for s in samples if s[\"split\"] == split]\n",
        "    print(f\"{split} ({len(sub)} samples)\")\n",
        "    for emo, cnt in sorted(Counter(s[\"emotion\"] for s in sub).items()):\n",
        "        print(f\"  {emo:12s} {cnt}\")\n",
        "    dets = [s[\"face_det_rate\"] for s in sub if s[\"face_det_rate\"] >= 0]\n",
        "    durs = [s.get(\"duration\", 0) for s in sub]\n",
        "    if dets:\n",
        "        print(f\"  face_det:    {np.mean(dets):.1%}\")\n",
        "    print(f\"  duration:    {np.mean(durs):.1f}s avg\\n\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 1380 samples -> /content/processed_data/metadata.json\n",
            "\n",
            "train (1020 samples)\n",
            "  angry        136\n",
            "  calm         136\n",
            "  disgust      136\n",
            "  fearful      136\n",
            "  happy        136\n",
            "  neutral      68\n",
            "  sad          136\n",
            "  surprised    136\n",
            "  face_det:    98.9%\n",
            "  duration:    3.7s avg\n",
            "\n",
            "val (180 samples)\n",
            "  angry        24\n",
            "  calm         24\n",
            "  disgust      24\n",
            "  fearful      24\n",
            "  happy        24\n",
            "  neutral      12\n",
            "  sad          24\n",
            "  surprised    24\n",
            "  face_det:    99.0%\n",
            "  duration:    3.5s avg\n",
            "\n",
            "test (180 samples)\n",
            "  angry        24\n",
            "  calm         24\n",
            "  disgust      24\n",
            "  fearful      24\n",
            "  happy        24\n",
            "  neutral      12\n",
            "  sad          24\n",
            "  surprised    24\n",
            "  face_det:    99.0%\n",
            "  duration:    3.8s avg\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp40nUInsC6L"
      },
      "source": [
        "class RAVDESSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, metadata_path: str, split: str = \"train\", modality: str = \"both\"):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data if s[\"split\"] == split]\n",
        "        self.modality = modality\n",
        "        if not self.samples:\n",
        "            raise ValueError(f\"No samples for split='{split}'\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        item = {\"sample_id\": s[\"sample_id\"], \"emotion\": s[\"emotion_idx\"]}\n",
        "\n",
        "        if self.modality in (\"audio\", \"both\"):\n",
        "            wav, _ = torchaudio.load(s[\"audio_path\"])\n",
        "            item[\"audio\"] = wav.squeeze(0)\n",
        "\n",
        "        if self.modality in (\"video\", \"both\"):\n",
        "            frames = np.load(s[\"frames_path\"])  # (T, H, W, 3) uint8\n",
        "            item[\"video\"] = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    out = {\n",
        "        \"sample_id\": [b[\"sample_id\"] for b in batch],\n",
        "        \"emotion\": torch.tensor([b[\"emotion\"] for b in batch]),\n",
        "    }\n",
        "    if \"audio\" in batch[0]:\n",
        "        out[\"audio\"] = [b[\"audio\"] for b in batch]\n",
        "    if \"video\" in batch[0]:\n",
        "        out[\"video\"] = torch.stack([b[\"video\"] for b in batch])\n",
        "    return out"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN14Hsg4sC6L",
        "outputId": "68fdc8c6-b3fe-4a6f-a94d-1d95eba4d58a"
      },
      "source": [
        "ds = RAVDESSDataset(str(meta_path), split=\"train\", modality=\"both\")\n",
        "print(f\"Train: {len(ds)} samples\\n\")\n",
        "\n",
        "sample = ds[0]\n",
        "print(f\"sample_id: {sample['sample_id']}\")\n",
        "print(f\"emotion:   {sample['emotion']}\")\n",
        "print(f\"audio:     {sample['audio'].shape}\")\n",
        "print(f\"video:     {sample['video'].shape}\")\n",
        "\n",
        "loader = torch.utils.data.DataLoader(ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "batch = next(iter(loader))\n",
        "print(f\"\\nBatch: {len(batch['audio'])} audio, video {batch['video'].shape}, emotions {batch['emotion']}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1020 samples\n",
            "\n",
            "sample_id: 01-01-01-01-01-01-02\n",
            "emotion:   0\n",
            "audio:     torch.Size([58368])\n",
            "video:     torch.Size([32, 3, 224, 224])\n",
            "\n",
            "Batch: 4 audio, video torch.Size([4, 32, 3, 224, 224]), emotions tensor([6, 7, 4, 7])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}