{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "01b2d54a",
        "outputId": "0663608c-d054-4195-81bd-63a3692d5f48"
      },
      "source": [
        "!pip install -q face-alignment torchaudio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: face-alignment in /usr/local/lib/python3.12/dist-packages (1.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from face-alignment) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from face-alignment) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.12/dist-packages (from face-alignment) (1.16.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from face-alignment) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from face-alignment) (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from face-alignment) (4.67.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from face-alignment) (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->face-alignment) (0.43.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->face-alignment) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->face-alignment) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->face-alignment) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wXZvLxCP__cV",
        "outputId": "1b3813ec-b1e2-4001-e4fa-3ebd7aef0c0b"
      },
      "source": [
        "import json\n",
        "import subprocess\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import face_alignment\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    raw_dir: Path = Path(\"/content/raw_data\")\n",
        "    out_dir: Path = Path(\"/content/processed_data\")\n",
        "    sample_rate: int = 16000\n",
        "    frame_size: int = 224\n",
        "    n_frames: int = 32\n",
        "    face_pad_factor: float = 1.2\n",
        "    min_face_pad: int = 40\n",
        "    val_ratio: float = 0.15\n",
        "    test_ratio: float = 0.15\n",
        "    split_seed: int = 42\n",
        "\n",
        "\n",
        "EMOTIONS = {\n",
        "    \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
        "    \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\",\n",
        "}\n",
        "EMOTION_TO_IDX = {v: i for i, v in enumerate(EMOTIONS.values())}\n",
        "\n",
        "cfg = Config()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: face-alignment in /usr/local/lib/python3.12/dist-packages (1.4.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.12/dist-packages (from face-alignment) (1.16.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from face-alignment) (0.25.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from face-alignment) (0.60.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->face-alignment) (0.43.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face-alignment) (0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2JOnCtwFOA9",
        "outputId": "b41bec6a-11ed-4c35-e005-7c6ffc6aa493"
      },
      "source": [
        "def parse_ravdess(path: Path) -> dict:\n",
        "    \"\"\"Modality-Channel-Emotion-Intensity-Statement-Repetition-Actor.mp4\"\"\"\n",
        "    parts = path.stem.split(\"-\")\n",
        "    if len(parts) != 7:\n",
        "        return None\n",
        "    actor_id = int(parts[6])\n",
        "    emotion = EMOTIONS.get(parts[2], \"unknown\")\n",
        "    return {\n",
        "        \"video_path\": str(path),\n",
        "        \"sample_id\": path.stem,\n",
        "        \"modality\": parts[0],\n",
        "        \"channel\": parts[1],\n",
        "        \"emotion\": emotion,\n",
        "        \"emotion_idx\": EMOTION_TO_IDX.get(emotion, -1),\n",
        "        \"intensity\": int(parts[3]),\n",
        "        \"actor_id\": actor_id,\n",
        "        \"gender\": \"female\" if actor_id % 2 == 0 else \"male\",\n",
        "    }\n",
        "\n",
        "\n",
        "def collect_av_speech_samples(raw_dir: Path) -> list[dict]:\n",
        "    samples = []\n",
        "    for path in sorted(raw_dir.glob(\"Actor_*/*.mp4\")):\n",
        "        meta = parse_ravdess(path)\n",
        "        if meta and meta[\"modality\"] == \"01\" and meta[\"channel\"] == \"01\":\n",
        "            samples.append(meta)\n",
        "    return samples\n",
        "\n",
        "\n",
        "def assign_splits(samples: list[dict], val_ratio: float, test_ratio: float, seed: int):\n",
        "    actors = sorted({s[\"actor_id\"] for s in samples})\n",
        "    rng = np.random.RandomState(seed)\n",
        "    shuffled = list(actors)\n",
        "    rng.shuffle(shuffled)\n",
        "    n_test = max(1, round(len(shuffled) * test_ratio))\n",
        "    n_val = max(1, round(len(shuffled) * val_ratio))\n",
        "    test_set = set(shuffled[:n_test])\n",
        "    val_set = set(shuffled[n_test:n_test + n_val])\n",
        "    for s in samples:\n",
        "        if s[\"actor_id\"] in test_set:\n",
        "            s[\"split\"] = \"test\"\n",
        "        elif s[\"actor_id\"] in val_set:\n",
        "            s[\"split\"] = \"val\"\n",
        "        else:\n",
        "            s[\"split\"] = \"train\"\n",
        "\n",
        "\n",
        "samples = collect_av_speech_samples(cfg.raw_dir)\n",
        "assign_splits(samples, cfg.val_ratio, cfg.test_ratio, cfg.split_seed)\n",
        "\n",
        "print(f\"Total: {len(samples)} AV speech samples\")\n",
        "for split in (\"train\", \"val\", \"test\"):\n",
        "    n = sum(1 for s in samples if s[\"split\"] == split)\n",
        "    print(f\"  {split}: {n}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "RAVDESS Dataset Preprocessing Pipeline\n",
            "================================================================================\n",
            "Found 6 actor folders in /content/raw_data\n",
            "Found 360 total video files across all actors\n",
            "\n",
            "Found 360 video files across all actor folders\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "\rProcessing RAVDESS videos:   0%|          | 0/360 [00:00<?, ?it/s]WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "\n",
            "Processing RAVDESS videos:  14%|█▍        | 51/360 [00:38<05:36,  1.09s/it] WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:247: UserWarning: torio.io._streaming_media_encoder.StreamingMediaEncoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n",
            "\n",
            "Processing RAVDESS videos: 100%|██████████| 360/360 [2:43:51<00:00, 27.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Processing Summary\n",
            "================================================================================\n",
            "Total videos found:          360\n",
            "Successfully processed:      360\n",
            "Skipped (modality filter):   0\n",
            "Skipped (vocal filter):      0\n",
            "Failed:                      0\n",
            "\n",
            "Metadata saved to: /content/processed_data/metadata.json\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Dataset Validation Report\n",
            "================================================================================\n",
            "\n",
            "Total samples: 360\n",
            "\n",
            "Actors represented: 6\n",
            "Samples per actor: 60.0 average\n",
            "\n",
            "Gender Distribution:\n",
            "  female: 180 (50.0%)\n",
            "  male: 180 (50.0%)\n",
            "\n",
            "Emotion Distribution:\n",
            "  calm        :  48 (13.3%)\n",
            "  happy       :  48 (13.3%)\n",
            "  sad         :  48 (13.3%)\n",
            "  angry       :  48 (13.3%)\n",
            "  fearful     :  48 (13.3%)\n",
            "  disgust     :  48 (13.3%)\n",
            "  surprised   :  48 (13.3%)\n",
            "  neutral     :  24 (6.7%)\n",
            "\n",
            "Intensity Distribution:\n",
            "  normal: 192 (53.3%)\n",
            "  strong: 168 (46.7%)\n",
            "\n",
            "Face Detection Statistics:\n",
            "  Average: 99.91%\n",
            "  Min: 96.88%\n",
            "  Max: 100.00%\n",
            "\n",
            "✓ All samples have ≥50% face detection rate\n",
            "\n",
            "Audio Duration Statistics:\n",
            "  Average: 3.80s\n",
            "  Min: 3.07s\n",
            "  Max: 5.27s\n",
            "\n",
            "Verifying file existence...\n",
            "✓ All audio and frame files exist\n",
            "\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Creating PyTorch Dataset\n",
            "================================================================================\n",
            "Loaded 360 samples from /content/processed_data/metadata.json\n",
            "Configuration: 32 frames, 224x224 resolution\n",
            "\n",
            "Sample ID: 01-01-01-01-01-01-19\n",
            "Emotion Label: 0\n",
            "Audio Shape: torch.Size([53920])\n",
            "Video Shape: torch.Size([32, 3, 224, 224])\n",
            "\n",
            "================================================================================\n",
            "Testing DataLoader\n",
            "================================================================================\n",
            "\n",
            "Batch Size: 4\n",
            "Emotion Labels: tensor([3, 6, 6, 5])\n",
            "Video Shape: torch.Size([4, 32, 3, 224, 224])\n",
            "Audio: 4 waveforms (variable length)\n",
            "\n",
            "================================================================================\n",
            "✓ Preprocessing and Dataset Creation Complete!\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_audio(video_path: str, out_path: Path, sr: int) -> float:\n",
        "    subprocess.run(\n",
        "        [\"ffmpeg\", \"-y\", \"-loglevel\", \"error\", \"-i\", video_path,\n",
        "         \"-ar\", str(sr), \"-ac\", \"1\", str(out_path)],\n",
        "        check=True,\n",
        "    )\n",
        "    info = torchaudio.info(str(out_path))\n",
        "    return info.num_frames / info.sample_rate\n",
        "\n",
        "\n",
        "audio_dir = cfg.out_dir / \"audio\"\n",
        "audio_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for s in tqdm(samples, desc=\"Extracting audio\"):\n",
        "    wav_path = audio_dir / f\"{s['sample_id']}.wav\"\n",
        "    s[\"audio_path\"] = str(wav_path)\n",
        "    if wav_path.exists():\n",
        "        info = torchaudio.info(str(wav_path))\n",
        "        s[\"duration\"] = info.num_frames / info.sample_rate\n",
        "        continue\n",
        "    try:\n",
        "        s[\"duration\"] = extract_audio(s[\"video_path\"], wav_path, cfg.sample_rate)\n",
        "    except Exception as e:\n",
        "        print(f\"FAILED {s['sample_id']}: {e}\")\n",
        "        s[\"duration\"] = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class FaceProcessor:\n",
        "    def __init__(self, size: int, pad_factor: float, min_pad: int):\n",
        "        self.size = size\n",
        "        self.pad_factor = pad_factor\n",
        "        self.min_pad = min_pad\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.fa = face_alignment.FaceAlignment(\n",
        "            face_alignment.LandmarksType.TWO_D, flip_input=False, device=device\n",
        "        )\n",
        "\n",
        "    def crop_face(self, frame: np.ndarray) -> tuple[np.ndarray, bool]:\n",
        "        landmarks = self.fa.get_landmarks(frame)\n",
        "        if not landmarks or landmarks[0].shape[0] != 68:\n",
        "            return self._center_crop(frame), False\n",
        "        lm = landmarks[0]\n",
        "        h, w = frame.shape[:2]\n",
        "        iod = np.linalg.norm(lm[36:42].mean(0) - lm[42:48].mean(0))\n",
        "        pad = int(max(self.min_pad, self.pad_factor * iod))\n",
        "        x0 = max(0, int(lm[:, 0].min()) - pad)\n",
        "        y0 = max(0, int(lm[:, 1].min()) - pad)\n",
        "        x1 = min(w, int(lm[:, 0].max()) + pad)\n",
        "        y1 = min(h, int(lm[:, 1].max()) + pad)\n",
        "        crop = frame[y0:y1, x0:x1]\n",
        "        if crop.size == 0:\n",
        "            return self._center_crop(frame), False\n",
        "        return cv2.resize(crop, (self.size, self.size)), True\n",
        "\n",
        "    def _center_crop(self, frame: np.ndarray) -> np.ndarray:\n",
        "        h, w = frame.shape[:2]\n",
        "        s = min(h, w)\n",
        "        y0, x0 = (h - s) // 2, (w - s) // 2\n",
        "        return cv2.resize(frame[y0:y0 + s, x0:x0 + s], (self.size, self.size))\n",
        "\n",
        "\n",
        "def sample_indices(total: int, n: int) -> np.ndarray:\n",
        "    if total <= 0:\n",
        "        return np.zeros(n, dtype=int)\n",
        "    if total <= n:\n",
        "        return np.pad(np.arange(total), (0, n - total), mode=\"edge\")\n",
        "    return np.linspace(0, total - 1, n, dtype=int)\n",
        "\n",
        "\n",
        "def process_video(path: str, fp: FaceProcessor, n_frames: int) -> tuple[np.ndarray, float]:\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    indices = sample_indices(total, n_frames)\n",
        "    frames, hits = [], 0\n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
        "        ok, bgr = cap.read()\n",
        "        if not ok:\n",
        "            frames.append(np.zeros((fp.size, fp.size, 3), dtype=np.uint8))\n",
        "            continue\n",
        "        face, detected = fp.crop_face(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))\n",
        "        hits += int(detected)\n",
        "        frames.append(face.astype(np.uint8))\n",
        "    cap.release()\n",
        "    return np.stack(frames), hits / max(len(indices), 1)\n",
        "\n",
        "\n",
        "fp = FaceProcessor(cfg.frame_size, cfg.face_pad_factor, cfg.min_face_pad)\n",
        "frames_dir = cfg.out_dir / \"frames\"\n",
        "frames_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for s in tqdm(samples, desc=\"Processing video\"):\n",
        "    npy_path = frames_dir / f\"{s['sample_id']}.npy\"\n",
        "    s[\"frames_path\"] = str(npy_path)\n",
        "    if npy_path.exists():\n",
        "        s[\"face_det_rate\"] = -1.0\n",
        "        continue\n",
        "    try:\n",
        "        frames, det_rate = process_video(s[\"video_path\"], fp, cfg.n_frames)\n",
        "        np.save(npy_path, frames)\n",
        "        s[\"face_det_rate\"] = det_rate\n",
        "    except Exception as e:\n",
        "        print(f\"FAILED {s['sample_id']}: {e}\")\n",
        "        s[\"face_det_rate\"] = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "meta_path = cfg.out_dir / \"metadata.json\"\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(samples, f, indent=2)\n",
        "print(f\"Saved {len(samples)} samples -> {meta_path}\\n\")\n",
        "\n",
        "for split in (\"train\", \"val\", \"test\"):\n",
        "    sub = [s for s in samples if s[\"split\"] == split]\n",
        "    print(f\"{split} ({len(sub)} samples)\")\n",
        "    for emo, cnt in sorted(Counter(s[\"emotion\"] for s in sub).items()):\n",
        "        print(f\"  {emo:12s} {cnt}\")\n",
        "    dets = [s[\"face_det_rate\"] for s in sub if s[\"face_det_rate\"] >= 0]\n",
        "    durs = [s.get(\"duration\", 0) for s in sub]\n",
        "    if dets:\n",
        "        print(f\"  face_det:    {np.mean(dets):.1%}\")\n",
        "    print(f\"  duration:    {np.mean(durs):.1f}s avg\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class RAVDESSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, metadata_path: str, split: str = \"train\", modality: str = \"both\"):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data if s[\"split\"] == split]\n",
        "        self.modality = modality\n",
        "        if not self.samples:\n",
        "            raise ValueError(f\"No samples for split='{split}'\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        item = {\"sample_id\": s[\"sample_id\"], \"emotion\": s[\"emotion_idx\"]}\n",
        "\n",
        "        if self.modality in (\"audio\", \"both\"):\n",
        "            wav, _ = torchaudio.load(s[\"audio_path\"])\n",
        "            item[\"audio\"] = wav.squeeze(0)\n",
        "\n",
        "        if self.modality in (\"video\", \"both\"):\n",
        "            frames = np.load(s[\"frames_path\"])  # (T, H, W, 3) uint8\n",
        "            item[\"video\"] = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    out = {\n",
        "        \"sample_id\": [b[\"sample_id\"] for b in batch],\n",
        "        \"emotion\": torch.tensor([b[\"emotion\"] for b in batch]),\n",
        "    }\n",
        "    if \"audio\" in batch[0]:\n",
        "        out[\"audio\"] = [b[\"audio\"] for b in batch]\n",
        "    if \"video\" in batch[0]:\n",
        "        out[\"video\"] = torch.stack([b[\"video\"] for b in batch])\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ds = RAVDESSDataset(str(meta_path), split=\"train\", modality=\"both\")\n",
        "print(f\"Train: {len(ds)} samples\\n\")\n",
        "\n",
        "sample = ds[0]\n",
        "print(f\"sample_id: {sample['sample_id']}\")\n",
        "print(f\"emotion:   {sample['emotion']}\")\n",
        "print(f\"audio:     {sample['audio'].shape}\")\n",
        "print(f\"video:     {sample['video'].shape}\")\n",
        "\n",
        "loader = torch.utils.data.DataLoader(ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "batch = next(iter(loader))\n",
        "print(f\"\\nBatch: {len(batch['audio'])} audio, video {batch['video'].shape}, emotions {batch['emotion']}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}