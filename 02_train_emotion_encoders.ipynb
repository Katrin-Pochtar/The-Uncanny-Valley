{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1k8C6avk0r2UN_W8EU0hH59dGI4crelgg","timestamp":1762145451486}],"authorship_tag":"ABX9TyOi+LmpbLnaR6guM6ups4Ka"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e1fc260b2d414be0a8758b203c2b45e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da81dc74e7f5428d8b9b550577407317","IPY_MODEL_e371f844914c483ea464680c293db45a","IPY_MODEL_ea8d581dbc3443708f0d3525012577ed"],"layout":"IPY_MODEL_6d43442536b04cf485a14f1aabdb0ecd"}},"da81dc74e7f5428d8b9b550577407317":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b808845936cd4c29ba0cd84b27d37030","placeholder":"​","style":"IPY_MODEL_c84f5201eef44a6dbade2e667d86b411","value":"config.json: "}},"e371f844914c483ea464680c293db45a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8da47976c3a94c6b8df7fe97d8595713","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_20bc4c21f8ea498b8b47c938c86d00b7","value":1}},"ea8d581dbc3443708f0d3525012577ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5bcd1ce96db40fe8d43b2424d3e0183","placeholder":"​","style":"IPY_MODEL_f62b280d37a84942b071c114876db9e7","value":" 2.15k/? [00:00&lt;00:00, 260kB/s]"}},"6d43442536b04cf485a14f1aabdb0ecd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b808845936cd4c29ba0cd84b27d37030":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c84f5201eef44a6dbade2e667d86b411":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8da47976c3a94c6b8df7fe97d8595713":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"20bc4c21f8ea498b8b47c938c86d00b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5bcd1ce96db40fe8d43b2424d3e0183":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f62b280d37a84942b071c114876db9e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7eed4856f88d40f2bce636e0ab569630":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_682ab862aef94ce08c030769aba7e8cc","IPY_MODEL_a2dc3bfec50f4598afe868d96b853e4a","IPY_MODEL_bc9f7bdb88d74b24a21ebe5813253ac0"],"layout":"IPY_MODEL_396f3ea2b5344ff1a3ce48d421283b18"}},"682ab862aef94ce08c030769aba7e8cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd161b5c1b1741fa80eac0694ab6663f","placeholder":"​","style":"IPY_MODEL_4900d870b686459197115df6bda63a8e","value":"pytorch_model.bin: 100%"}},"a2dc3bfec50f4598afe868d96b853e4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_77ba62d0b62b45a8b42723401aa83bd9","max":378361105,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52082a2cd0bf4b798b03b8f6a6aa6f5f","value":378361105}},"bc9f7bdb88d74b24a21ebe5813253ac0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a04687509da4b39945805c55383671f","placeholder":"​","style":"IPY_MODEL_961e2df81daf42ac85ee0bcaac0c9a3d","value":" 378M/378M [00:04&lt;00:00, 124MB/s]"}},"396f3ea2b5344ff1a3ce48d421283b18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd161b5c1b1741fa80eac0694ab6663f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4900d870b686459197115df6bda63a8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77ba62d0b62b45a8b42723401aa83bd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52082a2cd0bf4b798b03b8f6a6aa6f5f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a04687509da4b39945805c55383671f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"961e2df81daf42ac85ee0bcaac0c9a3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4f7c705e9124369910bb22523be769e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_366747f7ac144864acf9a9221ecf45cc","IPY_MODEL_0f6b64f385874bb29e13ce640f7014c5","IPY_MODEL_513cd8bd1cd24f1e99a758c20c9935cf"],"layout":"IPY_MODEL_b5b2a74c8c374e4f872b12631066e0b1"}},"366747f7ac144864acf9a9221ecf45cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e99c1d38ea53413d91fdb54aa03c28e6","placeholder":"​","style":"IPY_MODEL_64af70b73e764a48a472215897620a5d","value":"preprocessor_config.json: 100%"}},"0f6b64f385874bb29e13ce640f7014c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_369f41e7447741cead9910b4fea0a38f","max":215,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99fbd420361f46f89db21fbcd25b0785","value":215}},"513cd8bd1cd24f1e99a758c20c9935cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4142c875ed8b4d7881d2d5d9b6c4fb15","placeholder":"​","style":"IPY_MODEL_ff9d2a1a20e54ea5b83c2cea543a8b5e","value":" 215/215 [00:00&lt;00:00, 35.2kB/s]"}},"b5b2a74c8c374e4f872b12631066e0b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e99c1d38ea53413d91fdb54aa03c28e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64af70b73e764a48a472215897620a5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"369f41e7447741cead9910b4fea0a38f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99fbd420361f46f89db21fbcd25b0785":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4142c875ed8b4d7881d2d5d9b6c4fb15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff9d2a1a20e54ea5b83c2cea543a8b5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7523b3c7c2694dc497c74e42ad604548":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c12e2258fd44ca092b423d02a66e14f","IPY_MODEL_20ab473d85664fae867d82929b3f0be5","IPY_MODEL_e39f44b1ae1d4de39cf390b0f5453303"],"layout":"IPY_MODEL_56e9fd4f07ce46c0afe8fed8408e190a"}},"3c12e2258fd44ca092b423d02a66e14f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86c84ab87e4f4dea81caa9383041137d","placeholder":"​","style":"IPY_MODEL_d7ce31e7fac841dd9407d4360bd994b7","value":"model.safetensors: 100%"}},"20ab473d85664fae867d82929b3f0be5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9178c248258144c3afb44d0967a91641","max":378304498,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be788368956f48d1a9e68500a2ab3d43","value":378304498}},"e39f44b1ae1d4de39cf390b0f5453303":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3d30d9d2c764168a249da288fb7e656","placeholder":"​","style":"IPY_MODEL_52763c7d962e459ba3818bb274ad46cd","value":" 378M/378M [00:06&lt;00:00, 92.2MB/s]"}},"56e9fd4f07ce46c0afe8fed8408e190a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86c84ab87e4f4dea81caa9383041137d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7ce31e7fac841dd9407d4360bd994b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9178c248258144c3afb44d0967a91641":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be788368956f48d1a9e68500a2ab3d43":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a3d30d9d2c764168a249da288fb7e656":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52763c7d962e459ba3818bb274ad46cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fefe1b95498f424ab1bbb38521aa886b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7201fa4bdd7144349d42da26c0723224","IPY_MODEL_7ee0760f82a049f4bb20e0fcc08e4a2e","IPY_MODEL_5fb2c8315344499fbed75f490491a676"],"layout":"IPY_MODEL_42622d0f6ee546e2b3165699c0dc2a49"}},"7201fa4bdd7144349d42da26c0723224":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b886939616b486e94089d1132a3f58d","placeholder":"​","style":"IPY_MODEL_b057040ff96d45b6893f4c4ba2a1020a","value":"config.json: "}},"7ee0760f82a049f4bb20e0fcc08e4a2e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_55470e2e7a0348d6932a56963e065db0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2fe4a2745f941bfabdbfcdb568b97ec","value":1}},"5fb2c8315344499fbed75f490491a676":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2503143d92841178cdd83fcfb3cb5e7","placeholder":"​","style":"IPY_MODEL_ca30038206764771b32b379b76572828","value":" 22.7k/? [00:00&lt;00:00, 2.35MB/s]"}},"42622d0f6ee546e2b3165699c0dc2a49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b886939616b486e94089d1132a3f58d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b057040ff96d45b6893f4c4ba2a1020a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55470e2e7a0348d6932a56963e065db0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e2fe4a2745f941bfabdbfcdb568b97ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2503143d92841178cdd83fcfb3cb5e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca30038206764771b32b379b76572828":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4096ce9c56fa40c3b3ff70204fd18bd5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_926f495ca90a484eab7b671146471add","IPY_MODEL_d10d552123684380939359d7939e7faf","IPY_MODEL_5aee0926f9884a8e989038c03c170588"],"layout":"IPY_MODEL_41f8a8cf4cb2462b859f9f0322eb22df"}},"926f495ca90a484eab7b671146471add":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffbca58dfdd14f2099211571f1f2fc47","placeholder":"​","style":"IPY_MODEL_0e21be22a213445a8ffdc3626b284bcb","value":"pytorch_model.bin: 100%"}},"d10d552123684380939359d7939e7faf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_71faddbd55814745a66730e5f5a32b21","max":486348721,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f25720c173284d40b4a06b70c19a2664","value":486348721}},"5aee0926f9884a8e989038c03c170588":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9528c887b4446b7910cdcc29bf80e72","placeholder":"​","style":"IPY_MODEL_39a20c1d6dfb4158af700ec2b7d2321a","value":" 486M/486M [00:06&lt;00:00, 95.8MB/s]"}},"41f8a8cf4cb2462b859f9f0322eb22df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffbca58dfdd14f2099211571f1f2fc47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e21be22a213445a8ffdc3626b284bcb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71faddbd55814745a66730e5f5a32b21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f25720c173284d40b4a06b70c19a2664":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9528c887b4446b7910cdcc29bf80e72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39a20c1d6dfb4158af700ec2b7d2321a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b651a344d132481b95450d95b64bd21f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a2f670c97074778aaa44b072a430eb1","IPY_MODEL_8c55f34f2fe5483a803fa3b5563668c2","IPY_MODEL_af86b7f9ea6e483e99a4f5d42f65def1"],"layout":"IPY_MODEL_8fb6706ac2384b238e5e9f0f356261bc"}},"6a2f670c97074778aaa44b072a430eb1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b64eb91117cb40d9b9a932073a17c2f7","placeholder":"​","style":"IPY_MODEL_a308fc29ddd04de69c2cae34a463a47e","value":"preprocessor_config.json: 100%"}},"8c55f34f2fe5483a803fa3b5563668c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bc46dd329e6400e8bdf78f0007b7d41","max":412,"min":0,"orientation":"horizontal","style":"IPY_MODEL_753c318e418a407ebf22b660a70c632a","value":412}},"af86b7f9ea6e483e99a4f5d42f65def1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4366fdc9c21e4d0eb23c136b8d60c0b6","placeholder":"​","style":"IPY_MODEL_0487550a8a2b424ebeedb47bff336145","value":" 412/412 [00:00&lt;00:00, 70.6kB/s]"}},"8fb6706ac2384b238e5e9f0f356261bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b64eb91117cb40d9b9a932073a17c2f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a308fc29ddd04de69c2cae34a463a47e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bc46dd329e6400e8bdf78f0007b7d41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"753c318e418a407ebf22b660a70c632a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4366fdc9c21e4d0eb23c136b8d60c0b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0487550a8a2b424ebeedb47bff336145":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46cbf4e2bae3473b872509352ff26e6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7208800319e84ac28d841183f9c0f212","IPY_MODEL_7e4a7d0f1bac4ff0a9187e45b9856c63","IPY_MODEL_5f015c7151254666b5620b260abb2dbc"],"layout":"IPY_MODEL_2c574ee7b6974d9880dc95fa22a99a0d"}},"7208800319e84ac28d841183f9c0f212":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b98b26eda5b94a088ce153ebfe463cd4","placeholder":"​","style":"IPY_MODEL_1e335b05b82641a3a8bef8a906a1b4f9","value":"model.safetensors: 100%"}},"7e4a7d0f1bac4ff0a9187e45b9856c63":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0dc20ad94fb44cdb221642c943dd881","max":486296528,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ee543cdeb294800bbc04cdb75535898","value":486296528}},"5f015c7151254666b5620b260abb2dbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c403f1fb6ba347469e1d587fa1cf5b36","placeholder":"​","style":"IPY_MODEL_d97c2a6755b64543b65599ee76e9b7f0","value":" 486M/486M [00:06&lt;00:00, 59.2MB/s]"}},"2c574ee7b6974d9880dc95fa22a99a0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b98b26eda5b94a088ce153ebfe463cd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e335b05b82641a3a8bef8a906a1b4f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0dc20ad94fb44cdb221642c943dd881":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ee543cdeb294800bbc04cdb75535898":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c403f1fb6ba347469e1d587fa1cf5b36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d97c2a6755b64543b65599ee76e9b7f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["shutil.rmtree('/content/trained_encoders')"],"metadata":{"id":"xrm6PhAhYekg","colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"status":"error","timestamp":1762454229695,"user_tz":420,"elapsed":8,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}},"outputId":"6fa0b3b0-a7ea-4b0f-8278-ed41b0dceb76"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'shutil' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1332525631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/trained_encoders'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'shutil' is not defined"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1qCcCSG3Mhw","executionInfo":{"status":"ok","timestamp":1762454230166,"user_tz":420,"elapsed":436,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}},"outputId":"3615eca4-6454-45f4-dc51-46a5e33c93d8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Nov  6 18:37:10 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n","+-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA GeForce RTX 3090        Off |   00000000:0B:00.0  On |                  N/A |\n","|  0%   46C    P8             19W /  350W |     712MiB /  24576MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","\n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["import torch\n","print(torch.version.cuda)\n","print(torch.cuda.is_available())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbt5bWU-3KRf","executionInfo":{"status":"ok","timestamp":1762454230173,"user_tz":420,"elapsed":5,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}},"outputId":"577358db-6e23-4f38-d13a-fa22c2fd9c35"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["12.6\n","True\n"]}]},{"cell_type":"code","source":["# !wandb login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xLB4vnFGF9go","executionInfo":{"status":"ok","timestamp":1762454451674,"user_tz":420,"elapsed":221500,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}},"outputId":"11907d5a-8f0f-4afc-c467-7e81448d8ba6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","Aborted!\n"]}]},{"cell_type":"code","source":["# encoders_window_level_v3_fixed2.py\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Window-level Emotion Encoders (Audio + Video), stable on small VMs/Colab.\n","\n","Fixes vs v3_fixed:\n","- Do NOT call enable_input_require_grads() (it caused AttributeError with tuple outputs).\n","- Gradient checkpointing is optional and disabled by default (use_checkpoint=False).\n","- Keeps do_rescale=False, AMP, workers=0, pin_memory=False, etc.\n","\"\"\"\n","\n","import os\n","import json\n","import warnings\n","from pathlib import Path\n","from typing import Dict, List, Optional, Tuple, Union\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchaudio\n","from torch.utils.data import Dataset, DataLoader, Subset\n","\n","# Stable AMP API\n","from torch.amp import GradScaler, autocast\n","\n","# Silence noisy torchaudio warnings\n","warnings.filterwarnings(\"ignore\", message=\".*StreamingMediaDecoder.*\")\n","warnings.filterwarnings(\"ignore\", message=\".*load_with_torchcodec.*\")\n","\n","# Safer multiprocessing defaults (we still use workers=0 by default)\n","import torch.multiprocessing as mp\n","try:\n","    mp.set_start_method(\"spawn\", force=True)\n","except RuntimeError:\n","    pass\n","try:\n","    mp.set_sharing_strategy(\"file_system\")\n","except RuntimeError:\n","    pass\n","\n","from transformers import (\n","    Wav2Vec2ForSequenceClassification,\n","    HubertForSequenceClassification,\n","    Wav2Vec2FeatureExtractor,\n","    AutoImageProcessor,\n","    TimesformerForVideoClassification,\n",")\n","\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","\n","\n","# -------------------\n","# Constants / Labels\n","# -------------------\n","\n","EMOTION_TO_ID = {\n","    \"neutral\": 0, \"calm\": 1, \"happy\": 2, \"sad\": 3,\n","    \"angry\": 4, \"fearful\": 5, \"disgust\": 6, \"surprised\": 7\n","}\n","EMOTION_NAMES = [\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprised\"]\n","\n","\n","# -------------------\n","# Utils\n","# -------------------\n","\n","def ensure_dir(p: Union[str, Path]):\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","\n","\n","def uniform_indices(total: int, target: int) -> np.ndarray:\n","    if total <= 0:\n","        return np.zeros((target,), dtype=int)\n","    if total <= target:\n","        base = np.arange(total)\n","        pad = np.full(target - total, total - 1, dtype=int)\n","        return np.concatenate([base, pad])\n","    return np.round(np.linspace(0, total - 1, target)).astype(int)\n","\n","\n","def set_seed(seed: int = 42):\n","    import random\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","\n","def set_backbone_trainable_timesformer(model: nn.Module, trainable: bool):\n","    for n, p in model.named_parameters():\n","        if \"classifier\" in n:\n","            continue\n","        p.requires_grad = trainable\n","\n","\n","def safe_freeze_wav2vec_feature_encoder(model: nn.Module):\n","    if hasattr(model, \"freeze_feature_encoder\"):\n","        model.freeze_feature_encoder()\n","    else:\n","        for n, p in model.named_parameters():\n","            if \"classifier\" in n:\n","                continue\n","            p.requires_grad = False\n","\n","\n","def safe_unfreeze_wav2vec_feature_encoder(model: nn.Module):\n","    for p in model.parameters():\n","        p.requires_grad = True\n","\n","\n","# -------------------\n","# Dataset\n","# -------------------\n","\n","class EmotionDataset(Dataset):\n","    def __init__(\n","        self,\n","        metadata_path: Union[str, Path],\n","        video_max_frames: int = 64,\n","        audio_target_sr: int = 16000,\n","        load_audio: bool = True,\n","        load_video: bool = True,\n","    ):\n","        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n","            self.meta: List[Dict] = json.load(f)\n","        if len(self.meta) == 0:\n","            raise ValueError(\"Empty metadata file.\")\n","\n","        self.video_max_frames = int(video_max_frames)\n","        self.audio_target_sr = int(audio_target_sr)\n","        self.load_audio = bool(load_audio)\n","        self.load_video = bool(load_video)\n","\n","        m0 = self.meta[0]\n","        self.uses_npz = \"video_npz\" in m0\n","\n","        self.frames_per_clip = int(m0.get(\"frames_per_clip\", m0.get(\"fixed_T\", 32)))\n","        if \"frame_size\" in m0:\n","            self.frame_size = tuple(m0[\"frame_size\"])\n","        elif \"video_size\" in m0:\n","            H, W = m0[\"video_size\"]\n","            self.frame_size = (W, H)\n","        else:\n","            self.frame_size = (224, 224)\n","\n","    def __len__(self):\n","        return len(self.meta)\n","\n","    def _load_audio(self, audio_path: str) -> torch.Tensor:\n","        wav, sr = torchaudio.load(audio_path)\n","        if wav.shape[0] > 1:\n","            wav = wav.mean(dim=0, keepdim=True)\n","        if sr != self.audio_target_sr:\n","            wav = torchaudio.transforms.Resample(sr, self.audio_target_sr)(wav)\n","        return wav.squeeze(0)\n","\n","    def _load_video_npz(self, npz_path: str) -> Tuple[torch.Tensor, np.ndarray]:\n","        data = np.load(npz_path)\n","        frames = data[\"frames\"]\n","        ts = data.get(\"timestamps\", None)\n","        if ts is None:\n","            T = frames.shape[0]\n","            ts = np.linspace(0.0, float(T - 1) / 25.0, num=T, dtype=np.float32)\n","        if frames.shape[0] > self.video_max_frames:\n","            idx = uniform_indices(frames.shape[0], self.video_max_frames)\n","            frames = frames[idx]\n","            ts = ts[idx]\n","        frames = frames.astype(np.float32) / 255.0\n","        tchw = torch.from_numpy(frames).permute(0, 3, 1, 2).contiguous()\n","        return tchw, ts\n","\n","    def _load_video_frames_dir(self, frames_dir: str) -> torch.Tensor:\n","        frame_files = sorted(Path(frames_dir).glob(\"frame_*.npy\"))\n","        if len(frame_files) == 0:\n","            W, H = self.frame_size\n","            return torch.zeros((self.video_max_frames, 3, H, W), dtype=torch.float32)\n","        if len(frame_files) > self.video_max_frames:\n","            idx = uniform_indices(len(frame_files), self.video_max_frames)\n","            frame_files = [frame_files[i] for i in idx]\n","        frames = []\n","        for f in frame_files:\n","            arr = np.load(f, mmap_mode=\"r\")\n","            if arr.ndim != 3 or arr.shape[2] != 3:\n","                W, H = self.frame_size\n","                arr = np.zeros((H, W, 3), dtype=np.uint8)\n","            frames.append(arr.astype(np.float32) / 255.0)\n","        frames = np.stack(frames, axis=0)\n","        tchw = torch.from_numpy(frames).permute(0, 3, 1, 2).contiguous()\n","        return tchw\n","\n","    def __getitem__(self, idx: int) -> Dict:\n","        rec = self.meta[idx]\n","        out = {\n","            \"sample_id\": rec[\"sample_id\"],\n","            \"emotion_label\": EMOTION_TO_ID.get(rec.get(\"emotion\", \"\"), -1),\n","            \"meta\": rec,\n","        }\n","        if self.load_audio:\n","            out[\"audio\"] = self._load_audio(rec[\"audio_path\"])\n","            out[\"sample_rate\"] = self.audio_target_sr\n","        if self.load_video:\n","            if self.uses_npz:\n","                v, ts = self._load_video_npz(rec[\"video_npz\"])\n","                out[\"video\"] = v\n","                out[\"timestamps\"] = torch.from_numpy(ts)\n","            else:\n","                v = self._load_video_frames_dir(rec[\"video_frames_dir\"])\n","                out[\"video\"] = v\n","                fps = rec.get(\"target_fps\", rec.get(\"original_fps\", 25.0))\n","                T = v.shape[0]\n","                ts = np.arange(T, dtype=np.float32) / float(fps)\n","                out[\"timestamps\"] = torch.from_numpy(ts)\n","        return out\n","\n","\n","def emotion_collate(batch: List[Dict]) -> Dict:\n","    out: Dict[str, Union[List, torch.Tensor]] = {\n","        \"sample_id\": [b[\"sample_id\"] for b in batch],\n","        \"emotion_label\": torch.tensor([b[\"emotion_label\"] for b in batch], dtype=torch.long),\n","        \"meta\": [b[\"meta\"] for b in batch],\n","    }\n","    if \"audio\" in batch[0]:\n","        out[\"audio\"] = [b[\"audio\"] for b in batch]\n","        out[\"sample_rate\"] = batch[0][\"sample_rate\"]\n","    if \"video\" in batch[0]:\n","        out[\"video\"] = torch.stack([b[\"video\"] for b in batch], dim=0)\n","        out[\"timestamps\"] = [b[\"timestamps\"] for b in batch]\n","    return out\n","\n","\n","# Window cropping helpers\n","def crop_audio_random(wav_1d: torch.Tensor, sr: int, dur_s: float) -> torch.Tensor:\n","    n = wav_1d.numel()\n","    L = int(round(dur_s * sr))\n","    if n <= L:\n","        pad_val = wav_1d[-1] if n > 0 else torch.tensor(0.0, device=wav_1d.device)\n","        pad = pad_val.repeat(L - n)\n","        return torch.cat([wav_1d, pad], 0)\n","    start = torch.randint(0, n - L + 1, ()).item()\n","    return wav_1d[start:start + L]\n","\n","\n","def crop_audio_center(wav_1d: torch.Tensor, sr: int, dur_s: float) -> torch.Tensor:\n","    n = wav_1d.numel()\n","    L = int(round(dur_s * sr))\n","    if n <= L:\n","        pad_val = wav_1d[-1] if n > 0 else torch.tensor(0.0, device=wav_1d.device)\n","        pad = pad_val.repeat(L - n)\n","        return torch.cat([wav_1d, pad], 0)\n","    start = max(0, (n - L) // 2)\n","    return wav_1d[start:start + L]\n","\n","\n","def crop_video_random_T(video_TCHW: torch.Tensor, Ts: int) -> torch.Tensor:\n","    T = video_TCHW.shape[0]\n","    if T <= Ts:\n","        idx = torch.linspace(0, T - 1, Ts).round().long()\n","        return video_TCHW[idx]\n","    start = torch.randint(0, T - Ts + 1, ()).item()\n","    return video_TCHW[start:start + Ts]\n","\n","\n","def crop_video_center_T(video_TCHW: torch.Tensor, Ts: int) -> torch.Tensor:\n","    T = video_TCHW.shape[0]\n","    if T <= Ts:\n","        idx = torch.linspace(0, T - 1, Ts).round().long()\n","        return video_TCHW[idx]\n","    start = (T - Ts) // 2\n","    return video_TCHW[start:start + Ts]\n","\n","\n","# Audio encoder\n","class AudioEmotionEncoder:\n","    def __init__(\n","        self,\n","        model_name: str = \"superb/wav2vec2-base-superb-er\",\n","        num_emotions: int = 8,\n","        lr: float = 1e-5,\n","        device: Optional[str] = None,\n","        window_seconds: float = 1.5,\n","        grad_clip: float = 1.0,\n","        use_amp: bool = True,\n","    ):\n","        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.num_emotions = num_emotions\n","        self.window_seconds = float(window_seconds)\n","        self.grad_clip = float(grad_clip)\n","        self.use_amp = bool(use_amp)\n","        self.lr = lr\n","\n","        if \"hubert\" in model_name.lower():\n","            self.model = HubertForSequenceClassification.from_pretrained(\n","                model_name, num_labels=num_emotions, ignore_mismatched_sizes=True\n","            )\n","        else:\n","            self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\n","                model_name, num_labels=num_emotions, ignore_mismatched_sizes=True\n","            )\n","\n","        self.model.config.output_hidden_states = True\n","        self.model.to(self.device)\n","\n","        # Try load feature extractor; if model_name is a local dir without files this may throw.\n","        try:\n","            self.processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n","        except Exception:\n","            # fallback to a default extractor so validation/inference works\n","            self.processor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n","\n","        # create optimizer only from trainable params\n","        self.optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n","        self.crit = nn.CrossEntropyLoss()\n","        self.emotion_names = EMOTION_NAMES\n","\n","        # proper GradScaler init\n","        self.scaler = GradScaler(enabled=(self.use_amp and torch.cuda.is_available()))\n","\n","    def update_optimizer(self, lr: Optional[float] = None):\n","        \"\"\"Re-build the optimizer from current trainable parameters (call after unfreezing).\"\"\"\n","        if lr is not None:\n","            self.lr = lr\n","        self.optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n","\n","    def save(self, path: Union[str, Path]):\n","        \"\"\"Save model + processor so HF's from_pretrained(path) works later.\"\"\"\n","        p = Path(path)\n","        p.mkdir(parents=True, exist_ok=True)\n","        self.model.save_pretrained(str(p))\n","        try:\n","            # processor may be Wav2Vec2FeatureExtractor or Processor\n","            self.processor.save_pretrained(str(p))\n","        except Exception:\n","            # best effort\n","            pass\n","\n","    def _prepare(self, batch: Dict, train: bool = True):\n","        sr = batch[\"sample_rate\"]\n","        audios = []\n","        for a in batch[\"audio\"]:\n","            a = a.to(self.device)\n","            seg = crop_audio_random(a, sr, self.window_seconds) if train else crop_audio_center(a, sr, self.window_seconds)\n","            audios.append(seg.cpu().numpy())\n","        proc = self.processor(\n","            audios, sampling_rate=sr, return_tensors=\"pt\",\n","            padding=True, truncation=True, max_length=int(self.window_seconds * sr)\n","        )\n","        x = proc[\"input_values\"].to(self.device)\n","        m = proc.get(\"attention_mask\")\n","        m = m.to(self.device) if m is not None else None\n","        y = batch[\"emotion_label\"].to(self.device)\n","        return x, m, y\n","\n","    def train_epoch(self, loader: DataLoader) -> Dict[str, float]:\n","        self.model.train()\n","        total, preds_all, labels_all = 0.0, [], []\n","        for batch in tqdm(loader, desc=\"Training (Audio)\"):\n","            x, m, y = self._prepare(batch, train=True)\n","            self.optim.zero_grad(set_to_none=True)\n","            with autocast(\"cuda\", enabled=self.use_amp):\n","                out = self.model(input_values=x, attention_mask=m)\n","                loss = self.crit(out.logits, y)\n","            self.scaler.scale(loss).backward()\n","            if self.grad_clip is not None:\n","                self.scaler.unscale_(self.optim)\n","                nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n","            self.scaler.step(self.optim)\n","            self.scaler.update()\n","            total += loss.item()\n","            preds_all.extend(out.logits.argmax(dim=1).detach().cpu().numpy())\n","            labels_all.extend(y.detach().cpu().numpy())\n","        return {\n","            \"loss\": total / len(loader),\n","            \"accuracy\": accuracy_score(labels_all, preds_all),\n","            \"f1_score\": f1_score(labels_all, preds_all, average=\"weighted\")\n","        }\n","\n","    @torch.no_grad()\n","    def validate(self, loader: DataLoader) -> Dict[str, float]:\n","        self.model.eval()\n","        total, preds_all, labels_all = 0.0, [], []\n","        for batch in tqdm(loader, desc=\"Validation (Audio)\"):\n","            x, m, y = self._prepare(batch, train=False)\n","            with autocast(\"cuda\", enabled=self.use_amp):\n","                out = self.model(input_values=x, attention_mask=m)\n","                loss = self.crit(out.logits, y)\n","            total += loss.item()\n","            preds_all.extend(out.logits.argmax(dim=1).detach().cpu().numpy())\n","            labels_all.extend(y.detach().cpu().numpy())\n","        cm = confusion_matrix(labels_all, preds_all)\n","        return {\n","            \"loss\": total / len(loader),\n","            \"accuracy\": accuracy_score(labels_all, preds_all),\n","            \"f1_score\": f1_score(labels_all, preds_all, average=\"weighted\"),\n","            \"confusion_matrix\": cm,\n","            \"predictions\": preds_all,\n","            \"labels\": labels_all\n","        }\n","\n","    @torch.no_grad()\n","    def extract_embeddings_clip(self, audios_1d: List[torch.Tensor], sr: int = 16000, window_seconds: float = 1.5) -> torch.Tensor:\n","        self.model.eval()\n","        crops = [crop_audio_center(a.to(self.device), sr, window_seconds).cpu().numpy() for a in audios_1d]\n","        proc = self.processor(crops, sampling_rate=sr, return_tensors=\"pt\", padding=True, truncation=True,\n","                              max_length=int(window_seconds * sr))\n","        x = proc[\"input_values\"].to(self.device)\n","        m = proc.get(\"attention_mask\"); m = m.to(self.device) if m is not None else None\n","        out = self.model(input_values=x, attention_mask=m, output_hidden_states=True)\n","        last = getattr(out, \"hidden_states\", None)\n","        last = last[-1] if last is not None else getattr(out, \"last_hidden_state\")\n","        return last.mean(dim=1)  # (B, D)\n","\n","    @torch.no_grad()\n","    def extract_embeddings_window(self, audio_1d: torch.Tensor, sr: int, t0: float, t1: float) -> torch.Tensor:\n","        self.model.eval()\n","        start = int(max(0, round(t0 * sr)))\n","        end = int(max(start + 1, round(t1 * sr)))\n","        seg = audio_1d[start:end]\n","        L = max(1, end - start)\n","        if seg.numel() < L:\n","            pad_val = seg[-1] if seg.numel() > 0 else torch.tensor(0.0, device=audio_1d.device)\n","            seg = torch.cat([seg, pad_val.repeat(L - seg.numel())], 0)\n","        proc = self.processor([seg.cpu().numpy()], sampling_rate=sr, return_tensors=\"pt\", padding=True, truncation=True, max_length=L)\n","        x = proc[\"input_values\"].to(self.device)\n","        m = proc.get(\"attention_mask\"); m = m.to(self.device) if m is not None else None\n","        out = self.model(input_values=x, attention_mask=m, output_hidden_states=True)\n","        last = getattr(out, \"hidden_states\", None); last = last[-1] if last is not None else getattr(out, \"last_hidden_state\")\n","        return last.mean(dim=1)\n","\n","\n","# Video encoder (checkpointing disabled by default to avoid tuple hook crash)\n","class VideoEmotionEncoder:\n","    def __init__(\n","        self,\n","        model_name: str = \"facebook/timesformer-base-finetuned-k400\",\n","        num_emotions: int = 8,\n","        lr: float = 1e-5,\n","        frames_for_model: int = 16,\n","        device: Optional[str] = None,\n","        grad_clip: float = 1.0,\n","        use_amp: bool = True,\n","        use_checkpoint: bool = False,\n","    ):\n","        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.num_emotions = num_emotions\n","        self.frames_for_model = int(frames_for_model)\n","        self.grad_clip = float(grad_clip)\n","        self.use_amp = bool(use_amp)\n","        self.lr = lr\n","\n","        self.model = TimesformerForVideoClassification.from_pretrained(\n","            model_name, num_labels=num_emotions, ignore_mismatched_sizes=True\n","        )\n","        self.model.config.output_hidden_states = True\n","\n","        if use_checkpoint and hasattr(self.model, \"gradient_checkpointing_enable\"):\n","            try:\n","                self.model.gradient_checkpointing_enable()\n","            except Exception:\n","                pass\n","\n","        self.model.to(self.device)\n","\n","        # Robust processor loading\n","        try:\n","            self.processor = AutoImageProcessor.from_pretrained(model_name)\n","        except Exception:\n","            # fallback (the HF model should usually have a processor)\n","            self.processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n","\n","        self.optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n","        self.crit = nn.CrossEntropyLoss()\n","        self.emotion_names = EMOTION_NAMES\n","        self.scaler = GradScaler(enabled=(self.use_amp and torch.cuda.is_available()))\n","\n","    def update_optimizer(self, lr: Optional[float] = None):\n","        if lr is not None:\n","            self.lr = lr\n","        self.optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n","\n","    def save(self, path: Union[str, Path]):\n","        p = Path(path)\n","        p.mkdir(parents=True, exist_ok=True)\n","        self.model.save_pretrained(str(p))\n","        try:\n","            self.processor.save_pretrained(str(p))\n","        except Exception:\n","            pass\n","\n","    def _prepare(self, batch: Dict, train: bool = True):\n","        vids = []\n","        Ts = self.frames_for_model\n","        for v in batch[\"video\"]:\n","            s = crop_video_random_T(v, Ts) if train else crop_video_center_T(v, Ts)\n","            frames = [s[i].permute(1, 2, 0).cpu().numpy() for i in range(s.shape[0])]\n","            vids.append(frames)\n","        proc = self.processor(vids, return_tensors=\"pt\", do_rescale=False)  # frames already in [0,1]\n","        x = proc[\"pixel_values\"].to(self.device)\n","        y = batch[\"emotion_label\"].to(self.device)\n","        return x, y\n","\n","    def train_epoch(self, loader: DataLoader) -> Dict[str, float]:\n","        self.model.train()\n","        total, preds_all, labels_all = 0.0, [], []\n","        for batch in tqdm(loader, desc=\"Training (Video)\"):\n","            x, y = self._prepare(batch, train=True)\n","            self.optim.zero_grad(set_to_none=True)\n","            with autocast(\"cuda\", enabled=self.use_amp):\n","                out = self.model(pixel_values=x)\n","                loss = self.crit(out.logits, y)\n","            self.scaler.scale(loss).backward()\n","            if self.grad_clip is not None:\n","                self.scaler.unscale_(self.optim)\n","                nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n","            self.scaler.step(self.optim)\n","            self.scaler.update()\n","            total += loss.item()\n","            preds_all.extend(out.logits.argmax(dim=1).detach().cpu().numpy())\n","            labels_all.extend(y.detach().cpu().numpy())\n","        return {\n","            \"loss\": total / len(loader),\n","            \"accuracy\": accuracy_score(labels_all, preds_all),\n","            \"f1_score\": f1_score(labels_all, preds_all, average=\"weighted\"),\n","        }\n","\n","    @torch.no_grad()\n","    def validate(self, loader: DataLoader) -> Dict[str, float]:\n","        self.model.eval()\n","        total, preds_all, labels_all = 0.0, [], []\n","        for batch in tqdm(loader, desc=\"Validation (Video)\"):\n","            x, y = self._prepare(batch, train=False)\n","            with autocast(\"cuda\", enabled=self.use_amp):\n","                out = self.model(pixel_values=x)\n","                loss = self.crit(out.logits, y)\n","            total += loss.item()\n","            preds_all.extend(out.logits.argmax(dim=1).detach().cpu().numpy())\n","            labels_all.extend(y.detach().cpu().numpy())\n","        cm = confusion_matrix(labels_all, preds_all)\n","        return {\n","            \"loss\": total / len(loader),\n","            \"accuracy\": accuracy_score(labels_all, preds_all),\n","            \"f1_score\": f1_score(labels_all, preds_all, average=\"weighted\"),\n","            \"confusion_matrix\": cm,\n","            \"predictions\": preds_all,\n","            \"labels\": labels_all\n","        }\n","\n","    @torch.no_grad()\n","    def extract_embeddings_clip(self, video_TCHW: torch.Tensor, frames_for_model: Optional[int] = None) -> torch.Tensor:\n","        self.model.eval()\n","        Ts = frames_for_model or self.frames_for_model\n","        if video_TCHW.dim() == 4:\n","            video_TCHW = video_TCHW.unsqueeze(0)\n","        batch_embs = []\n","        for v in video_TCHW:\n","            s = crop_video_center_T(v, Ts)\n","            frames = [s[i].permute(1, 2, 0).cpu().numpy() for i in range(s.shape[0])]\n","            proc = self.processor([frames], return_tensors=\"pt\", do_rescale=False)\n","            x = proc[\"pixel_values\"].to(self.device)\n","            out = self.model(pixel_values=x, output_hidden_states=True)\n","            hs = getattr(out, \"hidden_states\", None)\n","            last = hs[-1] if hs is not None else getattr(out, \"last_hidden_state\")\n","            emb = last.mean(dim=1)\n","            batch_embs.append(emb)\n","        return torch.cat(batch_embs, dim=0)\n","\n","    @torch.no_grad()\n","    def extract_embeddings_window_from_npz(self, npz_path: str, t0: float, t1: float, Ts: Optional[int] = None) -> torch.Tensor:\n","        self.model.eval()\n","        Ts = Ts or self.frames_for_model\n","        data = np.load(npz_path)\n","        frames = data[\"frames\"]\n","        ts = data[\"timestamps\"].astype(np.float32) if \"timestamps\" in data else np.arange(frames.shape[0], dtype=np.float32) / 25.0\n","        mask = (ts >= t0) & (ts <= t1)\n","        sub = frames[mask]\n","        if sub.shape[0] == 0:\n","            center = 0.5 * (t0 + t1)\n","            idx = int(np.argmin(np.abs(ts - center)))\n","            sub = frames[idx:idx+1]\n","        idx = uniform_indices(sub.shape[0], Ts)\n","        sub = sub[idx].astype(np.float32) / 255.0\n","        frames_list = [sub[i] for i in range(sub.shape[0])]\n","        proc = self.processor([frames_list], return_tensors=\"pt\", do_rescale=False)\n","        x = proc[\"pixel_values\"].to(self.device)\n","        out = self.model(pixel_values=x, output_hidden_states=True)\n","        hs = getattr(out, \"hidden_states\", None)\n","        last = hs[-1] if hs is not None else getattr(out, \"last_hidden_state\")\n","        return last.mean(dim=1)\n","\n","\n","# Trainer\n","def train_encoders(\n","    metadata_path: str,\n","    output_dir: str,\n","    audio_model: str = \"superb/wav2vec2-base-superb-er\",\n","    video_model: str = \"facebook/timesformer-base-finetuned-k400\",\n","    num_epochs: int = 20,\n","    batch_size: int = 4,\n","    val_split: float = 0.2,\n","    audio_window_s: float = 1.5,\n","    video_Ts: int = 16,\n","    video_max_frames: int = 64,\n","    use_wandb: bool = True,\n","    seed: int = 42,\n","    audio_freeze_epochs: int = 2,\n","    video_freeze_epochs: int = 1,\n","):\n","    set_seed(seed)\n","    ensure_dir(output_dir)\n","\n","    WANDB = False\n","    if use_wandb:\n","        try:\n","            import wandb\n","            wandb.init(project=\"almost-human-encoders\", config=dict(\n","                audio_model=audio_model, video_model=video_model, num_epochs=num_epochs,\n","                batch_size=batch_size, val_split=val_split, audio_window_s=audio_window_s,\n","                video_Ts=video_Ts, seed=seed\n","            ))\n","            WANDB = True\n","        except Exception as e:\n","            print(f\"⚠ W&B init failed: {e}\")\n","            WANDB = False\n","\n","    base = EmotionDataset(metadata_path, video_max_frames=video_max_frames, load_audio=True, load_video=True)\n","    N = len(base)\n","    val_size = int(N * val_split)\n","    train_size = N - val_size\n","    indices = torch.randperm(N)\n","    train_idx, val_idx = indices[:train_size], indices[train_size:]\n","\n","    ds_audio_train = Subset(EmotionDataset(metadata_path, video_max_frames=video_max_frames, load_audio=True, load_video=False), train_idx)\n","    ds_audio_val   = Subset(EmotionDataset(metadata_path, video_max_frames=video_max_frames, load_audio=True, load_video=False), val_idx)\n","    ds_video_train = Subset(EmotionDataset(metadata_path, video_max_frames=video_max_frames, load_audio=False, load_video=True), train_idx)\n","    ds_video_val   = Subset(EmotionDataset(metadata_path, video_max_frames=video_max_frames, load_audio=False, load_video=True), val_idx)\n","\n","    train_loader_audio = DataLoader(ds_audio_train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, collate_fn=emotion_collate)\n","    val_loader_audio   = DataLoader(ds_audio_val,   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False, collate_fn=emotion_collate)\n","    train_loader_video = DataLoader(ds_video_train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, collate_fn=emotion_collate)\n","    val_loader_video   = DataLoader(ds_video_val,   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False, collate_fn=emotion_collate)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"Device: {device} | Train: {train_size} | Val: {val_size}\")\n","\n","    audio_enc = AudioEmotionEncoder(model_name=audio_model, device=device, window_seconds=audio_window_s, use_amp=True)\n","    video_enc = VideoEmotionEncoder(model_name=video_model, device=device, frames_for_model=video_Ts, use_amp=True, use_checkpoint=False)\n","\n","    safe_freeze_wav2vec_feature_encoder(audio_enc.model)\n","    set_backbone_trainable_timesformer(video_enc.model, trainable=False)\n","\n","    best_audio_f1 = 0.0\n","    best_video_f1 = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print(\"\\n\" + \"=\"*60)\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        print(\"=\"*60)\n","\n","        if epoch == audio_freeze_epochs:\n","            safe_unfreeze_wav2vec_feature_encoder(audio_enc.model)\n","            print(\"→ Unfroze Wav2Vec2/HubERT feature encoder\")\n","        if epoch == video_freeze_epochs:\n","            set_backbone_trainable_timesformer(video_enc.model, trainable=True)\n","            print(\"→ Unfroze TimeSformer backbone\")\n","\n","        a_train = audio_enc.train_epoch(train_loader_audio)\n","        a_val = audio_enc.validate(val_loader_audio)\n","        print(f\"[Audio] Train: loss={a_train['loss']:.4f} acc={a_train['accuracy']:.4f} f1={a_train['f1_score']:.4f}\")\n","        print(f\"[Audio]   Val: loss={a_val['loss']:.4f} acc={a_val['accuracy']:.4f} f1={a_val['f1_score']:.4f}\")\n","\n","        v_train = video_enc.train_epoch(train_loader_video)\n","        v_val = video_enc.validate(val_loader_video)\n","        print(f\"[Video] Train: loss={v_train['loss']:.4f} acc={v_train['accuracy']:.4f} f1={v_train['f1_score']:.4f}\")\n","        print(f\"[Video]   Val: loss={v_val['loss']:.4f} acc={v_val['accuracy']:.4f} f1={v_val['f1_score']:.4f}\")\n","\n","        if WANDB:\n","            wandb.log({\n","                \"epoch\": epoch + 1,\n","                \"audio/train_loss\": a_train[\"loss\"], \"audio/train_acc\": a_train[\"accuracy\"], \"audio/train_f1\": a_train[\"f1_score\"],\n","                \"audio/val_loss\": a_val[\"loss\"], \"audio/val_acc\": a_val[\"accuracy\"], \"audio/val_f1\": a_val[\"f1_score\"],\n","                \"video/train_loss\": v_train[\"loss\"], \"video/train_acc\": v_train[\"accuracy\"], \"video/train_f1\": v_train[\"f1_score\"],\n","                \"video/val_loss\": v_val[\"loss\"], \"video/val_acc\": v_val[\"accuracy\"], \"video/val_f1\": v_val[\"f1_score\"],\n","            })\n","\n","        if a_val[\"f1_score\"] > best_audio_f1:\n","            best_audio_f1 = a_val[\"f1_score\"]\n","            save_path = Path(output_dir) / \"best_audio_encoder\"\n","            audio_enc.model.save_pretrained(str(save_path))\n","            print(f\"✓ Saved best audio encoder → {save_path} (F1={best_audio_f1:.4f})\")\n","\n","        if v_val[\"f1_score\"] > best_video_f1:\n","            best_video_f1 = v_val[\"f1_score\"]\n","            save_path = Path(output_dir) / \"best_video_encoder\"\n","            video_enc.model.save_pretrained(str(save_path))\n","            print(f\"✓ Saved best video encoder → {save_path} (F1={best_video_f1:.4f})\")\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Training complete!\")\n","    print(f\"Best Audio F1: {best_audio_f1:.4f} | Best Video F1: {best_video_f1:.4f}\")\n","    print(\"=\"*60)\n","\n","    if WANDB:\n","        wandb.finish()\n","\n","    return audio_enc, video_enc, best_audio_f1, best_video_f1\n","\n","\n"],"metadata":{"id":"wjQCo0uJMhqK","executionInfo":{"status":"ok","timestamp":1762454463980,"user_tz":420,"elapsed":12243,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","if __name__ == \"__main__\":\n","    train_encoders(\n","        metadata_path=\"/content/processed_data/metadata.json\",\n","        output_dir=\"/content/trained_encoders\",\n","        audio_model=\"superb/wav2vec2-base-superb-er\",\n","        video_model=\"facebook/timesformer-base-finetuned-k400\",\n","        num_epochs=20,\n","        batch_size=4,\n","        val_split=0.2,\n","        audio_window_s=1.5,\n","        video_Ts=16,\n","        video_max_frames=64,\n","        use_wandb=True,\n","        seed=42,\n","        audio_freeze_epochs=2,\n","        video_freeze_epochs=1,\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e1fc260b2d414be0a8758b203c2b45e8","da81dc74e7f5428d8b9b550577407317","e371f844914c483ea464680c293db45a","ea8d581dbc3443708f0d3525012577ed","6d43442536b04cf485a14f1aabdb0ecd","b808845936cd4c29ba0cd84b27d37030","c84f5201eef44a6dbade2e667d86b411","8da47976c3a94c6b8df7fe97d8595713","20bc4c21f8ea498b8b47c938c86d00b7","c5bcd1ce96db40fe8d43b2424d3e0183","f62b280d37a84942b071c114876db9e7","7eed4856f88d40f2bce636e0ab569630","682ab862aef94ce08c030769aba7e8cc","a2dc3bfec50f4598afe868d96b853e4a","bc9f7bdb88d74b24a21ebe5813253ac0","396f3ea2b5344ff1a3ce48d421283b18","fd161b5c1b1741fa80eac0694ab6663f","4900d870b686459197115df6bda63a8e","77ba62d0b62b45a8b42723401aa83bd9","52082a2cd0bf4b798b03b8f6a6aa6f5f","5a04687509da4b39945805c55383671f","961e2df81daf42ac85ee0bcaac0c9a3d","c4f7c705e9124369910bb22523be769e","366747f7ac144864acf9a9221ecf45cc","0f6b64f385874bb29e13ce640f7014c5","513cd8bd1cd24f1e99a758c20c9935cf","b5b2a74c8c374e4f872b12631066e0b1","e99c1d38ea53413d91fdb54aa03c28e6","64af70b73e764a48a472215897620a5d","369f41e7447741cead9910b4fea0a38f","99fbd420361f46f89db21fbcd25b0785","4142c875ed8b4d7881d2d5d9b6c4fb15","ff9d2a1a20e54ea5b83c2cea543a8b5e","7523b3c7c2694dc497c74e42ad604548","3c12e2258fd44ca092b423d02a66e14f","20ab473d85664fae867d82929b3f0be5","e39f44b1ae1d4de39cf390b0f5453303","56e9fd4f07ce46c0afe8fed8408e190a","86c84ab87e4f4dea81caa9383041137d","d7ce31e7fac841dd9407d4360bd994b7","9178c248258144c3afb44d0967a91641","be788368956f48d1a9e68500a2ab3d43","a3d30d9d2c764168a249da288fb7e656","52763c7d962e459ba3818bb274ad46cd","fefe1b95498f424ab1bbb38521aa886b","7201fa4bdd7144349d42da26c0723224","7ee0760f82a049f4bb20e0fcc08e4a2e","5fb2c8315344499fbed75f490491a676","42622d0f6ee546e2b3165699c0dc2a49","5b886939616b486e94089d1132a3f58d","b057040ff96d45b6893f4c4ba2a1020a","55470e2e7a0348d6932a56963e065db0","e2fe4a2745f941bfabdbfcdb568b97ec","f2503143d92841178cdd83fcfb3cb5e7","ca30038206764771b32b379b76572828","4096ce9c56fa40c3b3ff70204fd18bd5","926f495ca90a484eab7b671146471add","d10d552123684380939359d7939e7faf","5aee0926f9884a8e989038c03c170588","41f8a8cf4cb2462b859f9f0322eb22df","ffbca58dfdd14f2099211571f1f2fc47","0e21be22a213445a8ffdc3626b284bcb","71faddbd55814745a66730e5f5a32b21","f25720c173284d40b4a06b70c19a2664","e9528c887b4446b7910cdcc29bf80e72","39a20c1d6dfb4158af700ec2b7d2321a","b651a344d132481b95450d95b64bd21f","6a2f670c97074778aaa44b072a430eb1","8c55f34f2fe5483a803fa3b5563668c2","af86b7f9ea6e483e99a4f5d42f65def1","8fb6706ac2384b238e5e9f0f356261bc","b64eb91117cb40d9b9a932073a17c2f7","a308fc29ddd04de69c2cae34a463a47e","9bc46dd329e6400e8bdf78f0007b7d41","753c318e418a407ebf22b660a70c632a","4366fdc9c21e4d0eb23c136b8d60c0b6","0487550a8a2b424ebeedb47bff336145","46cbf4e2bae3473b872509352ff26e6a","7208800319e84ac28d841183f9c0f212","7e4a7d0f1bac4ff0a9187e45b9856c63","5f015c7151254666b5620b260abb2dbc","2c574ee7b6974d9880dc95fa22a99a0d","b98b26eda5b94a088ce153ebfe463cd4","1e335b05b82641a3a8bef8a906a1b4f9","e0dc20ad94fb44cdb221642c943dd881","2ee543cdeb294800bbc04cdb75535898","c403f1fb6ba347469e1d587fa1cf5b36","d97c2a6755b64543b65599ee76e9b7f0"]},"id":"TV4ShT-WMzoa","outputId":"28215e71-efea-4d94-e339-ec631573e65b","executionInfo":{"status":"ok","timestamp":1762457325746,"user_tz":420,"elapsed":2861748,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n","  | |_| | '_ \\/ _` / _` |  _/ -_)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","wandb: Paste an API key from your profile and hit enter:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkatrinpochtar\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.22.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251106_184139-zv312fug</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/katrinpochtar/almost-human-encoders/runs/zv312fug' target=\"_blank\">chocolate-music-7</a></strong> to <a href='https://wandb.ai/katrinpochtar/almost-human-encoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/katrinpochtar/almost-human-encoders' target=\"_blank\">https://wandb.ai/katrinpochtar/almost-human-encoders</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/katrinpochtar/almost-human-encoders/runs/zv312fug' target=\"_blank\">https://wandb.ai/katrinpochtar/almost-human-encoders/runs/zv312fug</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Device: cuda | Train: 576 | Val: 144\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1fc260b2d414be0a8758b203c2b45e8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eed4856f88d40f2bce636e0ab569630"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-er and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated\n","- classifier.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([8]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4f7c705e9124369910bb22523be769e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7523b3c7c2694dc497c74e42ad604548"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fefe1b95498f424ab1bbb38521aa886b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/486M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4096ce9c56fa40c3b3ff70204fd18bd5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([8]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b651a344d132481b95450d95b64bd21f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/486M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46cbf4e2bae3473b872509352ff26e6a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","Epoch 1/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n","Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","\n","Training (Audio):   1%|          | 1/144 [00:00<02:00,  1.19it/s]\u001b[A\n","Training (Audio):   2%|▏         | 3/144 [00:01<00:39,  3.54it/s]\u001b[A\n","Training (Audio):   3%|▎         | 4/144 [00:01<00:31,  4.50it/s]\u001b[A\n","Training (Audio):   3%|▎         | 5/144 [00:01<00:25,  5.49it/s]\u001b[A\n","Training (Audio):   4%|▍         | 6/144 [00:01<00:21,  6.38it/s]\u001b[A\n","Training (Audio):   5%|▍         | 7/144 [00:01<00:19,  7.10it/s]\u001b[A\n","Training (Audio):   6%|▌         | 8/144 [00:01<00:17,  7.57it/s]\u001b[A\n","Training (Audio):   6%|▋         | 9/144 [00:01<00:16,  8.05it/s]\u001b[A\n","Training (Audio):   7%|▋         | 10/144 [00:01<00:16,  8.21it/s]\u001b[A\n","Training (Audio):   8%|▊         | 11/144 [00:01<00:15,  8.39it/s]\u001b[A\n","Training (Audio):   8%|▊         | 12/144 [00:01<00:15,  8.69it/s]\u001b[A\n","Training (Audio):   9%|▉         | 13/144 [00:02<00:15,  8.71it/s]\u001b[A\n","Training (Audio):  10%|▉         | 14/144 [00:02<00:14,  8.90it/s]\u001b[A\n","Training (Audio):  10%|█         | 15/144 [00:02<00:14,  9.19it/s]\u001b[A\n","Training (Audio):  12%|█▏        | 17/144 [00:02<00:12,  9.89it/s]\u001b[A\n","Training (Audio):  13%|█▎        | 19/144 [00:02<00:11, 10.81it/s]\u001b[A\n","Training (Audio):  15%|█▍        | 21/144 [00:02<00:12, 10.16it/s]\u001b[A\n","Training (Audio):  16%|█▌        | 23/144 [00:03<00:12,  9.85it/s]\u001b[A\n","Training (Audio):  17%|█▋        | 24/144 [00:03<00:12,  9.79it/s]\u001b[A\n","Training (Audio):  17%|█▋        | 25/144 [00:03<00:12,  9.57it/s]\u001b[A\n","Training (Audio):  18%|█▊        | 26/144 [00:03<00:12,  9.26it/s]\u001b[A\n","Training (Audio):  19%|█▉        | 27/144 [00:03<00:12,  9.42it/s]\u001b[A\n","Training (Audio):  19%|█▉        | 28/144 [00:03<00:12,  9.45it/s]\u001b[A\n","Training (Audio):  21%|██        | 30/144 [00:03<00:10, 10.52it/s]\u001b[A\n","Training (Audio):  22%|██▏       | 32/144 [00:03<00:10, 10.42it/s]\u001b[A\n","Training (Audio):  24%|██▎       | 34/144 [00:04<00:10, 10.59it/s]\u001b[A\n","Training (Audio):  25%|██▌       | 36/144 [00:04<00:10, 10.74it/s]\u001b[A\n","Training (Audio):  26%|██▋       | 38/144 [00:04<00:09, 10.93it/s]\u001b[A\n","Training (Audio):  28%|██▊       | 40/144 [00:04<00:09, 10.62it/s]\u001b[A\n","Training (Audio):  29%|██▉       | 42/144 [00:04<00:09, 10.27it/s]\u001b[A\n","Training (Audio):  31%|███       | 44/144 [00:05<00:09, 10.90it/s]\u001b[A\n","Training (Audio):  32%|███▏      | 46/144 [00:05<00:08, 11.17it/s]\u001b[A\n","Training (Audio):  33%|███▎      | 48/144 [00:05<00:08, 11.77it/s]\u001b[A\n","Training (Audio):  35%|███▍      | 50/144 [00:05<00:08, 11.74it/s]\u001b[A\n","Training (Audio):  36%|███▌      | 52/144 [00:05<00:08, 11.32it/s]\u001b[A\n","Training (Audio):  38%|███▊      | 54/144 [00:05<00:08, 10.77it/s]\u001b[A\n","Training (Audio):  39%|███▉      | 56/144 [00:06<00:08, 10.68it/s]\u001b[A\n","Training (Audio):  40%|████      | 58/144 [00:06<00:07, 11.33it/s]\u001b[A\n","Training (Audio):  42%|████▏     | 60/144 [00:06<00:07, 11.61it/s]\u001b[A\n","Training (Audio):  43%|████▎     | 62/144 [00:06<00:07, 11.71it/s]\u001b[A\n","Training (Audio):  44%|████▍     | 64/144 [00:06<00:06, 11.99it/s]\u001b[A\n","Training (Audio):  46%|████▌     | 66/144 [00:07<00:06, 11.34it/s]\u001b[A\n","Training (Audio):  47%|████▋     | 68/144 [00:07<00:06, 11.10it/s]\u001b[A\n","Training (Audio):  49%|████▊     | 70/144 [00:07<00:06, 10.70it/s]\u001b[A\n","Training (Audio):  50%|█████     | 72/144 [00:07<00:07, 10.24it/s]\u001b[A\n","Training (Audio):  51%|█████▏    | 74/144 [00:07<00:07,  9.94it/s]\u001b[A\n","Training (Audio):  53%|█████▎    | 76/144 [00:08<00:06, 10.14it/s]\u001b[A\n","Training (Audio):  54%|█████▍    | 78/144 [00:08<00:06, 10.04it/s]\u001b[A\n","Training (Audio):  56%|█████▌    | 80/144 [00:08<00:06, 10.13it/s]\u001b[A\n","Training (Audio):  57%|█████▋    | 82/144 [00:08<00:06, 10.05it/s]\u001b[A\n","Training (Audio):  58%|█████▊    | 84/144 [00:08<00:05, 10.20it/s]\u001b[A\n","Training (Audio):  60%|█████▉    | 86/144 [00:09<00:05,  9.94it/s]\u001b[A\n","Training (Audio):  60%|██████    | 87/144 [00:09<00:05,  9.89it/s]\u001b[A\n","Training (Audio):  62%|██████▏   | 89/144 [00:09<00:05, 10.33it/s]\u001b[A\n","Training (Audio):  63%|██████▎   | 91/144 [00:09<00:04, 10.80it/s]\u001b[A\n","Training (Audio):  65%|██████▍   | 93/144 [00:09<00:04, 10.65it/s]\u001b[A\n","Training (Audio):  66%|██████▌   | 95/144 [00:09<00:04, 10.63it/s]\u001b[A\n","Training (Audio):  67%|██████▋   | 97/144 [00:10<00:04, 11.26it/s]\u001b[A\n","Training (Audio):  69%|██████▉   | 99/144 [00:10<00:03, 11.36it/s]\u001b[A\n","Training (Audio):  70%|███████   | 101/144 [00:10<00:03, 10.92it/s]\u001b[A\n","Training (Audio):  72%|███████▏  | 103/144 [00:10<00:03, 10.87it/s]\u001b[A\n","Training (Audio):  73%|███████▎  | 105/144 [00:10<00:03, 10.65it/s]\u001b[A\n","Training (Audio):  74%|███████▍  | 107/144 [00:10<00:03, 10.74it/s]\u001b[A\n","Training (Audio):  76%|███████▌  | 109/144 [00:11<00:03, 10.78it/s]\u001b[A\n","Training (Audio):  77%|███████▋  | 111/144 [00:11<00:02, 11.11it/s]\u001b[A\n","Training (Audio):  78%|███████▊  | 113/144 [00:11<00:02, 10.77it/s]\u001b[A\n","Training (Audio):  80%|███████▉  | 115/144 [00:11<00:02, 11.19it/s]\u001b[A\n","Training (Audio):  81%|████████▏ | 117/144 [00:11<00:02, 11.12it/s]\u001b[A\n","Training (Audio):  83%|████████▎ | 119/144 [00:12<00:02, 11.05it/s]\u001b[A\n","Training (Audio):  84%|████████▍ | 121/144 [00:12<00:02, 10.61it/s]\u001b[A\n","Training (Audio):  85%|████████▌ | 123/144 [00:12<00:01, 10.64it/s]\u001b[A\n","Training (Audio):  87%|████████▋ | 125/144 [00:12<00:01, 10.87it/s]\u001b[A\n","Training (Audio):  88%|████████▊ | 127/144 [00:12<00:01, 10.55it/s]\u001b[A\n","Training (Audio):  90%|████████▉ | 129/144 [00:12<00:01, 10.67it/s]\u001b[A\n","Training (Audio):  91%|█████████ | 131/144 [00:13<00:01, 11.54it/s]\u001b[A\n","Training (Audio):  92%|█████████▏| 133/144 [00:13<00:00, 12.15it/s]\u001b[A\n","Training (Audio):  94%|█████████▍| 135/144 [00:13<00:00, 12.97it/s]\u001b[A\n","Training (Audio):  95%|█████████▌| 137/144 [00:13<00:00, 12.75it/s]\u001b[A\n","Training (Audio):  97%|█████████▋| 139/144 [00:13<00:00, 11.74it/s]\u001b[A\n","Training (Audio):  98%|█████████▊| 141/144 [00:13<00:00, 11.46it/s]\u001b[A\n","Training (Audio): 100%|██████████| 144/144 [00:14<00:00, 10.13it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 32.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=2.0724 acc=0.1198 f1=0.0957\n","[Audio]   Val: loss=2.0573 acc=0.2569 f1=0.1931\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:16<00:00,  1.89it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:19<00:00,  1.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=2.1382 acc=0.1719 f1=0.1440\n","[Video]   Val: loss=2.0686 acc=0.1667 f1=0.1471\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.1931)\n","✓ Saved best video encoder → /content/trained_encoders/best_video_encoder (F1=0.1471)\n","\n","============================================================\n","Epoch 2/20\n","============================================================\n","→ Unfroze TimeSformer backbone\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 13.37it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 32.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=2.0160 acc=0.2465 f1=0.1710\n","[Audio]   Val: loss=1.9726 acc=0.2778 f1=0.2073\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:49<00:00,  1.32it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:18<00:00,  1.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=1.5132 acc=0.4462 f1=0.4366\n","[Video]   Val: loss=0.9649 acc=0.6736 f1=0.6420\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.2073)\n","✓ Saved best video encoder → /content/trained_encoders/best_video_encoder (F1=0.6420)\n","\n","============================================================\n","Epoch 3/20\n","============================================================\n","→ Unfroze Wav2Vec2/HubERT feature encoder\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 13.63it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 32.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.8947 acc=0.2726 f1=0.2019\n","[Audio]   Val: loss=1.8493 acc=0.2917 f1=0.2015\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:48<00:00,  1.33it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:18<00:00,  1.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.7846 acc=0.7222 f1=0.7202\n","[Video]   Val: loss=0.8175 acc=0.6806 f1=0.6406\n","\n","============================================================\n","Epoch 4/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:14<00:00, 10.21it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 32.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.7981 acc=0.3021 f1=0.2047\n","[Audio]   Val: loss=1.7539 acc=0.3264 f1=0.2191\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:48<00:00,  1.32it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:18<00:00,  1.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.5224 acc=0.8281 f1=0.8287\n","[Video]   Val: loss=0.5964 acc=0.7917 f1=0.7812\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.2191)\n","✓ Saved best video encoder → /content/trained_encoders/best_video_encoder (F1=0.7812)\n","\n","============================================================\n","Epoch 5/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 13.72it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 32.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.6895 acc=0.3542 f1=0.2483\n","[Audio]   Val: loss=1.6479 acc=0.3403 f1=0.2446\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:49<00:00,  1.32it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:18<00:00,  1.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.4221 acc=0.8490 f1=0.8468\n","[Video]   Val: loss=0.4022 acc=0.8194 f1=0.8172\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.2446)\n","✓ Saved best video encoder → /content/trained_encoders/best_video_encoder (F1=0.8172)\n","\n","============================================================\n","Epoch 6/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 13.61it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 32.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.6300 acc=0.3819 f1=0.2900\n","[Audio]   Val: loss=1.5767 acc=0.3403 f1=0.2279\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:43<00:00,  1.39it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:17<00:00,  2.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.3024 acc=0.9045 f1=0.9048\n","[Video]   Val: loss=0.5522 acc=0.8194 f1=0.8139\n","\n","============================================================\n","Epoch 7/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 13.94it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 32.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.5420 acc=0.4045 f1=0.3047\n","[Audio]   Val: loss=1.5965 acc=0.3403 f1=0.2322\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:44<00:00,  1.38it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:17<00:00,  2.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.2273 acc=0.9288 f1=0.9292\n","[Video]   Val: loss=0.5012 acc=0.8125 f1=0.8136\n","\n","============================================================\n","Epoch 8/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 13.81it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 31.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.5061 acc=0.4271 f1=0.3308\n","[Audio]   Val: loss=1.5013 acc=0.4028 f1=0.2933\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:44<00:00,  1.38it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:17<00:00,  2.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.2099 acc=0.9306 f1=0.9313\n","[Video]   Val: loss=0.5110 acc=0.8333 f1=0.8274\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.2933)\n","✓ Saved best video encoder → /content/trained_encoders/best_video_encoder (F1=0.8274)\n","\n","============================================================\n","Epoch 9/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:13<00:00, 10.66it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 31.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.4045 acc=0.4878 f1=0.3934\n","[Audio]   Val: loss=1.4232 acc=0.4028 f1=0.2931\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:51<00:00,  1.29it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:18<00:00,  1.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.2221 acc=0.9236 f1=0.9237\n","[Video]   Val: loss=0.4005 acc=0.8958 f1=0.8955\n","✓ Saved best video encoder → /content/trained_encoders/best_video_encoder (F1=0.8955)\n","\n","============================================================\n","Epoch 10/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 13.85it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 32.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.3594 acc=0.4931 f1=0.4068\n","[Audio]   Val: loss=1.3871 acc=0.4167 f1=0.3015\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:49<00:00,  1.32it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:19<00:00,  1.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.1794 acc=0.9549 f1=0.9550\n","[Video]   Val: loss=0.4950 acc=0.8472 f1=0.8481\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.3015)\n","\n","============================================================\n","Epoch 11/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:09<00:00, 14.54it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 31.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.2497 acc=0.5729 f1=0.4938\n","[Audio]   Val: loss=1.2355 acc=0.5486 f1=0.4828\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:49<00:00,  1.32it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:18<00:00,  1.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.1456 acc=0.9583 f1=0.9583\n","[Video]   Val: loss=0.4338 acc=0.8819 f1=0.8800\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.4828)\n","\n","============================================================\n","Epoch 12/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:13<00:00, 10.93it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 31.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.2249 acc=0.5955 f1=0.5304\n","[Audio]   Val: loss=1.2299 acc=0.5903 f1=0.5289\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:51<00:00,  1.30it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:19<00:00,  1.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.0970 acc=0.9809 f1=0.9809\n","[Video]   Val: loss=0.3225 acc=0.8819 f1=0.8809\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.5289)\n","\n","============================================================\n","Epoch 13/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:09<00:00, 14.49it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 32.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.0988 acc=0.6302 f1=0.5657\n","[Audio]   Val: loss=1.1848 acc=0.5625 f1=0.4912\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:51<00:00,  1.30it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:19<00:00,  1.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.1122 acc=0.9618 f1=0.9619\n","[Video]   Val: loss=0.5836 acc=0.8333 f1=0.8281\n","\n","============================================================\n","Epoch 14/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 14.26it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 32.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.0982 acc=0.6267 f1=0.5727\n","[Audio]   Val: loss=1.3265 acc=0.5347 f1=0.4576\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:50<00:00,  1.30it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:20<00:00,  1.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.1476 acc=0.9618 f1=0.9624\n","[Video]   Val: loss=0.6223 acc=0.8264 f1=0.8239\n","\n","============================================================\n","Epoch 15/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 14.07it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 31.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=1.0496 acc=0.6597 f1=0.6133\n","[Audio]   Val: loss=1.1054 acc=0.5972 f1=0.5367\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:51<00:00,  1.29it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:19<00:00,  1.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.0956 acc=0.9740 f1=0.9739\n","[Video]   Val: loss=0.5583 acc=0.8681 f1=0.8659\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.5367)\n","\n","============================================================\n","Epoch 16/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 14.04it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 31.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=0.9645 acc=0.6927 f1=0.6651\n","[Audio]   Val: loss=1.0109 acc=0.7083 f1=0.6989\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:50<00:00,  1.30it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:19<00:00,  1.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.1229 acc=0.9670 f1=0.9670\n","[Video]   Val: loss=0.6023 acc=0.8750 f1=0.8739\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.6989)\n","\n","============================================================\n","Epoch 17/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:09<00:00, 15.16it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 33.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=0.8948 acc=0.7274 f1=0.7021\n","[Audio]   Val: loss=0.9164 acc=0.7222 f1=0.7181\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:49<00:00,  1.31it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:19<00:00,  1.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.0797 acc=0.9774 f1=0.9775\n","[Video]   Val: loss=0.6001 acc=0.8403 f1=0.8374\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.7181)\n","\n","============================================================\n","Epoch 18/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 13.79it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 31.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=0.8822 acc=0.7378 f1=0.7227\n","[Audio]   Val: loss=0.9148 acc=0.7292 f1=0.7253\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:51<00:00,  1.29it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:19<00:00,  1.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.1032 acc=0.9740 f1=0.9740\n","[Video]   Val: loss=0.5703 acc=0.8611 f1=0.8609\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.7253)\n","\n","============================================================\n","Epoch 19/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 13.79it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 33.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=0.8069 acc=0.7726 f1=0.7607\n","[Audio]   Val: loss=0.9015 acc=0.7778 f1=0.7791\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:49<00:00,  1.32it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:18<00:00,  1.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.0678 acc=0.9826 f1=0.9826\n","[Video]   Val: loss=0.6097 acc=0.8681 f1=0.8636\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.7791)\n","\n","============================================================\n","Epoch 20/20\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training (Audio):   0%|          | 0/144 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","Training (Audio): 100%|██████████| 144/144 [00:10<00:00, 14.29it/s]\n","Validation (Audio): 100%|██████████| 36/36 [00:01<00:00, 33.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Audio] Train: loss=0.7226 acc=0.7882 f1=0.7777\n","[Audio]   Val: loss=0.8005 acc=0.7847 f1=0.7864\n"]},{"output_type":"stream","name":"stderr","text":["Training (Video): 100%|██████████| 144/144 [01:52<00:00,  1.28it/s]\n","Validation (Video): 100%|██████████| 36/36 [00:20<00:00,  1.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Video] Train: loss=0.0580 acc=0.9878 f1=0.9879\n","[Video]   Val: loss=0.4600 acc=0.8750 f1=0.8753\n","✓ Saved best audio encoder → /content/trained_encoders/best_audio_encoder (F1=0.7864)\n","\n","============================================================\n","Training complete!\n","Best Audio F1: 0.7864 | Best Video F1: 0.8955\n","============================================================\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>audio/train_acc</td><td>▁▂▃▃▃▄▄▄▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>audio/train_f1</td><td>▁▂▂▂▃▃▃▃▄▄▅▅▆▆▆▇▇▇██</td></tr><tr><td>audio/train_loss</td><td>██▇▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>audio/val_acc</td><td>▁▁▁▂▂▂▂▃▃▃▅▅▅▅▆▇▇▇██</td></tr><tr><td>audio/val_f1</td><td>▁▁▁▁▂▁▁▂▂▂▄▅▅▄▅▇▇▇██</td></tr><tr><td>audio/val_loss</td><td>██▇▆▆▅▅▅▄▄▃▃▃▄▃▂▂▂▂▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>video/train_acc</td><td>▁▃▆▇▇▇▇█▇███████████</td></tr><tr><td>video/train_f1</td><td>▁▃▆▇▇▇██▇███████████</td></tr><tr><td>video/train_loss</td><td>█▆▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>audio/train_acc</td><td>0.78819</td></tr><tr><td>audio/train_f1</td><td>0.77765</td></tr><tr><td>audio/train_loss</td><td>0.72265</td></tr><tr><td>audio/val_acc</td><td>0.78472</td></tr><tr><td>audio/val_f1</td><td>0.78639</td></tr><tr><td>audio/val_loss</td><td>0.80054</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>video/train_acc</td><td>0.98785</td></tr><tr><td>video/train_f1</td><td>0.98786</td></tr><tr><td>video/train_loss</td><td>0.05797</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">chocolate-music-7</strong> at: <a href='https://wandb.ai/katrinpochtar/almost-human-encoders/runs/zv312fug' target=\"_blank\">https://wandb.ai/katrinpochtar/almost-human-encoders/runs/zv312fug</a><br> View project at: <a href='https://wandb.ai/katrinpochtar/almost-human-encoders' target=\"_blank\">https://wandb.ai/katrinpochtar/almost-human-encoders</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251106_184139-zv312fug/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"1BJFxUzKM29Y","executionInfo":{"status":"ok","timestamp":1762457325751,"user_tz":420,"elapsed":2,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}}},"execution_count":7,"outputs":[]}]}