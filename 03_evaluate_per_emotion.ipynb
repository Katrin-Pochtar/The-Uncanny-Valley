{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.amp import autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from transformers import (\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    HubertForSequenceClassification,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    AutoImageProcessor,\n",
        "    TimesformerForVideoClassification,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "METADATA = \"/content/processed_data/metadata.json\"\n",
        "ENCODERS_DIR = Path(\"/content/trained_encoders\")\n",
        "\n",
        "EMOTIONS = [\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprised\"]\n",
        "\n",
        "BEST_AUDIO = \"hubert-lr5e5-w3s\"\n",
        "BEST_VIDEO = \"timesformer-lr3e5-16f\"\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, metadata_path: str, split: str, modality: str):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data if s[\"split\"] == split]\n",
        "        self.modality = modality\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        item = {\"emotion\": s[\"emotion_idx\"]}\n",
        "        if self.modality == \"audio\":\n",
        "            wav, _ = torchaudio.load(s[\"audio_path\"])\n",
        "            item[\"audio\"] = wav.squeeze(0)\n",
        "        elif self.modality == \"video\":\n",
        "            frames = np.load(s[\"frames_path\"])\n",
        "            item[\"video\"] = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0\n",
        "        return item\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    out = {\"emotion\": torch.tensor([b[\"emotion\"] for b in batch])}\n",
        "    if \"audio\" in batch[0]:\n",
        "        out[\"audio\"] = [b[\"audio\"] for b in batch]\n",
        "    if \"video\" in batch[0]:\n",
        "        out[\"video\"] = torch.stack([b[\"video\"] for b in batch])\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crop_audio(wav, sr, duration):\n",
        "    L = int(round(duration * sr))\n",
        "    n = wav.numel()\n",
        "    if n <= L:\n",
        "        return torch.nn.functional.pad(wav, (0, L - n))\n",
        "    start = (n - L) // 2\n",
        "    return wav[start:start + L]\n",
        "\n",
        "\n",
        "def crop_video(video, n_frames):\n",
        "    T = video.shape[0]\n",
        "    if T <= n_frames:\n",
        "        idx = torch.linspace(0, T - 1, n_frames).round().long()\n",
        "        return video[idx]\n",
        "    start = (T - n_frames) // 2\n",
        "    return video[start:start + n_frames]\n",
        "\n",
        "\n",
        "def prepare_audio(batch, processor, window_s, device):\n",
        "    sr = 16000\n",
        "    wavs = [crop_audio(a, sr, window_s).numpy() for a in batch[\"audio\"]]\n",
        "    enc = processor(wavs, sampling_rate=sr, return_tensors=\"pt\", padding=True,\n",
        "                    truncation=True, max_length=int(window_s * sr))\n",
        "    kwargs = {\"input_values\": enc[\"input_values\"].to(device)}\n",
        "    if \"attention_mask\" in enc:\n",
        "        kwargs[\"attention_mask\"] = enc[\"attention_mask\"].to(device)\n",
        "    return kwargs, batch[\"emotion\"].to(device)\n",
        "\n",
        "\n",
        "def prepare_video(batch, processor, n_frames, device):\n",
        "    clips = []\n",
        "    for v in batch[\"video\"]:\n",
        "        clip = crop_video(v, n_frames)\n",
        "        clips.append([clip[i].permute(1, 2, 0).numpy() for i in range(clip.shape[0])])\n",
        "    enc = processor(clips, return_tensors=\"pt\", do_rescale=False)\n",
        "    return {\"pixel_values\": enc[\"pixel_values\"].to(device)}, batch[\"emotion\"].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def collect_predictions(model, loader, prep_fn):\n",
        "    model.eval()\n",
        "    all_preds, all_labels, all_losses = [], [], []\n",
        "    for batch in loader:\n",
        "        kwargs, y = prep_fn(batch)\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            logits = model(**kwargs).logits\n",
        "            loss = nn.CrossEntropyLoss(reduction=\"none\")(logits, y)\n",
        "        all_preds.extend(logits.argmax(1).cpu().tolist())\n",
        "        all_labels.extend(y.cpu().tolist())\n",
        "        all_losses.extend(loss.cpu().tolist())\n",
        "    return np.array(all_preds), np.array(all_labels), np.array(all_losses)\n",
        "\n",
        "\n",
        "def per_emotion_report(preds, labels, losses, title):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"  {title}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    print(classification_report(\n",
        "        labels, preds, target_names=EMOTIONS, digits=3, zero_division=0))\n",
        "\n",
        "    cm = confusion_matrix(labels, preds, labels=list(range(len(EMOTIONS))))\n",
        "\n",
        "    print(f\"\\n{'Emotion':<12s} {'N':>5s} {'Acc':>6s} {'F1':>6s} {'AvgLoss':>8s}\")\n",
        "    print(\"-\" * 40)\n",
        "    per_f1 = f1_score(labels, preds, average=None, labels=list(range(len(EMOTIONS))), zero_division=0)\n",
        "    for i, emo in enumerate(EMOTIONS):\n",
        "        mask = labels == i\n",
        "        n = mask.sum()\n",
        "        if n == 0:\n",
        "            continue\n",
        "        acc = (preds[mask] == i).mean()\n",
        "        avg_loss = losses[mask].mean()\n",
        "        print(f\"{emo:<12s} {n:5d} {acc:6.3f} {per_f1[i]:6.3f} {avg_loss:8.3f}\")\n",
        "\n",
        "    worst = np.argsort(per_f1)\n",
        "    print(f\"\\nWeakest emotions (by F1): \", end=\"\")\n",
        "    print(\" < \".join(f\"{EMOTIONS[i]} ({per_f1[i]:.3f})\" for i in worst[:3]))\n",
        "\n",
        "    return cm, per_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, title):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(9, 7))\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1\n",
        "    cm_norm = cm / row_sums\n",
        "\n",
        "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
        "                xticklabels=EMOTIONS, yticklabels=EMOTIONS, ax=ax,\n",
        "                vmin=0, vmax=1, linewidths=0.5)\n",
        "\n",
        "    for i in range(len(EMOTIONS)):\n",
        "        for j in range(len(EMOTIONS)):\n",
        "            if cm[i, j] > 0:\n",
        "                ax.text(j + 0.5, i + 0.72, f\"({cm[i,j]})\",\n",
        "                        ha=\"center\", va=\"center\", fontsize=7, color=\"gray\")\n",
        "\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    ax.set_title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_audio = EmotionDataset(METADATA, \"val\", \"audio\")\n",
        "audio_loader = DataLoader(val_audio, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "model_path = str(ENCODERS_DIR / BEST_AUDIO)\n",
        "audio_model = HubertForSequenceClassification.from_pretrained(model_path).to(DEVICE)\n",
        "audio_processor = Wav2Vec2FeatureExtractor.from_pretrained(model_path)\n",
        "audio_prep = partial(prepare_audio, processor=audio_processor, window_s=3.0, device=DEVICE)\n",
        "\n",
        "print(f\"Loaded: {BEST_AUDIO}\")\n",
        "print(f\"Val samples: {len(val_audio)}\")\n",
        "\n",
        "a_preds, a_labels, a_losses = collect_predictions(audio_model, audio_loader, audio_prep)\n",
        "a_cm, a_f1 = per_emotion_report(a_preds, a_labels, a_losses, f\"AUDIO — {BEST_AUDIO}\")\n",
        "plot_confusion_matrix(a_cm, f\"Audio Confusion Matrix — {BEST_AUDIO}\")\n",
        "\n",
        "del audio_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_video = EmotionDataset(METADATA, \"val\", \"video\")\n",
        "video_loader = DataLoader(val_video, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "model_path = str(ENCODERS_DIR / BEST_VIDEO)\n",
        "video_model = TimesformerForVideoClassification.from_pretrained(model_path).to(DEVICE)\n",
        "video_processor = AutoImageProcessor.from_pretrained(model_path)\n",
        "video_prep = partial(prepare_video, processor=video_processor, n_frames=16, device=DEVICE)\n",
        "\n",
        "print(f\"Loaded: {BEST_VIDEO}\")\n",
        "print(f\"Val samples: {len(val_video)}\")\n",
        "\n",
        "v_preds, v_labels, v_losses = collect_predictions(video_model, video_loader, video_prep)\n",
        "v_cm, v_f1 = per_emotion_report(v_preds, v_labels, v_losses, f\"VIDEO — {BEST_VIDEO}\")\n",
        "plot_confusion_matrix(v_cm, f\"Video Confusion Matrix — {BEST_VIDEO}\")\n",
        "\n",
        "del video_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "x = np.arange(len(EMOTIONS))\n",
        "w = 0.35\n",
        "\n",
        "ax.bar(x - w/2, a_f1, w, label=f\"Audio ({BEST_AUDIO})\", color=\"#4C72B0\")\n",
        "ax.bar(x + w/2, v_f1, w, label=f\"Video ({BEST_VIDEO})\", color=\"#DD8452\")\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(EMOTIONS, rotation=30, ha=\"right\")\n",
        "ax.set_ylabel(\"F1 Score\")\n",
        "ax.set_title(\"Per-Emotion F1: Audio vs Video Encoder\")\n",
        "ax.set_ylim(0, 1)\n",
        "ax.legend()\n",
        "ax.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "for i in x:\n",
        "    ax.text(i - w/2, a_f1[i] + 0.02, f\"{a_f1[i]:.2f}\", ha=\"center\", fontsize=8)\n",
        "    ax.text(i + w/2, v_f1[i] + 0.02, f\"{v_f1[i]:.2f}\", ha=\"center\", fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CROSS-MODAL EMOTION GAPS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Emotion':<12s} {'Audio F1':>9s} {'Video F1':>9s} {'Gap':>7s}\")\n",
        "print(\"-\" * 40)\n",
        "gaps = []\n",
        "for i, emo in enumerate(EMOTIONS):\n",
        "    gap = abs(a_f1[i] - v_f1[i])\n",
        "    better = \"A\" if a_f1[i] > v_f1[i] else \"V\"\n",
        "    gaps.append((emo, gap, better))\n",
        "    print(f\"{emo:<12s} {a_f1[i]:9.3f} {v_f1[i]:9.3f} {gap:7.3f} ({better})\")\n",
        "\n",
        "print(f\"\\nLargest cross-modal gaps:\")\n",
        "for emo, gap, better in sorted(gaps, key=lambda x: -x[1])[:3]:\n",
        "    print(f\"  {emo}: {gap:.3f} ({'audio' if better == 'A' else 'video'} stronger)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
