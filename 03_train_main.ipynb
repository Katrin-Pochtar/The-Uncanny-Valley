{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1FPzR2--oEtd1sVvm86CHwgLUYHcU5TLT","timestamp":1762236733157},{"file_id":"1k8C6avk0r2UN_W8EU0hH59dGI4crelgg","timestamp":1762145451486}],"authorship_tag":"ABX9TyNlkih4LLDLXbxrppOVl4Ud"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Cross-Modal Emotion Loss Module\n","\n","This module implements the core novelty of the research:\n","Ensuring emotional consistency between audio (voice) and video (facial expressions)\n","in talking face generation.\n","\n","The loss penalizes emotional misalignment between modalities.\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from typing import Dict, Optional, Literal, Tuple\n","import numpy as np\n","\n","\n","class CrossModalEmotionLoss(nn.Module):\n","    \"\"\"\n","    Cross-modal emotion loss that enforces emotional consistency\n","    between audio and video modalities.\n","\n","    Supports multiple loss types:\n","    - 'cosine': Cosine similarity between embeddings\n","    - 'mse': Mean squared error between embeddings\n","    - 'kl': KL divergence between emotion distributions\n","    - 'ce': Cross-entropy using predicted classes\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        loss_type: Literal['cosine', 'mse', 'kl', 'ce'] = 'cosine',\n","        temperature: float = 0.07,\n","        weight: float = 1.0,\n","        normalize_embeddings: bool = True,\n","    ):\n","        \"\"\"\n","        Args:\n","            loss_type: Type of loss function\n","                - 'cosine': 1 - cosine_similarity (encourages similar direction)\n","                - 'mse': Mean squared error (encourages similar magnitude)\n","                - 'kl': KL divergence (for probability distributions)\n","                - 'ce': Cross-entropy (for discrete emotion matching)\n","            temperature: Temperature for scaling similarities (for 'kl')\n","            weight: Weight multiplier for the loss\n","            normalize_embeddings: Whether to L2-normalize embeddings before comparison\n","        \"\"\"\n","        super().__init__()\n","        self.loss_type = loss_type\n","        self.temperature = temperature\n","        self.weight = weight\n","        self.normalize_embeddings = normalize_embeddings\n","\n","    def forward(\n","        self,\n","        audio_embeddings: torch.Tensor,\n","        video_embeddings: torch.Tensor,\n","        audio_logits: Optional[torch.Tensor] = None,\n","        video_logits: Optional[torch.Tensor] = None,\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Compute cross-modal emotion loss.\n","\n","        Args:\n","            audio_embeddings: (B, D) audio emotion embeddings\n","            video_embeddings: (B, D) video emotion embeddings\n","            audio_logits: (B, num_classes) optional logits from audio encoder\n","            video_logits: (B, num_classes) optional logits from video encoder\n","\n","        Returns:\n","            loss: Scalar tensor representing the emotion misalignment\n","        \"\"\"\n","        if self.normalize_embeddings:\n","            audio_embeddings = F.normalize(audio_embeddings, p=2, dim=-1)\n","            video_embeddings = F.normalize(video_embeddings, p=2, dim=-1)\n","\n","        if self.loss_type == 'cosine':\n","            # Cosine similarity loss: 1 - cosine_similarity\n","            # Range: [0, 2], where 0 = perfect alignment, 2 = opposite\n","            cos_sim = F.cosine_similarity(audio_embeddings, video_embeddings, dim=-1)\n","            loss = (1.0 - cos_sim).mean()\n","\n","        elif self.loss_type == 'mse':\n","            # MSE between embeddings\n","            loss = F.mse_loss(audio_embeddings, video_embeddings)\n","\n","        elif self.loss_type == 'kl':\n","            # KL divergence between emotion probability distributions\n","            # Convert embeddings to probabilities via softmax\n","            audio_probs = F.softmax(audio_embeddings / self.temperature, dim=-1)\n","            video_probs = F.softmax(video_embeddings / self.temperature, dim=-1)\n","\n","            # KL(video || audio): how much video differs from audio\n","            loss = F.kl_div(\n","                video_probs.log(),\n","                audio_probs,\n","                reduction='batchmean'\n","            )\n","\n","        elif self.loss_type == 'ce':\n","            # Cross-entropy using predicted emotion classes\n","            if audio_logits is None or video_logits is None:\n","                raise ValueError(\"audio_logits and video_logits required for 'ce' loss\")\n","\n","            # Use audio predictions as pseudo-labels for video\n","            audio_preds = audio_logits.argmax(dim=-1)\n","            video_log_probs = F.log_softmax(video_logits, dim=-1)\n","            loss = F.nll_loss(video_log_probs, audio_preds)\n","\n","        else:\n","            raise ValueError(f\"Unknown loss_type: {self.loss_type}\")\n","\n","        return self.weight * loss\n","\n","\n","class AdaptiveCrossModalEmotionLoss(nn.Module):\n","    \"\"\"\n","    Adaptive cross-modal emotion loss that combines multiple loss types\n","    with learnable weights.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        use_cosine: bool = True,\n","        use_mse: bool = True,\n","        use_kl: bool = False,\n","        temperature: float = 0.07,\n","        initial_weights: Optional[Dict[str, float]] = None,\n","        learnable_weights: bool = True,\n","    ):\n","        \"\"\"\n","        Args:\n","            use_cosine: Whether to include cosine similarity loss\n","            use_mse: Whether to include MSE loss\n","            use_kl: Whether to include KL divergence loss\n","            temperature: Temperature for KL divergence\n","            initial_weights: Initial weights for each loss type\n","            learnable_weights: Whether weights are learnable parameters\n","        \"\"\"\n","        super().__init__()\n","\n","        self.use_cosine = use_cosine\n","        self.use_mse = use_mse\n","        self.use_kl = use_kl\n","        self.temperature = temperature\n","\n","        # Initialize weights\n","        if initial_weights is None:\n","            initial_weights = {\n","                'cosine': 1.0,\n","                'mse': 0.5,\n","                'kl': 0.3,\n","            }\n","\n","        # Create weight parameters\n","        if learnable_weights:\n","            if use_cosine:\n","                self.weight_cosine = nn.Parameter(\n","                    torch.tensor(initial_weights.get('cosine', 1.0))\n","                )\n","            if use_mse:\n","                self.weight_mse = nn.Parameter(\n","                    torch.tensor(initial_weights.get('mse', 0.5))\n","                )\n","            if use_kl:\n","                self.weight_kl = nn.Parameter(\n","                    torch.tensor(initial_weights.get('kl', 0.3))\n","                )\n","        else:\n","            if use_cosine:\n","                self.register_buffer(\n","                    'weight_cosine',\n","                    torch.tensor(initial_weights.get('cosine', 1.0))\n","                )\n","            if use_mse:\n","                self.register_buffer(\n","                    'weight_mse',\n","                    torch.tensor(initial_weights.get('mse', 0.5))\n","                )\n","            if use_kl:\n","                self.register_buffer(\n","                    'weight_kl',\n","                    torch.tensor(initial_weights.get('kl', 0.3))\n","                )\n","\n","    def forward(\n","        self,\n","        audio_embeddings: torch.Tensor,\n","        video_embeddings: torch.Tensor,\n","    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n","        \"\"\"\n","        Compute adaptive cross-modal emotion loss.\n","\n","        Args:\n","            audio_embeddings: (B, D) audio emotion embeddings\n","            video_embeddings: (B, D) video emotion embeddings\n","\n","        Returns:\n","            total_loss: Combined weighted loss\n","            loss_dict: Dictionary of individual loss components\n","        \"\"\"\n","        loss_dict = {}\n","        total_loss = 0.0\n","\n","        # Normalize embeddings\n","        audio_norm = F.normalize(audio_embeddings, p=2, dim=-1)\n","        video_norm = F.normalize(video_embeddings, p=2, dim=-1)\n","\n","        if self.use_cosine:\n","            cos_sim = F.cosine_similarity(audio_norm, video_norm, dim=-1)\n","            loss_cosine = (1.0 - cos_sim).mean()\n","            total_loss = total_loss + self.weight_cosine * loss_cosine\n","            loss_dict['cosine'] = loss_cosine.item()\n","            loss_dict['weight_cosine'] = self.weight_cosine.item()\n","\n","        if self.use_mse:\n","            loss_mse = F.mse_loss(audio_norm, video_norm)\n","            total_loss = total_loss + self.weight_mse * loss_mse\n","            loss_dict['mse'] = loss_mse.item()\n","            loss_dict['weight_mse'] = self.weight_mse.item()\n","\n","        if self.use_kl:\n","            audio_probs = F.softmax(audio_embeddings / self.temperature, dim=-1)\n","            video_probs = F.softmax(video_embeddings / self.temperature, dim=-1)\n","            loss_kl = F.kl_div(\n","                video_probs.log(),\n","                audio_probs,\n","                reduction='batchmean'\n","            )\n","            total_loss = total_loss + self.weight_kl * loss_kl\n","            loss_dict['kl'] = loss_kl.item()\n","            loss_dict['weight_kl'] = self.weight_kl.item()\n","\n","        loss_dict['total'] = total_loss.item()\n","\n","        return total_loss, loss_dict\n","\n","\n","class WindowLevelEmotionLoss(nn.Module):\n","    \"\"\"\n","    Window-level emotion loss for temporal alignment.\n","    Computes emotion loss across multiple temporal windows.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        loss_type: Literal['cosine', 'mse', 'kl'] = 'cosine',\n","        aggregation: Literal['mean', 'max', 'weighted'] = 'mean',\n","        temperature: float = 0.07,\n","        weight: float = 1.0,\n","    ):\n","        \"\"\"\n","        Args:\n","            loss_type: Type of loss function\n","            aggregation: How to aggregate losses across windows\n","                - 'mean': Simple average\n","                - 'max': Maximum loss (focus on worst alignment)\n","                - 'weighted': Weighted by temporal importance\n","            temperature: Temperature for KL divergence\n","            weight: Weight multiplier for the loss\n","        \"\"\"\n","        super().__init__()\n","        self.loss_type = loss_type\n","        self.aggregation = aggregation\n","        self.temperature = temperature\n","        self.weight = weight\n","\n","        # Learnable temporal weights if using weighted aggregation\n","        if aggregation == 'weighted':\n","            self.temporal_weights = nn.Parameter(torch.ones(1))\n","\n","    def forward(\n","        self,\n","        audio_windows: torch.Tensor,\n","        video_windows: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Compute window-level emotion loss.\n","\n","        Args:\n","            audio_windows: (B, T, D) audio embeddings for T windows\n","            video_windows: (B, T, D) video embeddings for T windows\n","\n","        Returns:\n","            loss: Aggregated loss across windows\n","        \"\"\"\n","        B, T, D = audio_windows.shape\n","\n","        # Normalize\n","        audio_norm = F.normalize(audio_windows, p=2, dim=-1)\n","        video_norm = F.normalize(video_windows, p=2, dim=-1)\n","\n","        if self.loss_type == 'cosine':\n","            # Compute cosine similarity per window\n","            cos_sim = F.cosine_similarity(audio_norm, video_norm, dim=-1)  # (B, T)\n","            window_losses = 1.0 - cos_sim  # (B, T)\n","\n","        elif self.loss_type == 'mse':\n","            # MSE per window\n","            window_losses = ((audio_norm - video_norm) ** 2).mean(dim=-1)  # (B, T)\n","\n","        elif self.loss_type == 'kl':\n","            # KL divergence per window\n","            audio_probs = F.softmax(audio_windows / self.temperature, dim=-1)\n","            video_probs = F.softmax(video_windows / self.temperature, dim=-1)\n","\n","            window_losses = (audio_probs * (\n","                audio_probs.log() - video_probs.log()\n","            )).sum(dim=-1)  # (B, T)\n","        else:\n","            raise ValueError(f\"Unknown loss_type: {self.loss_type}\")\n","\n","        # Aggregate across windows\n","        if self.aggregation == 'mean':\n","            loss = window_losses.mean()\n","        elif self.aggregation == 'max':\n","            loss = window_losses.max(dim=-1)[0].mean()\n","        elif self.aggregation == 'weighted':\n","            # Weighted by temporal position (learnable)\n","            weights = F.softmax(self.temporal_weights.expand(T), dim=0)\n","            loss = (window_losses * weights.view(1, -1)).sum(dim=-1).mean()\n","        else:\n","            raise ValueError(f\"Unknown aggregation: {self.aggregation}\")\n","\n","        return self.weight * loss\n","\n","\n","class EmotionAgreementMetric:\n","    \"\"\"\n","    Metric for evaluating emotion agreement between audio and video.\n","    This is your main evaluation metric: Emotion Agreement.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        num_classes: int = 8,\n","        threshold: float = 0.8,  # Cosine similarity threshold\n","    ):\n","        \"\"\"\n","        Args:\n","            num_classes: Number of emotion classes\n","            threshold: Threshold for considering emotions \"aligned\"\n","        \"\"\"\n","        self.num_classes = num_classes\n","        self.threshold = threshold\n","        self.reset()\n","\n","    def reset(self):\n","        \"\"\"Reset metric counters\"\"\"\n","        self.total = 0\n","        self.matches = 0\n","        self.cosine_sims = []\n","        self.audio_preds = []\n","        self.video_preds = []\n","\n","    def update(\n","        self,\n","        audio_embeddings: torch.Tensor,\n","        video_embeddings: torch.Tensor,\n","        audio_logits: Optional[torch.Tensor] = None,\n","        video_logits: Optional[torch.Tensor] = None,\n","    ):\n","        \"\"\"\n","        Update metric with a batch.\n","\n","        Args:\n","            audio_embeddings: (B, D) audio emotion embeddings\n","            video_embeddings: (B, D) video emotion embeddings\n","            audio_logits: (B, num_classes) optional logits\n","            video_logits: (B, num_classes) optional logits\n","        \"\"\"\n","        B = audio_embeddings.shape[0]\n","        self.total += B\n","\n","        # Normalize embeddings\n","        audio_norm = F.normalize(audio_embeddings, p=2, dim=-1)\n","        video_norm = F.normalize(video_embeddings, p=2, dim=-1)\n","\n","        # Compute cosine similarity\n","        cos_sim = F.cosine_similarity(audio_norm, video_norm, dim=-1)\n","        self.cosine_sims.extend(cos_sim.detach().cpu().tolist())\n","\n","        # Count matches based on threshold\n","        matches = (cos_sim >= self.threshold).sum().item()\n","        self.matches += matches\n","\n","        # If logits provided, also track class agreement\n","        if audio_logits is not None and video_logits is not None:\n","            audio_pred = audio_logits.argmax(dim=-1)\n","            video_pred = video_logits.argmax(dim=-1)\n","            self.audio_preds.extend(audio_pred.detach().cpu().tolist())\n","            self.video_preds.extend(video_pred.detach().cpu().tolist())\n","\n","    def compute(self) -> Dict[str, float]:\n","        \"\"\"\n","        Compute final metrics.\n","\n","        Returns:\n","            metrics: Dictionary containing:\n","                - agreement_rate: % of samples with high cosine similarity\n","                - avg_cosine_sim: Average cosine similarity\n","                - class_agreement: % of samples with matching predicted classes\n","        \"\"\"\n","        if self.total == 0:\n","            return {\n","                'agreement_rate': 0.0,\n","                'avg_cosine_sim': 0.0,\n","                'class_agreement': 0.0,\n","            }\n","\n","        metrics = {\n","            'agreement_rate': self.matches / self.total,\n","            'avg_cosine_sim': np.mean(self.cosine_sims),\n","        }\n","\n","        # Class agreement if predictions available\n","        if len(self.audio_preds) > 0:\n","            class_matches = sum(\n","                a == v for a, v in zip(self.audio_preds, self.video_preds)\n","            )\n","            metrics['class_agreement'] = class_matches / len(self.audio_preds)\n","        else:\n","            metrics['class_agreement'] = 0.0\n","\n","        return metrics\n","\n","\n","# Example usage function\n","def compute_emotion_loss_example(\n","    audio_encoder,\n","    video_encoder,\n","    batch: Dict,\n","    loss_fn: CrossModalEmotionLoss,\n","    device: str = 'cuda',\n",") -> Tuple[torch.Tensor, Dict[str, float]]:\n","    \"\"\"\n","    Example function showing how to compute emotion loss during training.\n","\n","    Args:\n","        audio_encoder: Trained audio emotion encoder\n","        video_encoder: Trained video emotion encoder\n","        batch: Batch from dataloader with 'audio' and 'video'\n","        loss_fn: CrossModalEmotionLoss instance\n","        device: Device to use\n","\n","    Returns:\n","        loss: Emotion loss value\n","        metrics: Dictionary with additional metrics\n","    \"\"\"\n","    # Extract embeddings from encoders\n","    audio_embs = audio_encoder.extract_embeddings_clip(\n","        batch['audio'],\n","        sr=batch.get('sample_rate', 16000),\n","        window_seconds=1.5\n","    )\n","\n","    video_embs = video_encoder.extract_embeddings_clip(\n","        batch['video'],\n","        frames_for_model=16\n","    )\n","\n","    # Compute loss\n","    loss = loss_fn(audio_embs, video_embs)\n","\n","    # Compute metrics\n","    with torch.no_grad():\n","        cos_sim = F.cosine_similarity(\n","            F.normalize(audio_embs, p=2, dim=-1),\n","            F.normalize(video_embs, p=2, dim=-1),\n","            dim=-1\n","        )\n","        metrics = {\n","            'emotion_loss': loss.item(),\n","            'avg_cosine_sim': cos_sim.mean().item(),\n","            'min_cosine_sim': cos_sim.min().item(),\n","            'max_cosine_sim': cos_sim.max().item(),\n","        }\n","\n","    return loss, metrics"],"metadata":{"id":"1BJFxUzKM29Y","executionInfo":{"status":"ok","timestamp":1762406668613,"user_tz":420,"elapsed":56,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Example usage\n","    print(\"=\"*60)\n","    print(\"Cross-Modal Emotion Loss - Example Usage\")\n","    print(\"=\"*60)\n","\n","    # Create sample embeddings\n","    batch_size = 4\n","    embed_dim = 768\n","\n","    audio_emb = torch.randn(batch_size, embed_dim)\n","    video_emb = torch.randn(batch_size, embed_dim)\n","\n","    # Test different loss types\n","    loss_types = ['cosine', 'mse', 'kl']\n","\n","    for loss_type in loss_types:\n","        loss_fn = CrossModalEmotionLoss(\n","            loss_type=loss_type,\n","            weight=1.0,\n","            normalize_embeddings=True\n","        )\n","\n","        loss = loss_fn(audio_emb, video_emb)\n","        print(f\"\\n{loss_type.upper()} Loss: {loss.item():.4f}\")\n","\n","    # Test adaptive loss\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Adaptive Cross-Modal Emotion Loss\")\n","    print(\"=\"*60)\n","\n","    adaptive_loss_fn = AdaptiveCrossModalEmotionLoss(\n","        use_cosine=True,\n","        use_mse=True,\n","        use_kl=False,\n","        learnable_weights=True\n","    )\n","\n","    total_loss, loss_dict = adaptive_loss_fn(audio_emb, video_emb)\n","    print(f\"\\nTotal Loss: {total_loss.item():.4f}\")\n","    print(\"Components:\")\n","    for key, value in loss_dict.items():\n","        print(f\"  {key}: {value:.4f}\")\n","\n","    # Test emotion agreement metric\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Emotion Agreement Metric\")\n","    print(\"=\"*60)\n","\n","    metric = EmotionAgreementMetric(num_classes=8, threshold=0.8)\n","\n","    # Simulate predictions\n","    audio_logits = torch.randn(batch_size, 8)\n","    video_logits = torch.randn(batch_size, 8)\n","\n","    metric.update(audio_emb, video_emb, audio_logits, video_logits)\n","    results = metric.compute()\n","\n","    print(f\"\\nAgreement Rate: {results['agreement_rate']:.2%}\")\n","    print(f\"Avg Cosine Sim: {results['avg_cosine_sim']:.4f}\")\n","    print(f\"Class Agreement: {results['class_agreement']:.2%}\")\n","\n","    print(\"\\n✓ Cross-modal emotion loss module ready!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eMMSbM2sftBG","executionInfo":{"status":"ok","timestamp":1762406668735,"user_tz":420,"elapsed":121,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}},"outputId":"36bac955-7c77-49ab-bf4d-e0f162fd3fbd"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Cross-Modal Emotion Loss - Example Usage\n","============================================================\n","\n","COSINE Loss: 0.9795\n","\n","MSE Loss: 0.0026\n","\n","KL Loss: 0.2595\n","\n","============================================================\n","Adaptive Cross-Modal Emotion Loss\n","============================================================\n","\n","Total Loss: 0.9808\n","Components:\n","  cosine: 0.9795\n","  weight_cosine: 1.0000\n","  mse: 0.0026\n","  weight_mse: 0.5000\n","  total: 0.9808\n","\n","============================================================\n","Emotion Agreement Metric\n","============================================================\n","\n","Agreement Rate: 0.00%\n","Avg Cosine Sim: 0.0205\n","Class Agreement: 25.00%\n","\n","✓ Cross-modal emotion loss module ready!\n"]}]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader, Subset"],"metadata":{"id":"MqyYJggqgvGy","executionInfo":{"status":"ok","timestamp":1762406668737,"user_tz":420,"elapsed":1,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# encoders_window_level_v3_fixed2.py\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Window-level Emotion Encoders (Audio + Video), stable on small VMs/Colab.\n","\n","Fixes vs v3_fixed:\n","- Do NOT call enable_input_require_grads() (it caused AttributeError with tuple outputs).\n","- Gradient checkpointing is optional and disabled by default (use_checkpoint=False).\n","- Keeps do_rescale=False, AMP, workers=0, pin_memory=False, etc.\n","\"\"\"\n","\n","import os\n","import json\n","import warnings\n","from pathlib import Path\n","from typing import Dict, List, Optional, Tuple, Union\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchaudio\n","from torch.utils.data import Dataset, DataLoader, Subset\n","\n","# Stable AMP API\n","from torch.amp import GradScaler, autocast\n","\n","# Silence noisy torchaudio warnings\n","warnings.filterwarnings(\"ignore\", message=\".*StreamingMediaDecoder.*\")\n","warnings.filterwarnings(\"ignore\", message=\".*load_with_torchcodec.*\")\n","\n","# Safer multiprocessing defaults (we still use workers=0 by default)\n","import torch.multiprocessing as mp\n","try:\n","    mp.set_start_method(\"spawn\", force=True)\n","except RuntimeError:\n","    pass\n","try:\n","    mp.set_sharing_strategy(\"file_system\")\n","except RuntimeError:\n","    pass\n","\n","from transformers import (\n","    Wav2Vec2ForSequenceClassification,\n","    HubertForSequenceClassification,\n","    Wav2Vec2FeatureExtractor,\n","    AutoImageProcessor,\n","    TimesformerForVideoClassification,\n",")\n","\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","\n","\n","# -------------------\n","# Constants / Labels\n","# -------------------\n","\n","EMOTION_TO_ID = {\n","    \"neutral\": 0, \"calm\": 1, \"happy\": 2, \"sad\": 3,\n","    \"angry\": 4, \"fearful\": 5, \"disgust\": 6, \"surprised\": 7\n","}\n","EMOTION_NAMES = [\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprised\"]\n","\n","\n","# -------------------\n","# Utils\n","# -------------------\n","\n","def ensure_dir(p: Union[str, Path]):\n","    Path(p).mkdir(parents=True, exist_ok=True)\n","\n","\n","def uniform_indices(total: int, target: int) -> np.ndarray:\n","    if total <= 0:\n","        return np.zeros((target,), dtype=int)\n","    if total <= target:\n","        base = np.arange(total)\n","        pad = np.full(target - total, total - 1, dtype=int)\n","        return np.concatenate([base, pad])\n","    return np.round(np.linspace(0, total - 1, target)).astype(int)\n","\n","\n","def set_seed(seed: int = 42):\n","    import random\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","\n","def set_backbone_trainable_timesformer(model: nn.Module, trainable: bool):\n","    for n, p in model.named_parameters():\n","        if \"classifier\" in n:\n","            continue\n","        p.requires_grad = trainable\n","\n","\n","def safe_freeze_wav2vec_feature_encoder(model: nn.Module):\n","    if hasattr(model, \"freeze_feature_encoder\"):\n","        model.freeze_feature_encoder()\n","    else:\n","        for n, p in model.named_parameters():\n","            if \"classifier\" in n:\n","                continue\n","            p.requires_grad = False\n","\n","\n","def safe_unfreeze_wav2vec_feature_encoder(model: nn.Module):\n","    for p in model.parameters():\n","        p.requires_grad = True\n","\n","\n","# -------------------\n","# Dataset\n","# -------------------\n","\n","class EmotionDataset(Dataset):\n","    def __init__(\n","        self,\n","        metadata_path: Union[str, Path],\n","        video_max_frames: int = 64,\n","        audio_target_sr: int = 16000,\n","        load_audio: bool = True,\n","        load_video: bool = True,\n","    ):\n","        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n","            self.meta: List[Dict] = json.load(f)\n","        if len(self.meta) == 0:\n","            raise ValueError(\"Empty metadata file.\")\n","\n","        self.video_max_frames = int(video_max_frames)\n","        self.audio_target_sr = int(audio_target_sr)\n","        self.load_audio = bool(load_audio)\n","        self.load_video = bool(load_video)\n","\n","        m0 = self.meta[0]\n","        self.uses_npz = \"video_npz\" in m0\n","\n","        self.frames_per_clip = int(m0.get(\"frames_per_clip\", m0.get(\"fixed_T\", 32)))\n","        if \"frame_size\" in m0:\n","            self.frame_size = tuple(m0[\"frame_size\"])\n","        elif \"video_size\" in m0:\n","            H, W = m0[\"video_size\"]\n","            self.frame_size = (W, H)\n","        else:\n","            self.frame_size = (224, 224)\n","\n","    def __len__(self):\n","        return len(self.meta)\n","\n","    def _load_audio(self, audio_path: str) -> torch.Tensor:\n","        wav, sr = torchaudio.load(audio_path)\n","        if wav.shape[0] > 1:\n","            wav = wav.mean(dim=0, keepdim=True)\n","        if sr != self.audio_target_sr:\n","            wav = torchaudio.transforms.Resample(sr, self.audio_target_sr)(wav)\n","        return wav.squeeze(0)\n","\n","    def _load_video_npz(self, npz_path: str) -> Tuple[torch.Tensor, np.ndarray]:\n","        data = np.load(npz_path)\n","        frames = data[\"frames\"]\n","        ts = data.get(\"timestamps\", None)\n","        if ts is None:\n","            T = frames.shape[0]\n","            ts = np.linspace(0.0, float(T - 1) / 25.0, num=T, dtype=np.float32)\n","        if frames.shape[0] > self.video_max_frames:\n","            idx = uniform_indices(frames.shape[0], self.video_max_frames)\n","            frames = frames[idx]\n","            ts = ts[idx]\n","        frames = frames.astype(np.float32) / 255.0\n","        tchw = torch.from_numpy(frames).permute(0, 3, 1, 2).contiguous()\n","        return tchw, ts\n","\n","    def _load_video_frames_dir(self, frames_dir: str) -> torch.Tensor:\n","        frame_files = sorted(Path(frames_dir).glob(\"frame_*.npy\"))\n","        if len(frame_files) == 0:\n","            W, H = self.frame_size\n","            return torch.zeros((self.video_max_frames, 3, H, W), dtype=torch.float32)\n","        if len(frame_files) > self.video_max_frames:\n","            idx = uniform_indices(len(frame_files), self.video_max_frames)\n","            frame_files = [frame_files[i] for i in idx]\n","        frames = []\n","        for f in frame_files:\n","            arr = np.load(f, mmap_mode=\"r\")\n","            if arr.ndim != 3 or arr.shape[2] != 3:\n","                W, H = self.frame_size\n","                arr = np.zeros((H, W, 3), dtype=np.uint8)\n","            frames.append(arr.astype(np.float32) / 255.0)\n","        frames = np.stack(frames, axis=0)\n","        tchw = torch.from_numpy(frames).permute(0, 3, 1, 2).contiguous()\n","        return tchw\n","\n","    def __getitem__(self, idx: int) -> Dict:\n","        rec = self.meta[idx]\n","        out = {\n","            \"sample_id\": rec[\"sample_id\"],\n","            \"emotion_label\": EMOTION_TO_ID.get(rec.get(\"emotion\", \"\"), -1),\n","            \"meta\": rec,\n","        }\n","        if self.load_audio:\n","            out[\"audio\"] = self._load_audio(rec[\"audio_path\"])\n","            out[\"sample_rate\"] = self.audio_target_sr\n","        if self.load_video:\n","            if self.uses_npz:\n","                v, ts = self._load_video_npz(rec[\"video_npz\"])\n","                out[\"video\"] = v\n","                out[\"timestamps\"] = torch.from_numpy(ts)\n","            else:\n","                v = self._load_video_frames_dir(rec[\"video_frames_dir\"])\n","                out[\"video\"] = v\n","                fps = rec.get(\"target_fps\", rec.get(\"original_fps\", 25.0))\n","                T = v.shape[0]\n","                ts = np.arange(T, dtype=np.float32) / float(fps)\n","                out[\"timestamps\"] = torch.from_numpy(ts)\n","        return out\n","\n","\n","def emotion_collate(batch: List[Dict]) -> Dict:\n","    out: Dict[str, Union[List, torch.Tensor]] = {\n","        \"sample_id\": [b[\"sample_id\"] for b in batch],\n","        \"emotion_label\": torch.tensor([b[\"emotion_label\"] for b in batch], dtype=torch.long),\n","        \"meta\": [b[\"meta\"] for b in batch],\n","    }\n","    if \"audio\" in batch[0]:\n","        out[\"audio\"] = [b[\"audio\"] for b in batch]\n","        out[\"sample_rate\"] = batch[0][\"sample_rate\"]\n","    if \"video\" in batch[0]:\n","        out[\"video\"] = torch.stack([b[\"video\"] for b in batch], dim=0)\n","        out[\"timestamps\"] = [b[\"timestamps\"] for b in batch]\n","    return out\n","\n","\n","# Window cropping helpers\n","def crop_audio_random(wav_1d: torch.Tensor, sr: int, dur_s: float) -> torch.Tensor:\n","    n = wav_1d.numel()\n","    L = int(round(dur_s * sr))\n","    if n <= L:\n","        pad_val = wav_1d[-1] if n > 0 else torch.tensor(0.0, device=wav_1d.device)\n","        pad = pad_val.repeat(L - n)\n","        return torch.cat([wav_1d, pad], 0)\n","    start = torch.randint(0, n - L + 1, ()).item()\n","    return wav_1d[start:start + L]\n","\n","\n","def crop_audio_center(wav_1d: torch.Tensor, sr: int, dur_s: float) -> torch.Tensor:\n","    n = wav_1d.numel()\n","    L = int(round(dur_s * sr))\n","    if n <= L:\n","        pad_val = wav_1d[-1] if n > 0 else torch.tensor(0.0, device=wav_1d.device)\n","        pad = pad_val.repeat(L - n)\n","        return torch.cat([wav_1d, pad], 0)\n","    start = max(0, (n - L) // 2)\n","    return wav_1d[start:start + L]\n","\n","\n","def crop_video_random_T(video_TCHW: torch.Tensor, Ts: int) -> torch.Tensor:\n","    T = video_TCHW.shape[0]\n","    if T <= Ts:\n","        idx = torch.linspace(0, T - 1, Ts).round().long()\n","        return video_TCHW[idx]\n","    start = torch.randint(0, T - Ts + 1, ()).item()\n","    return video_TCHW[start:start + Ts]\n","\n","\n","def crop_video_center_T(video_TCHW: torch.Tensor, Ts: int) -> torch.Tensor:\n","    T = video_TCHW.shape[0]\n","    if T <= Ts:\n","        idx = torch.linspace(0, T - 1, Ts).round().long()\n","        return video_TCHW[idx]\n","    start = (T - Ts) // 2\n","    return video_TCHW[start:start + Ts]\n","\n","\n","# Audio encoder\n","class AudioEmotionEncoder:\n","    def __init__(\n","        self,\n","        model_name: str = \"superb/wav2vec2-base-superb-er\",\n","        num_emotions: int = 8,\n","        lr: float = 1e-5,\n","        device: Optional[str] = None,\n","        window_seconds: float = 1.5,\n","        grad_clip: float = 1.0,\n","        use_amp: bool = True,\n","    ):\n","        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.num_emotions = num_emotions\n","        self.window_seconds = float(window_seconds)\n","        self.grad_clip = float(grad_clip)\n","        self.use_amp = bool(use_amp)\n","        self.lr = lr\n","\n","        if \"hubert\" in model_name.lower():\n","            self.model = HubertForSequenceClassification.from_pretrained(\n","                model_name, num_labels=num_emotions, ignore_mismatched_sizes=True\n","            )\n","        else:\n","            self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\n","                model_name, num_labels=num_emotions, ignore_mismatched_sizes=True\n","            )\n","\n","        self.model.config.output_hidden_states = True\n","        self.model.to(self.device)\n","\n","        # Try load feature extractor; if model_name is a local dir without files this may throw.\n","        try:\n","            self.processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n","        except Exception:\n","            # fallback to a default extractor so validation/inference works\n","            self.processor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n","\n","        # create optimizer only from trainable params\n","        self.optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n","        self.crit = nn.CrossEntropyLoss()\n","        self.emotion_names = EMOTION_NAMES\n","\n","        # proper GradScaler init\n","        self.scaler = GradScaler(enabled=(self.use_amp and torch.cuda.is_available()))\n","\n","    def update_optimizer(self, lr: Optional[float] = None):\n","        \"\"\"Re-build the optimizer from current trainable parameters (call after unfreezing).\"\"\"\n","        if lr is not None:\n","            self.lr = lr\n","        self.optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n","\n","    def save(self, path: Union[str, Path]):\n","        \"\"\"Save model + processor so HF's from_pretrained(path) works later.\"\"\"\n","        p = Path(path)\n","        p.mkdir(parents=True, exist_ok=True)\n","        self.model.save_pretrained(str(p))\n","        try:\n","            # processor may be Wav2Vec2FeatureExtractor or Processor\n","            self.processor.save_pretrained(str(p))\n","        except Exception:\n","            # best effort\n","            pass\n","\n","    def _prepare(self, batch: Dict, train: bool = True):\n","        sr = batch[\"sample_rate\"]\n","        audios = []\n","        for a in batch[\"audio\"]:\n","            a = a.to(self.device)\n","            seg = crop_audio_random(a, sr, self.window_seconds) if train else crop_audio_center(a, sr, self.window_seconds)\n","            audios.append(seg.cpu().numpy())\n","        proc = self.processor(\n","            audios, sampling_rate=sr, return_tensors=\"pt\",\n","            padding=True, truncation=True, max_length=int(self.window_seconds * sr)\n","        )\n","        x = proc[\"input_values\"].to(self.device)\n","        m = proc.get(\"attention_mask\")\n","        m = m.to(self.device) if m is not None else None\n","        y = batch[\"emotion_label\"].to(self.device)\n","        return x, m, y\n","\n","    def train_epoch(self, loader: DataLoader) -> Dict[str, float]:\n","        self.model.train()\n","        total, preds_all, labels_all = 0.0, [], []\n","        for batch in tqdm(loader, desc=\"Training (Audio)\"):\n","            x, m, y = self._prepare(batch, train=True)\n","            self.optim.zero_grad(set_to_none=True)\n","            with autocast(\"cuda\", enabled=self.use_amp):\n","                out = self.model(input_values=x, attention_mask=m)\n","                loss = self.crit(out.logits, y)\n","            self.scaler.scale(loss).backward()\n","            if self.grad_clip is not None:\n","                self.scaler.unscale_(self.optim)\n","                nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n","            self.scaler.step(self.optim)\n","            self.scaler.update()\n","            total += loss.item()\n","            preds_all.extend(out.logits.argmax(dim=1).detach().cpu().numpy())\n","            labels_all.extend(y.detach().cpu().numpy())\n","        return {\n","            \"loss\": total / len(loader),\n","            \"accuracy\": accuracy_score(labels_all, preds_all),\n","            \"f1_score\": f1_score(labels_all, preds_all, average=\"weighted\")\n","        }\n","\n","    @torch.no_grad()\n","    def validate(self, loader: DataLoader) -> Dict[str, float]:\n","        self.model.eval()\n","        total, preds_all, labels_all = 0.0, [], []\n","        for batch in tqdm(loader, desc=\"Validation (Audio)\"):\n","            x, m, y = self._prepare(batch, train=False)\n","            with autocast(\"cuda\", enabled=self.use_amp):\n","                out = self.model(input_values=x, attention_mask=m)\n","                loss = self.crit(out.logits, y)\n","            total += loss.item()\n","            preds_all.extend(out.logits.argmax(dim=1).detach().cpu().numpy())\n","            labels_all.extend(y.detach().cpu().numpy())\n","        cm = confusion_matrix(labels_all, preds_all)\n","        return {\n","            \"loss\": total / len(loader),\n","            \"accuracy\": accuracy_score(labels_all, preds_all),\n","            \"f1_score\": f1_score(labels_all, preds_all, average=\"weighted\"),\n","            \"confusion_matrix\": cm,\n","            \"predictions\": preds_all,\n","            \"labels\": labels_all\n","        }\n","\n","    @torch.no_grad()\n","    def extract_embeddings_clip(self, audios_1d: List[torch.Tensor], sr: int = 16000, window_seconds: float = 1.5) -> torch.Tensor:\n","        self.model.eval()\n","        crops = [crop_audio_center(a.to(self.device), sr, window_seconds).cpu().numpy() for a in audios_1d]\n","        proc = self.processor(crops, sampling_rate=sr, return_tensors=\"pt\", padding=True, truncation=True,\n","                              max_length=int(window_seconds * sr))\n","        x = proc[\"input_values\"].to(self.device)\n","        m = proc.get(\"attention_mask\"); m = m.to(self.device) if m is not None else None\n","        out = self.model(input_values=x, attention_mask=m, output_hidden_states=True)\n","        last = getattr(out, \"hidden_states\", None)\n","        last = last[-1] if last is not None else getattr(out, \"last_hidden_state\")\n","        return last.mean(dim=1)  # (B, D)\n","\n","    @torch.no_grad()\n","    def extract_embeddings_window(self, audio_1d: torch.Tensor, sr: int, t0: float, t1: float) -> torch.Tensor:\n","        self.model.eval()\n","        start = int(max(0, round(t0 * sr)))\n","        end = int(max(start + 1, round(t1 * sr)))\n","        seg = audio_1d[start:end]\n","        L = max(1, end - start)\n","        if seg.numel() < L:\n","            pad_val = seg[-1] if seg.numel() > 0 else torch.tensor(0.0, device=audio_1d.device)\n","            seg = torch.cat([seg, pad_val.repeat(L - seg.numel())], 0)\n","        proc = self.processor([seg.cpu().numpy()], sampling_rate=sr, return_tensors=\"pt\", padding=True, truncation=True, max_length=L)\n","        x = proc[\"input_values\"].to(self.device)\n","        m = proc.get(\"attention_mask\"); m = m.to(self.device) if m is not None else None\n","        out = self.model(input_values=x, attention_mask=m, output_hidden_states=True)\n","        last = getattr(out, \"hidden_states\", None); last = last[-1] if last is not None else getattr(out, \"last_hidden_state\")\n","        return last.mean(dim=1)\n","\n","\n","# Video encoder (checkpointing disabled by default to avoid tuple hook crash)\n","class VideoEmotionEncoder:\n","    def __init__(\n","        self,\n","        model_name: str = \"facebook/timesformer-base-finetuned-k400\",\n","        num_emotions: int = 8,\n","        lr: float = 1e-5,\n","        frames_for_model: int = 16,\n","        device: Optional[str] = None,\n","        grad_clip: float = 1.0,\n","        use_amp: bool = True,\n","        use_checkpoint: bool = False,\n","    ):\n","        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.num_emotions = num_emotions\n","        self.frames_for_model = int(frames_for_model)\n","        self.grad_clip = float(grad_clip)\n","        self.use_amp = bool(use_amp)\n","        self.lr = lr\n","\n","        self.model = TimesformerForVideoClassification.from_pretrained(\n","            model_name, num_labels=num_emotions, ignore_mismatched_sizes=True\n","        )\n","        self.model.config.output_hidden_states = True\n","\n","        if use_checkpoint and hasattr(self.model, \"gradient_checkpointing_enable\"):\n","            try:\n","                self.model.gradient_checkpointing_enable()\n","            except Exception:\n","                pass\n","\n","        self.model.to(self.device)\n","\n","        # Robust processor loading\n","        try:\n","            self.processor = AutoImageProcessor.from_pretrained(model_name)\n","        except Exception:\n","            # fallback (the HF model should usually have a processor)\n","            self.processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n","\n","        self.optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n","        self.crit = nn.CrossEntropyLoss()\n","        self.emotion_names = EMOTION_NAMES\n","        self.scaler = GradScaler(enabled=(self.use_amp and torch.cuda.is_available()))\n","\n","    def update_optimizer(self, lr: Optional[float] = None):\n","        if lr is not None:\n","            self.lr = lr\n","        self.optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n","\n","    def save(self, path: Union[str, Path]):\n","        p = Path(path)\n","        p.mkdir(parents=True, exist_ok=True)\n","        self.model.save_pretrained(str(p))\n","        try:\n","            self.processor.save_pretrained(str(p))\n","        except Exception:\n","            pass\n","\n","    def _prepare(self, batch: Dict, train: bool = True):\n","        vids = []\n","        Ts = self.frames_for_model\n","        for v in batch[\"video\"]:\n","            s = crop_video_random_T(v, Ts) if train else crop_video_center_T(v, Ts)\n","            frames = [s[i].permute(1, 2, 0).cpu().numpy() for i in range(s.shape[0])]\n","            vids.append(frames)\n","        proc = self.processor(vids, return_tensors=\"pt\", do_rescale=False)  # frames already in [0,1]\n","        x = proc[\"pixel_values\"].to(self.device)\n","        y = batch[\"emotion_label\"].to(self.device)\n","        return x, y\n","\n","    def train_epoch(self, loader: DataLoader) -> Dict[str, float]:\n","        self.model.train()\n","        total, preds_all, labels_all = 0.0, [], []\n","        for batch in tqdm(loader, desc=\"Training (Video)\"):\n","            x, y = self._prepare(batch, train=True)\n","            self.optim.zero_grad(set_to_none=True)\n","            with autocast(\"cuda\", enabled=self.use_amp):\n","                out = self.model(pixel_values=x)\n","                loss = self.crit(out.logits, y)\n","            self.scaler.scale(loss).backward()\n","            if self.grad_clip is not None:\n","                self.scaler.unscale_(self.optim)\n","                nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n","            self.scaler.step(self.optim)\n","            self.scaler.update()\n","            total += loss.item()\n","            preds_all.extend(out.logits.argmax(dim=1).detach().cpu().numpy())\n","            labels_all.extend(y.detach().cpu().numpy())\n","        return {\n","            \"loss\": total / len(loader),\n","            \"accuracy\": accuracy_score(labels_all, preds_all),\n","            \"f1_score\": f1_score(labels_all, preds_all, average=\"weighted\"),\n","        }\n","\n","    @torch.no_grad()\n","    def validate(self, loader: DataLoader) -> Dict[str, float]:\n","        self.model.eval()\n","        total, preds_all, labels_all = 0.0, [], []\n","        for batch in tqdm(loader, desc=\"Validation (Video)\"):\n","            x, y = self._prepare(batch, train=False)\n","            with autocast(\"cuda\", enabled=self.use_amp):\n","                out = self.model(pixel_values=x)\n","                loss = self.crit(out.logits, y)\n","            total += loss.item()\n","            preds_all.extend(out.logits.argmax(dim=1).detach().cpu().numpy())\n","            labels_all.extend(y.detach().cpu().numpy())\n","        cm = confusion_matrix(labels_all, preds_all)\n","        return {\n","            \"loss\": total / len(loader),\n","            \"accuracy\": accuracy_score(labels_all, preds_all),\n","            \"f1_score\": f1_score(labels_all, preds_all, average=\"weighted\"),\n","            \"confusion_matrix\": cm,\n","            \"predictions\": preds_all,\n","            \"labels\": labels_all\n","        }\n","\n","    @torch.no_grad()\n","    def extract_embeddings_clip(self, video_TCHW: torch.Tensor, frames_for_model: Optional[int] = None) -> torch.Tensor:\n","        self.model.eval()\n","        Ts = frames_for_model or self.frames_for_model\n","        if video_TCHW.dim() == 4:\n","            video_TCHW = video_TCHW.unsqueeze(0)\n","        batch_embs = []\n","        for v in video_TCHW:\n","            s = crop_video_center_T(v, Ts)\n","            frames = [s[i].permute(1, 2, 0).cpu().numpy() for i in range(s.shape[0])]\n","            proc = self.processor([frames], return_tensors=\"pt\", do_rescale=False)\n","            x = proc[\"pixel_values\"].to(self.device)\n","            out = self.model(pixel_values=x, output_hidden_states=True)\n","            hs = getattr(out, \"hidden_states\", None)\n","            last = hs[-1] if hs is not None else getattr(out, \"last_hidden_state\")\n","            emb = last.mean(dim=1)\n","            batch_embs.append(emb)\n","        return torch.cat(batch_embs, dim=0)\n","\n","    @torch.no_grad()\n","    def extract_embeddings_window_from_npz(self, npz_path: str, t0: float, t1: float, Ts: Optional[int] = None) -> torch.Tensor:\n","        self.model.eval()\n","        Ts = Ts or self.frames_for_model\n","        data = np.load(npz_path)\n","        frames = data[\"frames\"]\n","        ts = data[\"timestamps\"].astype(np.float32) if \"timestamps\" in data else np.arange(frames.shape[0], dtype=np.float32) / 25.0\n","        mask = (ts >= t0) & (ts <= t1)\n","        sub = frames[mask]\n","        if sub.shape[0] == 0:\n","            center = 0.5 * (t0 + t1)\n","            idx = int(np.argmin(np.abs(ts - center)))\n","            sub = frames[idx:idx+1]\n","        idx = uniform_indices(sub.shape[0], Ts)\n","        sub = sub[idx].astype(np.float32) / 255.0\n","        frames_list = [sub[i] for i in range(sub.shape[0])]\n","        proc = self.processor([frames_list], return_tensors=\"pt\", do_rescale=False)\n","        x = proc[\"pixel_values\"].to(self.device)\n","        out = self.model(pixel_values=x, output_hidden_states=True)\n","        hs = getattr(out, \"hidden_states\", None)\n","        last = hs[-1] if hs is not None else getattr(out, \"last_hidden_state\")\n","        return last.mean(dim=1)\n","\n","\n","# Trainer\n","def train_encoders(\n","    metadata_path: str,\n","    output_dir: str,\n","    audio_model: str = \"superb/wav2vec2-base-superb-er\",\n","    video_model: str = \"facebook/timesformer-base-finetuned-k400\",\n","    num_epochs: int = 20,\n","    batch_size: int = 4,\n","    val_split: float = 0.2,\n","    audio_window_s: float = 1.5,\n","    video_Ts: int = 16,\n","    video_max_frames: int = 64,\n","    use_wandb: bool = True,\n","    seed: int = 42,\n","    audio_freeze_epochs: int = 2,\n","    video_freeze_epochs: int = 1,\n","):\n","    set_seed(seed)\n","    ensure_dir(output_dir)\n","\n","    WANDB = False\n","    if use_wandb:\n","        try:\n","            import wandb\n","            wandb.init(project=\"almost-human-encoders\", config=dict(\n","                audio_model=audio_model, video_model=video_model, num_epochs=num_epochs,\n","                batch_size=batch_size, val_split=val_split, audio_window_s=audio_window_s,\n","                video_Ts=video_Ts, seed=seed\n","            ))\n","            WANDB = True\n","        except Exception as e:\n","            print(f\"⚠ W&B init failed: {e}\")\n","            WANDB = False\n","\n","    base = EmotionDataset(metadata_path, video_max_frames=video_max_frames, load_audio=True, load_video=True)\n","    N = len(base)\n","    val_size = int(N * val_split)\n","    train_size = N - val_size\n","    indices = torch.randperm(N)\n","    train_idx, val_idx = indices[:train_size], indices[train_size:]\n","\n","    ds_audio_train = Subset(EmotionDataset(metadata_path, video_max_frames=video_max_frames, load_audio=True, load_video=False), train_idx)\n","    ds_audio_val   = Subset(EmotionDataset(metadata_path, video_max_frames=video_max_frames, load_audio=True, load_video=False), val_idx)\n","    ds_video_train = Subset(EmotionDataset(metadata_path, video_max_frames=video_max_frames, load_audio=False, load_video=True), train_idx)\n","    ds_video_val   = Subset(EmotionDataset(metadata_path, video_max_frames=video_max_frames, load_audio=False, load_video=True), val_idx)\n","\n","    train_loader_audio = DataLoader(ds_audio_train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, collate_fn=emotion_collate)\n","    val_loader_audio   = DataLoader(ds_audio_val,   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False, collate_fn=emotion_collate)\n","    train_loader_video = DataLoader(ds_video_train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, collate_fn=emotion_collate)\n","    val_loader_video   = DataLoader(ds_video_val,   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False, collate_fn=emotion_collate)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"Device: {device} | Train: {train_size} | Val: {val_size}\")\n","\n","    audio_enc = AudioEmotionEncoder(model_name=audio_model, device=device, window_seconds=audio_window_s, use_amp=True)\n","    video_enc = VideoEmotionEncoder(model_name=video_model, device=device, frames_for_model=video_Ts, use_amp=True, use_checkpoint=False)\n","\n","    safe_freeze_wav2vec_feature_encoder(audio_enc.model)\n","    set_backbone_trainable_timesformer(video_enc.model, trainable=False)\n","\n","    best_audio_f1 = 0.0\n","    best_video_f1 = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print(\"\\n\" + \"=\"*60)\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        print(\"=\"*60)\n","\n","        if epoch == audio_freeze_epochs:\n","            safe_unfreeze_wav2vec_feature_encoder(audio_enc.model)\n","            print(\"→ Unfroze Wav2Vec2/HubERT feature encoder\")\n","        if epoch == video_freeze_epochs:\n","            set_backbone_trainable_timesformer(video_enc.model, trainable=True)\n","            print(\"→ Unfroze TimeSformer backbone\")\n","\n","        a_train = audio_enc.train_epoch(train_loader_audio)\n","        a_val = audio_enc.validate(val_loader_audio)\n","        print(f\"[Audio] Train: loss={a_train['loss']:.4f} acc={a_train['accuracy']:.4f} f1={a_train['f1_score']:.4f}\")\n","        print(f\"[Audio]   Val: loss={a_val['loss']:.4f} acc={a_val['accuracy']:.4f} f1={a_val['f1_score']:.4f}\")\n","\n","        v_train = video_enc.train_epoch(train_loader_video)\n","        v_val = video_enc.validate(val_loader_video)\n","        print(f\"[Video] Train: loss={v_train['loss']:.4f} acc={v_train['accuracy']:.4f} f1={v_train['f1_score']:.4f}\")\n","        print(f\"[Video]   Val: loss={v_val['loss']:.4f} acc={v_val['accuracy']:.4f} f1={v_val['f1_score']:.4f}\")\n","\n","        if WANDB:\n","            wandb.log({\n","                \"epoch\": epoch + 1,\n","                \"audio/train_loss\": a_train[\"loss\"], \"audio/train_acc\": a_train[\"accuracy\"], \"audio/train_f1\": a_train[\"f1_score\"],\n","                \"audio/val_loss\": a_val[\"loss\"], \"audio/val_acc\": a_val[\"accuracy\"], \"audio/val_f1\": a_val[\"f1_score\"],\n","                \"video/train_loss\": v_train[\"loss\"], \"video/train_acc\": v_train[\"accuracy\"], \"video/train_f1\": v_train[\"f1_score\"],\n","                \"video/val_loss\": v_val[\"loss\"], \"video/val_acc\": v_val[\"accuracy\"], \"video/val_f1\": v_val[\"f1_score\"],\n","            })\n","\n","        if a_val[\"f1_score\"] > best_audio_f1:\n","            best_audio_f1 = a_val[\"f1_score\"]\n","            save_path = Path(output_dir) / \"best_audio_encoder\"\n","            audio_enc.model.save_pretrained(str(save_path))\n","            print(f\"✓ Saved best audio encoder → {save_path} (F1={best_audio_f1:.4f})\")\n","\n","        if v_val[\"f1_score\"] > best_video_f1:\n","            best_video_f1 = v_val[\"f1_score\"]\n","            save_path = Path(output_dir) / \"best_video_encoder\"\n","            video_enc.model.save_pretrained(str(save_path))\n","            print(f\"✓ Saved best video encoder → {save_path} (F1={best_video_f1:.4f})\")\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Training complete!\")\n","    print(f\"Best Audio F1: {best_audio_f1:.4f} | Best Video F1: {best_video_f1:.4f}\")\n","    print(\"=\"*60)\n","\n","    if WANDB:\n","        wandb.finish()\n","\n","    return audio_enc, video_enc, best_audio_f1, best_video_f1\n"],"metadata":{"id":"tjHNKYodhAp8","executionInfo":{"status":"ok","timestamp":1762406668825,"user_tz":420,"elapsed":74,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Integration Guide: Using Cross-Modal Emotion Loss with Trained Encoders\n","\n","This script shows how to:\n","1. Load trained emotion encoders\n","2. Compute emotion loss during inference\n","3. Evaluate emotion agreement on validation set\n","4. Integrate with talking face models (Wav2Lip/SadTalker)\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","from pathlib import Path\n","from typing import Dict, Tuple\n","import json\n","\n","\n","\n","# ============================================================================\n","# Step 1: Load Trained Encoders\n","# ============================================================================\n","\n","def load_trained_encoders(\n","    audio_encoder_path: str,\n","    video_encoder_path: str,\n","    device: str = 'cuda',\n",") -> Tuple[AudioEmotionEncoder, VideoEmotionEncoder]:\n","    \"\"\"\n","    Load pre-trained emotion encoders.\n","\n","    Args:\n","        audio_encoder_path: Path to trained audio encoder\n","        video_encoder_path: Path to trained video encoder\n","        device: Device to load models on\n","\n","    Returns:\n","        audio_encoder: Loaded audio encoder\n","        video_encoder: Loaded video encoder\n","    \"\"\"\n","    print(\"Loading trained emotion encoders...\")\n","\n","    # Initialize encoders with pretrained weights\n","    audio_enc = AudioEmotionEncoder(\n","        model_name=audio_encoder_path,\n","        num_emotions=8,\n","        device=device,\n","        window_seconds=1.5,\n","        use_amp=False,  # Disable AMP for inference\n","    )\n","\n","    video_enc = VideoEmotionEncoder(\n","        model_name=video_encoder_path,\n","        num_emotions=8,\n","        frames_for_model=16,\n","        device=device,\n","        use_amp=False,  # Disable AMP for inference\n","        use_checkpoint=False,\n","    )\n","\n","    # Set to eval mode\n","    audio_enc.model.eval()\n","    video_enc.model.eval()\n","\n","    print(f\"✓ Audio encoder loaded from: {audio_encoder_path}\")\n","    print(f\"✓ Video encoder loaded from: {video_encoder_path}\")\n","\n","    return audio_enc, video_enc\n","\n","\n","# ============================================================================\n","# Step 2: Compute Emotion Loss for a Batch\n","# ============================================================================\n","\n","@torch.no_grad()\n","def compute_emotion_alignment(\n","    batch: Dict,\n","    audio_encoder: AudioEmotionEncoder,\n","    video_encoder: VideoEmotionEncoder,\n","    loss_fn: CrossModalEmotionLoss,\n","    device: str = 'cuda',\n",") -> Dict[str, float]:\n","    \"\"\"\n","    Compute emotion alignment metrics for a batch.\n","\n","    Args:\n","        batch: Batch from dataloader\n","        audio_encoder: Trained audio encoder\n","        video_encoder: Trained video encoder\n","        loss_fn: Emotion loss function\n","        device: Device to use\n","\n","    Returns:\n","        metrics: Dictionary with loss and alignment metrics\n","    \"\"\"\n","    audio_encoder.model.eval()\n","    video_encoder.model.eval()\n","\n","    # Extract audio embeddings\n","    audio_embs = audio_encoder.extract_embeddings_clip(\n","        batch['audio'],\n","        sr=batch.get('sample_rate', 16000),\n","        window_seconds=1.5\n","    )\n","\n","    # Extract video embeddings\n","    video_embs = video_encoder.extract_embeddings_clip(\n","        batch['video'],\n","        frames_for_model=16\n","    )\n","\n","    # Compute loss\n","    loss = loss_fn(audio_embs, video_embs)\n","\n","    # Compute cosine similarity\n","    audio_norm = torch.nn.functional.normalize(audio_embs, p=2, dim=-1)\n","    video_norm = torch.nn.functional.normalize(video_embs, p=2, dim=-1)\n","    cos_sim = torch.nn.functional.cosine_similarity(audio_norm, video_norm, dim=-1)\n","\n","    metrics = {\n","        'emotion_loss': loss.item(),\n","        'avg_cosine_sim': cos_sim.mean().item(),\n","        'min_cosine_sim': cos_sim.min().item(),\n","        'max_cosine_sim': cos_sim.max().item(),\n","        'std_cosine_sim': cos_sim.std().item(),\n","    }\n","\n","    return metrics\n","\n","\n","# ============================================================================\n","# Step 3: Evaluate on Validation Set\n","# ============================================================================\n","\n","def evaluate_emotion_agreement(\n","    metadata_path: str,\n","    audio_encoder: AudioEmotionEncoder,\n","    video_encoder: VideoEmotionEncoder,\n","    batch_size: int = 8,\n","    device: str = 'cuda',\n",") -> Dict[str, float]:\n","    \"\"\"\n","    Evaluate emotion agreement on entire validation set.\n","\n","    Args:\n","        metadata_path: Path to metadata.json\n","        audio_encoder: Trained audio encoder\n","        video_encoder: Trained video encoder\n","        batch_size: Batch size for evaluation\n","        device: Device to use\n","\n","    Returns:\n","        results: Dictionary with evaluation metrics\n","    \"\"\"\n","    from torch.utils.data import DataLoader\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Evaluating Emotion Agreement on Validation Set\")\n","    print(\"=\"*60)\n","\n","    # Load dataset\n","    dataset = EmotionDataset(\n","        metadata_path,\n","        video_max_frames=64,\n","        load_audio=True,\n","        load_video=True,\n","    )\n","\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=0,\n","        collate_fn=emotion_collate,\n","    )\n","\n","    # Initialize metric\n","    metric = EmotionAgreementMetric(num_classes=8, threshold=0.8)\n","\n","    # Initialize loss\n","    loss_fn = CrossModalEmotionLoss(\n","        loss_type='cosine',\n","        weight=1.0,\n","        normalize_embeddings=True,\n","    )\n","\n","    total_loss = 0.0\n","    num_batches = 0\n","\n","    print(f\"\\nProcessing {len(dataset)} samples...\")\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            # Extract embeddings\n","            audio_embs = audio_encoder.extract_embeddings_clip(\n","                batch['audio'],\n","                sr=batch.get('sample_rate', 16000),\n","                window_seconds=1.5\n","            )\n","\n","            video_embs = video_encoder.extract_embeddings_clip(\n","                batch['video'],\n","                frames_for_model=16\n","            )\n","\n","            # Update metric\n","            metric.update(audio_embs, video_embs)\n","\n","            # Compute loss\n","            loss = loss_fn(audio_embs, video_embs)\n","            total_loss += loss.item()\n","            num_batches += 1\n","\n","    # Compute final metrics\n","    results = metric.compute()\n","    results['avg_loss'] = total_loss / num_batches\n","\n","    print(f\"\\nResults:\")\n","    print(f\"  Agreement Rate: {results['agreement_rate']:.2%}\")\n","    print(f\"  Avg Cosine Sim: {results['avg_cosine_sim']:.4f}\")\n","    print(f\"  Class Agreement: {results['class_agreement']:.2%}\")\n","    print(f\"  Avg Loss: {results['avg_loss']:.4f}\")\n","\n","    return results\n","\n","\n","# ============================================================================\n","# Step 4: Integration with Talking Face Model\n","# ============================================================================\n","\n","class TalkingFaceWithEmotionLoss(nn.Module):\n","    \"\"\"\n","    Wrapper for talking face model (Wav2Lip/SadTalker) with emotion loss.\n","\n","    This is a template showing how to integrate emotion loss into\n","    your talking face model training.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        base_model: nn.Module,  # Your Wav2Lip or SadTalker model\n","        audio_encoder: AudioEmotionEncoder,\n","        video_encoder: VideoEmotionEncoder,\n","        emotion_loss_weight: float = 0.1,\n","        freeze_encoders: bool = True,\n","    ):\n","        \"\"\"\n","        Args:\n","            base_model: Base talking face model (Wav2Lip/SadTalker)\n","            audio_encoder: Trained audio emotion encoder\n","            video_encoder: Trained video emotion encoder\n","            emotion_loss_weight: Weight for emotion loss\n","            freeze_encoders: Whether to freeze encoder weights\n","        \"\"\"\n","        super().__init__()\n","\n","        self.base_model = base_model\n","        self.audio_encoder = audio_encoder\n","        self.video_encoder = video_encoder\n","        self.emotion_loss_weight = emotion_loss_weight\n","\n","        # Freeze emotion encoders (they're pre-trained)\n","        if freeze_encoders:\n","            for param in self.audio_encoder.model.parameters():\n","                param.requires_grad = False\n","            for param in self.video_encoder.model.parameters():\n","                param.requires_grad = False\n","\n","        # Initialize emotion loss\n","        self.emotion_loss_fn = CrossModalEmotionLoss(\n","            loss_type='cosine',\n","            weight=self.emotion_loss_weight,\n","            normalize_embeddings=True,\n","        )\n","\n","    def forward(\n","        self,\n","        audio: torch.Tensor,\n","        reference_frame: torch.Tensor,\n","        **kwargs\n","    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n","        \"\"\"\n","        Forward pass with emotion loss.\n","\n","        Args:\n","            audio: Input audio waveform\n","            reference_frame: Reference face image\n","            **kwargs: Additional arguments for base model\n","\n","        Returns:\n","            generated_video: Generated video frames\n","            losses: Dictionary of loss components\n","        \"\"\"\n","        # Generate video with base model\n","        generated_video = self.base_model(audio, reference_frame, **kwargs)\n","\n","        # Compute emotion loss\n","        # Extract audio emotion embeddings\n","        audio_embs = self.audio_encoder.extract_embeddings_clip(\n","            [audio],\n","            sr=16000,\n","            window_seconds=1.5\n","        )\n","\n","        # Extract video emotion embeddings from generated frames\n","        video_embs = self.video_encoder.extract_embeddings_clip(\n","            generated_video,\n","            frames_for_model=16\n","        )\n","\n","        # Compute emotion loss\n","        emotion_loss = self.emotion_loss_fn(audio_embs, video_embs)\n","\n","        losses = {\n","            'emotion_loss': emotion_loss,\n","            # Add your base model losses here (reconstruction, adversarial, etc.)\n","        }\n","\n","        return generated_video, losses\n","\n","\n","# ============================================================================\n","# Step 5: Training Loop Example\n","# ============================================================================\n","\n","def training_step_with_emotion_loss(\n","    model: TalkingFaceWithEmotionLoss,\n","    batch: Dict,\n","    optimizer: torch.optim.Optimizer,\n","    reconstruction_loss_fn: nn.Module,\n","    device: str = 'cuda',\n",") -> Dict[str, float]:\n","    \"\"\"\n","    Example training step with emotion loss.\n","\n","    Args:\n","        model: Talking face model with emotion loss\n","        batch: Training batch\n","        optimizer: Optimizer\n","        reconstruction_loss_fn: Your base reconstruction loss\n","        device: Device to use\n","\n","    Returns:\n","        metrics: Dictionary with loss values\n","    \"\"\"\n","    model.train()\n","\n","    audio = batch['audio']\n","    reference_frame = batch['reference_frame']  # First frame or specific ref\n","    target_video = batch['video']\n","\n","    # Forward pass\n","    generated_video, losses = model(audio, reference_frame)\n","\n","    # Compute reconstruction loss (your original loss)\n","    recon_loss = reconstruction_loss_fn(generated_video, target_video)\n","\n","    # Total loss = reconstruction + emotion\n","    total_loss = recon_loss + losses['emotion_loss']\n","\n","    # Backward pass\n","    optimizer.zero_grad()\n","    total_loss.backward()\n","    optimizer.step()\n","\n","    # Return metrics\n","    metrics = {\n","        'total_loss': total_loss.item(),\n","        'recon_loss': recon_loss.item(),\n","        'emotion_loss': losses['emotion_loss'].item(),\n","    }\n","\n","    return metrics\n","\n","\n"],"metadata":{"id":"7-URxYbYgCDe","executionInfo":{"status":"ok","timestamp":1762406668827,"user_tz":420,"elapsed":1,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from transformers import (\n","    Wav2Vec2ForSequenceClassification,\n","    HubertForSequenceClassification,\n","    Wav2Vec2FeatureExtractor,\n","    AutoImageProcessor,\n","    TimesformerForVideoClassification,\n",")"],"metadata":{"id":"i77iFAazhXR4","executionInfo":{"status":"ok","timestamp":1762406668828,"user_tz":420,"elapsed":0,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import torch\n","print(torch.version.cuda)\n","print(torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6WQ1rFr3GkO","executionInfo":{"status":"ok","timestamp":1762406668844,"user_tz":420,"elapsed":15,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}},"outputId":"fc4f74c5-d1e8-4db1-c858-3456a776877a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["12.6\n","False\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# Main Example Usage\n","# ============================================================================\n","\n","def main():\n","    \"\"\"\n","    Main example showing complete workflow.\n","    \"\"\"\n","    print(\"=\"*60)\n","    print(\"Cross-Modal Emotion Loss - Complete Integration Example\")\n","    print(\"=\"*60)\n","\n","    # Paths\n","    audio_encoder_path = \"/content/trained_encoders/best_audio_encoder\"\n","    video_encoder_path = \"/content/trained_encoders/best_video_encoder\"\n","    metadata_path = \"/content/processed_data/metadata.json\"\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print(f\"\\nUsing device: {device}\")\n","\n","    # Step 1: Load encoders\n","    audio_enc, video_enc = load_trained_encoders(\n","        audio_encoder_path,\n","        video_encoder_path,\n","        device=device\n","    )\n","\n","    # Step 2: Test on a single batch\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Testing on Single Batch\")\n","    print(\"=\"*60)\n","\n","    from torch.utils.data import DataLoader\n","\n","    dataset = EmotionDataset(\n","        metadata_path,\n","        video_max_frames=64,\n","        load_audio=True,\n","        load_video=True,\n","    )\n","\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=4,\n","        shuffle=False,\n","        collate_fn=emotion_collate,\n","    )\n","\n","    batch = next(iter(loader))\n","\n","    loss_fn = CrossModalEmotionLoss(\n","        loss_type='cosine',\n","        weight=1.0,\n","        normalize_embeddings=True,\n","    )\n","\n","    metrics = compute_emotion_alignment(\n","        batch,\n","        audio_enc,\n","        video_enc,\n","        loss_fn,\n","        device=device\n","    )\n","\n","    print(f\"\\nSingle Batch Metrics:\")\n","    for key, value in metrics.items():\n","        print(f\"  {key}: {value:.4f}\")\n","\n","    # Step 3: Evaluate on full validation set\n","    results = evaluate_emotion_agreement(\n","        metadata_path,\n","        audio_enc,\n","        video_enc,\n","        batch_size=8,\n","        device=device\n","    )\n","\n","    # Step 4: Save results\n","    output_dir = Path(\"/content/emotion_evaluation\")\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    with open(output_dir / \"emotion_agreement_results.json\", \"w\") as f:\n","        json.dump(results, f, indent=2)\n","\n","    print(f\"\\n✓ Results saved to: {output_dir / 'emotion_agreement_results.json'}\")\n","\n","    # Step 5: Example integration with talking face model\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Integration with Talking Face Model\")\n","    print(\"=\"*60)\n","    print(\"\\nTo integrate with Wav2Lip/SadTalker:\")\n","    print(\"1. Wrap your base model with TalkingFaceWithEmotionLoss\")\n","    print(\"2. Add emotion_loss to your total loss\")\n","    print(\"3. Use training_step_with_emotion_loss() in your training loop\")\n","    print(\"\\nExample:\")\n","    print(\"\"\"\n","    # Pseudo-code\n","    base_model = Wav2Lip()  # or SadTalker()\n","    model = TalkingFaceWithEmotionLoss(\n","        base_model,\n","        audio_enc,\n","        video_enc,\n","        emotion_loss_weight=0.1\n","    )\n","\n","    for batch in train_loader:\n","        metrics = training_step_with_emotion_loss(\n","            model, batch, optimizer, reconstruction_loss_fn\n","        )\n","    \"\"\")\n","\n","    print(\"\\n✓ Complete integration example finished!\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":775},"id":"Zf2DNk54gWSe","executionInfo":{"status":"error","timestamp":1762406687435,"user_tz":420,"elapsed":18590,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}},"outputId":"285ebacd-7f80-40c5-a53a-02dcaf522348"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","Cross-Modal Emotion Loss - Complete Integration Example\n","============================================================\n","\n","Using device: cpu\n","Loading trained emotion encoders...\n","✓ Audio encoder loaded from: /content/trained_encoders/best_audio_encoder\n","✓ Video encoder loaded from: /content/trained_encoders/best_video_encoder\n","\n","============================================================\n","Testing on Single Batch\n","============================================================\n","\n","Single Batch Metrics:\n","  emotion_loss: 1.0330\n","  avg_cosine_sim: -0.0330\n","  min_cosine_sim: -0.0446\n","  max_cosine_sim: -0.0114\n","  std_cosine_sim: 0.0147\n","\n","============================================================\n","Evaluating Emotion Agreement on Validation Set\n","============================================================\n","\n","Processing 720 samples...\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1258171166.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1258171166.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Step 3: Evaluate on full validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     results = evaluate_emotion_agreement(\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mmetadata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0maudio_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1636386374.py\u001b[0m in \u001b[0;36mevaluate_emotion_agreement\u001b[0;34m(metadata_path, audio_encoder, video_encoder, batch_size, device)\u001b[0m\n\u001b[1;32m    198\u001b[0m             )\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             video_embs = video_encoder.extract_embeddings_clip(\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mframes_for_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2310369475.py\u001b[0m in \u001b[0;36mextract_embeddings_clip\u001b[0;34m(self, video_TCHW, frames_for_model)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_rescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hidden_states\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last_hidden_state\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/timesformer/modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         outputs = self.timesformer(\n\u001b[0m\u001b[1;32m    724\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/timesformer/modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    592\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/timesformer/modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/timesformer/modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mspatial_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             spatial_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_before\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/timesformer/modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mself_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/timesformer/modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"RVdm0LVshRtF","executionInfo":{"status":"aborted","timestamp":1762406687437,"user_tz":420,"elapsed":5,"user":{"displayName":"Katrin Dar","userId":"11764921960668828556"}}},"execution_count":null,"outputs":[]}]}