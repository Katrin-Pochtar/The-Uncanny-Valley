{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    HubertForSequenceClassification,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    TimesformerForVideoClassification,\n",
        ")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "EMOTIONS = [\"happy\", \"angry\", \"disgust\"]\n",
        "NUM_EMOTIONS = 3\n",
        "\n",
        "BEST_AUDIO_PATH = \"/content/trained_encoders_v2/w2v2-lg-lr2e5\"\n",
        "BEST_VIDEO_PATH = \"/content/trained_encoders_v2/tsf-lr3e5-16f-nf\"\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class CrossModalEmotionLoss(nn.Module):\n",
        "    def __init__(self, weight=1.0):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, audio_emb, video_emb):\n",
        "        a = F.normalize(audio_emb, p=2, dim=-1)\n",
        "        v = F.normalize(video_emb, p=2, dim=-1)\n",
        "        return self.weight * (1.0 - F.cosine_similarity(a, v, dim=-1)).mean()\n",
        "\n",
        "\n",
        "class DifferentiableVideoPreprocess(nn.Module):\n",
        "    \"\"\"Replaces HF AutoImageProcessor with differentiable PyTorch ops.\n",
        "    Allows gradient to flow from emotion loss back through generated frames.\"\"\"\n",
        "\n",
        "    def __init__(self, size=224):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, frames):\n",
        "        \"\"\"frames: (B, T, C, H, W) in [0, 1] -> (B, T, C, 224, 224) normalized\"\"\"\n",
        "        B, T, C, H, W = frames.shape\n",
        "        x = frames.reshape(B * T, C, H, W)\n",
        "        if H != self.size or W != self.size:\n",
        "            x = F.interpolate(x, size=(self.size, self.size), mode=\"bilinear\", align_corners=False)\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x.reshape(B, T, C, self.size, self.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_frozen_audio_encoder(path, device=\"cuda\"):\n",
        "    is_hubert = \"hubert\" in str(path).lower()\n",
        "    cls = HubertForSequenceClassification if is_hubert else Wav2Vec2ForSequenceClassification\n",
        "    model = cls.from_pretrained(path)\n",
        "    model.config.output_hidden_states = True\n",
        "    model.eval().to(device)\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    processor = Wav2Vec2FeatureExtractor.from_pretrained(path)\n",
        "    return model, processor\n",
        "\n",
        "\n",
        "def load_frozen_video_encoder(path, device=\"cuda\"):\n",
        "    model = TimesformerForVideoClassification.from_pretrained(path)\n",
        "    model.config.output_hidden_states = True\n",
        "    model.eval().to(device)\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    return model\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_audio_embedding(model, processor, audio_list, sr=16000, window_s=3.0, device=\"cuda\"):\n",
        "    \"\"\"No gradient needed -- audio embedding is the target.\"\"\"\n",
        "    L = int(window_s * sr)\n",
        "    wavs = []\n",
        "    for a in audio_list:\n",
        "        n = a.numel()\n",
        "        if n <= L:\n",
        "            a = F.pad(a, (0, L - n))\n",
        "        else:\n",
        "            start = (n - L) // 2\n",
        "            a = a[start:start + L]\n",
        "        wavs.append(a.cpu().numpy())\n",
        "    enc = processor(wavs, sampling_rate=sr, return_tensors=\"pt\",\n",
        "                    padding=True, truncation=True, max_length=L)\n",
        "    x = enc[\"input_values\"].to(device)\n",
        "    mask = enc.get(\"attention_mask\")\n",
        "    if mask is not None:\n",
        "        mask = mask.to(device)\n",
        "    out = model(input_values=x, attention_mask=mask, output_hidden_states=True)\n",
        "    return out.hidden_states[-1].mean(dim=1)\n",
        "\n",
        "\n",
        "def extract_video_embedding(model, preprocess, frames, device=\"cuda\"):\n",
        "    \"\"\"Gradient flows through frames back to the generator.\"\"\"\n",
        "    # frames: (B, T, C, H, W) in [0, 1]\n",
        "    pv = preprocess(frames)           # (B, T, C, 224, 224), differentiable\n",
        "    pv = pv.to(device)\n",
        "    out = model(pixel_values=pv, output_hidden_states=True)\n",
        "    return out.hidden_states[-1].mean(dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class EmotionAgreementMetric:\n",
        "    def __init__(self, threshold=0.8):\n",
        "        self.threshold = threshold\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sims = []\n",
        "\n",
        "    def update(self, audio_emb, video_emb):\n",
        "        a = F.normalize(audio_emb.detach(), p=2, dim=-1)\n",
        "        v = F.normalize(video_emb.detach(), p=2, dim=-1)\n",
        "        self.sims.extend(F.cosine_similarity(a, v, dim=-1).cpu().tolist())\n",
        "\n",
        "    def compute(self):\n",
        "        s = np.array(self.sims)\n",
        "        return {\n",
        "            \"avg_cosine_sim\": float(s.mean()),\n",
        "            \"agreement_rate\": float((s >= self.threshold).mean()),\n",
        "            \"std_cosine_sim\": float(s.std()),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss_fn = CrossModalEmotionLoss(weight=0.1)\n",
        "a = torch.randn(4, 768)\n",
        "v = torch.randn(4, 768)\n",
        "print(f\"Emotion loss (random): {loss_fn(a, v).item():.4f}\")\n",
        "\n",
        "preprocess = DifferentiableVideoPreprocess(224).to(\"cpu\")\n",
        "frames = torch.rand(2, 8, 3, 96, 96, requires_grad=True)\n",
        "out = preprocess(frames)\n",
        "loss = out.sum()\n",
        "loss.backward()\n",
        "print(f\"Preprocess: {frames.shape} -> {out.shape}, grad flows: {frames.grad is not None}\")\n",
        "\n",
        "metric = EmotionAgreementMetric()\n",
        "metric.update(a, v)\n",
        "print(f\"Agreement: {metric.compute()}\")\n",
        "print(\"All modules ready.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile /content/emotion_utils.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    HubertForSequenceClassification,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    TimesformerForVideoClassification,\n",
        ")\n",
        "\n",
        "EMOTIONS = [\"happy\", \"angry\", \"disgust\"]\n",
        "NUM_EMOTIONS = 3\n",
        "\n",
        "\n",
        "class CrossModalEmotionLoss(nn.Module):\n",
        "    def __init__(self, weight=1.0):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, audio_emb, video_emb):\n",
        "        a = F.normalize(audio_emb, p=2, dim=-1)\n",
        "        v = F.normalize(video_emb, p=2, dim=-1)\n",
        "        return self.weight * (1.0 - F.cosine_similarity(a, v, dim=-1)).mean()\n",
        "\n",
        "\n",
        "class DifferentiableVideoPreprocess(nn.Module):\n",
        "    def __init__(self, size=224):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, frames):\n",
        "        B, T, C, H, W = frames.shape\n",
        "        x = frames.reshape(B * T, C, H, W)\n",
        "        if H != self.size or W != self.size:\n",
        "            x = F.interpolate(x, size=(self.size, self.size), mode=\"bilinear\", align_corners=False)\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x.reshape(B, T, C, self.size, self.size)\n",
        "\n",
        "\n",
        "def load_frozen_audio_encoder(path, device=\"cuda\"):\n",
        "    is_hubert = \"hubert\" in str(path).lower()\n",
        "    cls = HubertForSequenceClassification if is_hubert else Wav2Vec2ForSequenceClassification\n",
        "    model = cls.from_pretrained(path)\n",
        "    model.config.output_hidden_states = True\n",
        "    model.eval().to(device)\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    processor = Wav2Vec2FeatureExtractor.from_pretrained(path)\n",
        "    return model, processor\n",
        "\n",
        "\n",
        "def load_frozen_video_encoder(path, device=\"cuda\"):\n",
        "    model = TimesformerForVideoClassification.from_pretrained(path)\n",
        "    model.config.output_hidden_states = True\n",
        "    model.eval().to(device)\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    return model\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_audio_embedding(model, processor, audio_list, sr=16000, window_s=3.0, device=\"cuda\"):\n",
        "    L = int(window_s * sr)\n",
        "    wavs = []\n",
        "    for a in audio_list:\n",
        "        n = a.numel()\n",
        "        if n <= L:\n",
        "            a = F.pad(a, (0, L - n))\n",
        "        else:\n",
        "            start = (n - L) // 2\n",
        "            a = a[start:start + L]\n",
        "        wavs.append(a.cpu().numpy())\n",
        "    enc = processor(wavs, sampling_rate=sr, return_tensors=\"pt\",\n",
        "                    padding=True, truncation=True, max_length=L)\n",
        "    x = enc[\"input_values\"].to(device)\n",
        "    mask = enc.get(\"attention_mask\")\n",
        "    if mask is not None:\n",
        "        mask = mask.to(device)\n",
        "    out = model(input_values=x, attention_mask=mask, output_hidden_states=True)\n",
        "    return out.hidden_states[-1].mean(dim=1)\n",
        "\n",
        "\n",
        "def extract_video_embedding(model, preprocess, frames, device=\"cuda\"):\n",
        "    pv = preprocess(frames).to(device)\n",
        "    out = model(pixel_values=pv, output_hidden_states=True)\n",
        "    return out.hidden_states[-1].mean(dim=1)\n",
        "\n",
        "\n",
        "class EmotionAgreementMetric:\n",
        "    def __init__(self, threshold=0.8):\n",
        "        self.threshold = threshold\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sims = []\n",
        "\n",
        "    def update(self, audio_emb, video_emb):\n",
        "        a = F.normalize(audio_emb.detach(), p=2, dim=-1)\n",
        "        v = F.normalize(video_emb.detach(), p=2, dim=-1)\n",
        "        self.sims.extend(F.cosine_similarity(a, v, dim=-1).cpu().tolist())\n",
        "\n",
        "    def compute(self):\n",
        "        s = np.array(self.sims)\n",
        "        return {\n",
        "            \"avg_cosine_sim\": float(s.mean()),\n",
        "            \"agreement_rate\": float((s >= self.threshold).mean()),\n",
        "            \"std_cosine_sim\": float(s.std()),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}