{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import json\n",
        "import random\n",
        "import warnings\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from transformers import (\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    HubertForSequenceClassification,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    AutoImageProcessor,\n",
        "    TimesformerForVideoClassification,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "METADATA = \"/content/processed_data/metadata.json\"\n",
        "OUT_DIR = Path(\"/content/trained_encoders_v2\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EXCLUDE = {0, 7}\n",
        "REMAP = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5}\n",
        "EMOTIONS = [\"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
        "NUM_EMOTIONS = len(EMOTIONS)\n",
        "LABEL_SMOOTHING = 0.1\n",
        "\n",
        "print(f\"Device: {DEVICE}, Classes: {NUM_EMOTIONS}, Emotions: {EMOTIONS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, metadata_path, split, modality):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data\n",
        "                        if s[\"split\"] == split and s[\"emotion_idx\"] not in EXCLUDE]\n",
        "        self.modality = modality\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        item = {\"emotion\": REMAP[s[\"emotion_idx\"]]}\n",
        "        if self.modality == \"audio\":\n",
        "            wav, _ = torchaudio.load(s[\"audio_path\"])\n",
        "            item[\"audio\"] = wav.squeeze(0)\n",
        "        else:\n",
        "            frames = np.load(s[\"frames_path\"])\n",
        "            item[\"video\"] = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0\n",
        "        return item\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    out = {\"emotion\": torch.tensor([b[\"emotion\"] for b in batch])}\n",
        "    if \"audio\" in batch[0]:\n",
        "        out[\"audio\"] = [b[\"audio\"] for b in batch]\n",
        "    if \"video\" in batch[0]:\n",
        "        out[\"video\"] = torch.stack([b[\"video\"] for b in batch])\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crop_audio(wav, sr, duration, train):\n",
        "    L = int(round(duration * sr))\n",
        "    n = wav.numel()\n",
        "    if n <= L:\n",
        "        return torch.nn.functional.pad(wav, (0, L - n))\n",
        "    start = torch.randint(0, n - L + 1, ()).item() if train else (n - L) // 2\n",
        "    return wav[start:start + L]\n",
        "\n",
        "\n",
        "def crop_video(video, n_frames, train):\n",
        "    T = video.shape[0]\n",
        "    if T <= n_frames:\n",
        "        idx = torch.linspace(0, T - 1, n_frames).round().long()\n",
        "        return video[idx]\n",
        "    start = torch.randint(0, T - n_frames + 1, ()).item() if train else (T - n_frames) // 2\n",
        "    return video[start:start + n_frames]\n",
        "\n",
        "\n",
        "def prepare_audio(batch, processor, window_s, device, train=True):\n",
        "    sr = 16000\n",
        "    wavs = [crop_audio(a, sr, window_s, train).numpy() for a in batch[\"audio\"]]\n",
        "    enc = processor(wavs, sampling_rate=sr, return_tensors=\"pt\", padding=True,\n",
        "                    truncation=True, max_length=int(window_s * sr))\n",
        "    kwargs = {\"input_values\": enc[\"input_values\"].to(device)}\n",
        "    if \"attention_mask\" in enc:\n",
        "        kwargs[\"attention_mask\"] = enc[\"attention_mask\"].to(device)\n",
        "    return kwargs, batch[\"emotion\"].to(device)\n",
        "\n",
        "\n",
        "def prepare_video(batch, processor, n_frames, device, train=True):\n",
        "    clips = []\n",
        "    for v in batch[\"video\"]:\n",
        "        clip = crop_video(v, n_frames, train)\n",
        "        clips.append([clip[i].permute(1, 2, 0).numpy() for i in range(clip.shape[0])])\n",
        "    enc = processor(clips, return_tensors=\"pt\", do_rescale=False)\n",
        "    return {\"pixel_values\": enc[\"pixel_values\"].to(device)}, batch[\"emotion\"].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, prep_fn, optimizer, scaler, loss_fn):\n",
        "    model.train()\n",
        "    total_loss, preds, labels = 0.0, [], []\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        kwargs, y = prep_fn(batch, train=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            logits = model(**kwargs).logits\n",
        "            loss = loss_fn(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "        preds.extend(logits.argmax(1).detach().cpu().tolist())\n",
        "        labels.extend(y.cpu().tolist())\n",
        "    return {\n",
        "        \"loss\": total_loss / len(loader),\n",
        "        \"acc\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, prep_fn, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss, preds, labels = 0.0, [], []\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        kwargs, y = prep_fn(batch, train=False)\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            logits = model(**kwargs).logits\n",
        "            loss = loss_fn(logits, y)\n",
        "        total_loss += loss.item()\n",
        "        preds.extend(logits.argmax(1).cpu().tolist())\n",
        "        labels.extend(y.cpu().tolist())\n",
        "    return {\n",
        "        \"loss\": total_loss / len(loader),\n",
        "        \"acc\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def seed_all(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def run_experiment(cfg):\n",
        "    seed_all()\n",
        "    name, modality = cfg[\"name\"], cfg[\"modality\"]\n",
        "    print(f\"{'='*60}\\n{name}\\n{'='*60}\")\n",
        "\n",
        "    wandb.init(project=\"uncanny-valley-encoders-v2\", name=name,\n",
        "               group=modality, config=cfg, reinit=True)\n",
        "\n",
        "    train_ds = EmotionDataset(METADATA, \"train\", modality)\n",
        "    val_ds = EmotionDataset(METADATA, \"val\", modality)\n",
        "    bs = cfg.get(\"batch_size\", 8)\n",
        "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True,\n",
        "                              num_workers=0, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False,\n",
        "                            num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "    if modality == \"audio\":\n",
        "        model_cls = (HubertForSequenceClassification if \"hubert\" in cfg[\"model\"].lower()\n",
        "                     else Wav2Vec2ForSequenceClassification)\n",
        "        model = model_cls.from_pretrained(\n",
        "            cfg[\"model\"], num_labels=NUM_EMOTIONS, ignore_mismatched_sizes=True)\n",
        "        processor = Wav2Vec2FeatureExtractor.from_pretrained(cfg[\"model\"])\n",
        "        prep_fn = partial(prepare_audio, processor=processor,\n",
        "                          window_s=cfg.get(\"window_s\", 3.0), device=DEVICE)\n",
        "        if hasattr(model, \"freeze_feature_encoder\"):\n",
        "            model.freeze_feature_encoder()\n",
        "    else:\n",
        "        model = TimesformerForVideoClassification.from_pretrained(\n",
        "            cfg[\"model\"], num_labels=NUM_EMOTIONS, ignore_mismatched_sizes=True)\n",
        "        processor = AutoImageProcessor.from_pretrained(cfg[\"model\"])\n",
        "        prep_fn = partial(prepare_video, processor=processor,\n",
        "                          n_frames=cfg.get(\"n_frames\", 8), device=DEVICE)\n",
        "        for n, p in model.named_parameters():\n",
        "            if \"classifier\" not in n:\n",
        "                p.requires_grad = False\n",
        "\n",
        "    model.to(DEVICE)\n",
        "    loss_fn = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()), lr=cfg[\"lr\"])\n",
        "    scaler = GradScaler(enabled=DEVICE == \"cuda\")\n",
        "    scheduler = None\n",
        "\n",
        "    best_f1, patience_cnt = 0.0, 0\n",
        "    save_path = OUT_DIR / name\n",
        "\n",
        "    for epoch in range(cfg[\"epochs\"]):\n",
        "        freeze_ep = cfg.get(\"freeze_epochs\", 2)\n",
        "        if epoch == freeze_ep:\n",
        "            for p in model.parameters():\n",
        "                p.requires_grad = True\n",
        "            unfreeze_lr = cfg[\"lr\"] if freeze_ep == 0 else cfg[\"lr\"] * 0.1\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=unfreeze_lr)\n",
        "            scaler = GradScaler(enabled=DEVICE == \"cuda\")\n",
        "            scheduler = CosineAnnealingLR(\n",
        "                optimizer, T_max=cfg[\"epochs\"] - epoch, eta_min=1e-7)\n",
        "\n",
        "        t = train_one_epoch(model, train_loader, prep_fn, optimizer, scaler, loss_fn)\n",
        "        v = evaluate(model, val_loader, prep_fn, loss_fn)\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/loss\": t[\"loss\"], \"train/acc\": t[\"acc\"], \"train/f1\": t[\"f1\"],\n",
        "            \"val/loss\": v[\"loss\"], \"val/acc\": v[\"acc\"], \"val/f1\": v[\"f1\"],\n",
        "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "        })\n",
        "        print(f\"  [{epoch+1:2d}/{cfg['epochs']}] \"\n",
        "              f\"t_f1={t['f1']:.3f} v_f1={v['f1']:.3f} v_loss={v['loss']:.3f}\")\n",
        "\n",
        "        if v[\"f1\"] > best_f1:\n",
        "            best_f1 = v[\"f1\"]\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "            model.save_pretrained(str(save_path))\n",
        "            processor.save_pretrained(str(save_path))\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= cfg.get(\"patience\", 5):\n",
        "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    wandb.log({\"best_val_f1\": best_f1})\n",
        "    wandb.finish()\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(f\"  Best F1: {best_f1:.4f} -> {save_path}\\n\")\n",
        "    return {\"name\": name, \"best_f1\": best_f1, \"path\": str(save_path), \"modality\": modality}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENTS = [\n",
        "    # --- Audio: Wav2Vec2 base (superb-er) ---\n",
        "    {\"name\": \"w2v2-er-lr3e5\", \"modality\": \"audio\",\n",
        "     \"model\": \"superb/wav2vec2-base-superb-er\",\n",
        "     \"lr\": 3e-5, \"window_s\": 3.0, \"batch_size\": 8,\n",
        "     \"epochs\": 50, \"freeze_epochs\": 2, \"patience\": 8},\n",
        "\n",
        "    {\"name\": \"w2v2-er-lr5e5\", \"modality\": \"audio\",\n",
        "     \"model\": \"superb/wav2vec2-base-superb-er\",\n",
        "     \"lr\": 5e-5, \"window_s\": 3.0, \"batch_size\": 8,\n",
        "     \"epochs\": 50, \"freeze_epochs\": 2, \"patience\": 8},\n",
        "\n",
        "    {\"name\": \"w2v2-er-lr1e4\", \"modality\": \"audio\",\n",
        "     \"model\": \"superb/wav2vec2-base-superb-er\",\n",
        "     \"lr\": 1e-4, \"window_s\": 3.0, \"batch_size\": 8,\n",
        "     \"epochs\": 50, \"freeze_epochs\": 2, \"patience\": 8},\n",
        "\n",
        "    {\"name\": \"w2v2-er-lr5e5-w5\", \"modality\": \"audio\",\n",
        "     \"model\": \"superb/wav2vec2-base-superb-er\",\n",
        "     \"lr\": 5e-5, \"window_s\": 5.0, \"batch_size\": 8,\n",
        "     \"epochs\": 50, \"freeze_epochs\": 2, \"patience\": 8},\n",
        "\n",
        "    # --- Audio: HuBERT base (superb-er) ---\n",
        "    {\"name\": \"hubert-er-lr3e5\", \"modality\": \"audio\",\n",
        "     \"model\": \"superb/hubert-base-superb-er\",\n",
        "     \"lr\": 3e-5, \"window_s\": 3.0, \"batch_size\": 8,\n",
        "     \"epochs\": 50, \"freeze_epochs\": 2, \"patience\": 8},\n",
        "\n",
        "    {\"name\": \"hubert-er-lr5e5\", \"modality\": \"audio\",\n",
        "     \"model\": \"superb/hubert-base-superb-er\",\n",
        "     \"lr\": 5e-5, \"window_s\": 3.0, \"batch_size\": 8,\n",
        "     \"epochs\": 50, \"freeze_epochs\": 2, \"patience\": 8},\n",
        "\n",
        "    {\"name\": \"hubert-er-lr1e4\", \"modality\": \"audio\",\n",
        "     \"model\": \"superb/hubert-base-superb-er\",\n",
        "     \"lr\": 1e-4, \"window_s\": 3.0, \"batch_size\": 8,\n",
        "     \"epochs\": 50, \"freeze_epochs\": 2, \"patience\": 8},\n",
        "\n",
        "    {\"name\": \"hubert-er-lr5e5-w5\", \"modality\": \"audio\",\n",
        "     \"model\": \"superb/hubert-base-superb-er\",\n",
        "     \"lr\": 5e-5, \"window_s\": 5.0, \"batch_size\": 8,\n",
        "     \"epochs\": 50, \"freeze_epochs\": 2, \"patience\": 8},\n",
        "\n",
        "    # --- Audio: Large models ---\n",
        "    {\"name\": \"w2v2-lg-lr2e5\", \"modality\": \"audio\",\n",
        "     \"model\": \"facebook/wav2vec2-large\",\n",
        "     \"lr\": 2e-5, \"window_s\": 3.0, \"batch_size\": 4,\n",
        "     \"epochs\": 40, \"freeze_epochs\": 3, \"patience\": 7},\n",
        "\n",
        "    {\"name\": \"hubert-lg-lr2e5\", \"modality\": \"audio\",\n",
        "     \"model\": \"facebook/hubert-large-ll60k\",\n",
        "     \"lr\": 2e-5, \"window_s\": 3.0, \"batch_size\": 4,\n",
        "     \"epochs\": 40, \"freeze_epochs\": 3, \"patience\": 7},\n",
        "\n",
        "    # --- Video: TimeSformer ---\n",
        "    {\"name\": \"tsf-lr1e5-16f\", \"modality\": \"video\",\n",
        "     \"model\": \"facebook/timesformer-base-finetuned-k400\",\n",
        "     \"lr\": 1e-5, \"n_frames\": 16, \"batch_size\": 2,\n",
        "     \"epochs\": 30, \"freeze_epochs\": 1, \"patience\": 7},\n",
        "\n",
        "    {\"name\": \"tsf-lr3e5-16f\", \"modality\": \"video\",\n",
        "     \"model\": \"facebook/timesformer-base-finetuned-k400\",\n",
        "     \"lr\": 3e-5, \"n_frames\": 16, \"batch_size\": 2,\n",
        "     \"epochs\": 30, \"freeze_epochs\": 1, \"patience\": 7},\n",
        "\n",
        "    {\"name\": \"tsf-lr5e5-16f\", \"modality\": \"video\",\n",
        "     \"model\": \"facebook/timesformer-base-finetuned-k400\",\n",
        "     \"lr\": 5e-5, \"n_frames\": 16, \"batch_size\": 2,\n",
        "     \"epochs\": 30, \"freeze_epochs\": 1, \"patience\": 7},\n",
        "\n",
        "    {\"name\": \"tsf-lr3e5-16f-nf\", \"modality\": \"video\",\n",
        "     \"model\": \"facebook/timesformer-base-finetuned-k400\",\n",
        "     \"lr\": 3e-5, \"n_frames\": 16, \"batch_size\": 2,\n",
        "     \"epochs\": 30, \"freeze_epochs\": 0, \"patience\": 7},\n",
        "\n",
        "    {\"name\": \"tsf-lr3e5-8f\", \"modality\": \"video\",\n",
        "     \"model\": \"facebook/timesformer-base-finetuned-k400\",\n",
        "     \"lr\": 3e-5, \"n_frames\": 8, \"batch_size\": 4,\n",
        "     \"epochs\": 30, \"freeze_epochs\": 1, \"patience\": 7},\n",
        "\n",
        "    {\"name\": \"tsf-lr1e5-16f-f3\", \"modality\": \"video\",\n",
        "     \"model\": \"facebook/timesformer-base-finetuned-k400\",\n",
        "     \"lr\": 1e-5, \"n_frames\": 16, \"batch_size\": 2,\n",
        "     \"epochs\": 30, \"freeze_epochs\": 3, \"patience\": 7},\n",
        "]\n",
        "\n",
        "results = []\n",
        "for exp in EXPERIMENTS:\n",
        "    results.append(run_experiment(exp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*60}\")\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"{'Name':35s} {'Modality':>8s} {'Best Val F1':>12s}\")\n",
        "print(\"-\" * 58)\n",
        "for r in sorted(results, key=lambda x: -x[\"best_f1\"]):\n",
        "    print(f\"{r['name']:35s} {r['modality']:>8s} {r['best_f1']:12.4f}\")\n",
        "\n",
        "best_audio = max((r for r in results if r[\"modality\"] == \"audio\"), key=lambda x: x[\"best_f1\"])\n",
        "best_video = max((r for r in results if r[\"modality\"] == \"video\"), key=lambda x: x[\"best_f1\"])\n",
        "print(f\"\\nBest audio: {best_audio['name']} (F1={best_audio['best_f1']:.4f})\")\n",
        "print(f\"Best video: {best_video['name']} (F1={best_video['best_f1']:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def collect_predictions(model, loader, prep_fn):\n",
        "    model.eval()\n",
        "    all_preds, all_labels, all_losses = [], [], []\n",
        "    for batch in loader:\n",
        "        kwargs, y = prep_fn(batch, train=False)\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            logits = model(**kwargs).logits\n",
        "            loss = nn.CrossEntropyLoss(reduction=\"none\")(logits, y)\n",
        "        all_preds.extend(logits.argmax(1).cpu().tolist())\n",
        "        all_labels.extend(y.cpu().tolist())\n",
        "        all_losses.extend(loss.cpu().tolist())\n",
        "    return np.array(all_preds), np.array(all_labels), np.array(all_losses)\n",
        "\n",
        "\n",
        "def per_emotion_report(preds, labels, losses, title):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"  {title}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    print(classification_report(labels, preds, target_names=EMOTIONS, digits=3, zero_division=0))\n",
        "\n",
        "    per_f1 = f1_score(labels, preds, average=None,\n",
        "                      labels=list(range(NUM_EMOTIONS)), zero_division=0)\n",
        "    cm = confusion_matrix(labels, preds, labels=list(range(NUM_EMOTIONS)))\n",
        "\n",
        "    print(f\"{'Emotion':<12s} {'N':>5s} {'Acc':>6s} {'F1':>6s} {'AvgLoss':>8s}\")\n",
        "    print(\"-\" * 40)\n",
        "    for i, emo in enumerate(EMOTIONS):\n",
        "        mask = labels == i\n",
        "        n = mask.sum()\n",
        "        if n == 0:\n",
        "            continue\n",
        "        acc = (preds[mask] == i).mean()\n",
        "        avg_loss = losses[mask].mean()\n",
        "        print(f\"{emo:<12s} {n:5d} {acc:6.3f} {per_f1[i]:6.3f} {avg_loss:8.3f}\")\n",
        "\n",
        "    worst = np.argsort(per_f1)\n",
        "    print(f\"\\nWeakest: \", end=\"\")\n",
        "    print(\" < \".join(f\"{EMOTIONS[i]} ({per_f1[i]:.3f})\" for i in worst[:3]))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1\n",
        "    cm_norm = cm / row_sums\n",
        "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
        "                xticklabels=EMOTIONS, yticklabels=EMOTIONS, ax=ax,\n",
        "                vmin=0, vmax=1, linewidths=0.5)\n",
        "    for i in range(NUM_EMOTIONS):\n",
        "        for j in range(NUM_EMOTIONS):\n",
        "            if cm[i, j] > 0:\n",
        "                ax.text(j + 0.5, i + 0.72, f\"({cm[i,j]})\",\n",
        "                        ha=\"center\", va=\"center\", fontsize=7, color=\"gray\")\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    ax.set_title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return per_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_audio = EmotionDataset(METADATA, \"val\", \"audio\")\n",
        "audio_loader = DataLoader(val_audio, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "audio_cfg = next(e for e in EXPERIMENTS if e[\"name\"] == best_audio[\"name\"])\n",
        "if \"hubert\" in best_audio[\"name\"]:\n",
        "    audio_model = HubertForSequenceClassification.from_pretrained(best_audio[\"path\"]).to(DEVICE)\n",
        "else:\n",
        "    audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(best_audio[\"path\"]).to(DEVICE)\n",
        "audio_processor = Wav2Vec2FeatureExtractor.from_pretrained(best_audio[\"path\"])\n",
        "audio_prep = partial(prepare_audio, processor=audio_processor,\n",
        "                     window_s=audio_cfg.get(\"window_s\", 3.0), device=DEVICE)\n",
        "\n",
        "print(f\"Evaluating: {best_audio['name']} (F1={best_audio['best_f1']:.4f})\")\n",
        "a_preds, a_labels, a_losses = collect_predictions(audio_model, audio_loader, audio_prep)\n",
        "a_f1 = per_emotion_report(a_preds, a_labels, a_losses, f\"AUDIO — {best_audio['name']}\")\n",
        "\n",
        "del audio_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_video = EmotionDataset(METADATA, \"val\", \"video\")\n",
        "video_loader = DataLoader(val_video, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "video_cfg = next(e for e in EXPERIMENTS if e[\"name\"] == best_video[\"name\"])\n",
        "video_model = TimesformerForVideoClassification.from_pretrained(best_video[\"path\"]).to(DEVICE)\n",
        "video_processor = AutoImageProcessor.from_pretrained(best_video[\"path\"])\n",
        "video_prep = partial(prepare_video, processor=video_processor,\n",
        "                     n_frames=video_cfg.get(\"n_frames\", 16), device=DEVICE)\n",
        "\n",
        "print(f\"Evaluating: {best_video['name']} (F1={best_video['best_f1']:.4f})\")\n",
        "v_preds, v_labels, v_losses = collect_predictions(video_model, video_loader, video_prep)\n",
        "v_f1 = per_emotion_report(v_preds, v_labels, v_losses, f\"VIDEO — {best_video['name']}\")\n",
        "\n",
        "del video_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "x = np.arange(NUM_EMOTIONS)\n",
        "w = 0.35\n",
        "\n",
        "ax.bar(x - w/2, a_f1, w, label=f\"Audio ({best_audio['name']})\", color=\"#4C72B0\")\n",
        "ax.bar(x + w/2, v_f1, w, label=f\"Video ({best_video['name']})\", color=\"#DD8452\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(EMOTIONS, rotation=30, ha=\"right\")\n",
        "ax.set_ylabel(\"F1 Score\")\n",
        "ax.set_title(\"Per-Emotion F1: Audio vs Video (6 classes)\")\n",
        "ax.set_ylim(0, 1)\n",
        "ax.legend()\n",
        "ax.grid(axis=\"y\", alpha=0.3)\n",
        "for i in x:\n",
        "    ax.text(i - w/2, a_f1[i] + 0.02, f\"{a_f1[i]:.2f}\", ha=\"center\", fontsize=8)\n",
        "    ax.text(i + w/2, v_f1[i] + 0.02, f\"{v_f1[i]:.2f}\", ha=\"center\", fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"CROSS-MODAL GAPS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"{'Emotion':<12s} {'Audio F1':>9s} {'Video F1':>9s} {'Gap':>7s}\")\n",
        "print(\"-\" * 42)\n",
        "for i, emo in enumerate(EMOTIONS):\n",
        "    gap = abs(a_f1[i] - v_f1[i])\n",
        "    better = \"A\" if a_f1[i] > v_f1[i] else \"V\"\n",
        "    print(f\"{emo:<12s} {a_f1[i]:9.3f} {v_f1[i]:9.3f} {gap:7.3f} ({better})\")\n",
        "\n",
        "overall_a = f1_score(a_labels, a_preds, average=\"weighted\")\n",
        "overall_v = f1_score(v_labels, v_preds, average=\"weighted\")\n",
        "print(f\"\\nOverall weighted F1 — Audio: {overall_a:.4f}, Video: {overall_v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"GPU RAM cleaned.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
