{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AcIdlpbRYE0",
        "outputId": "24a36ea4-8715-4fb2-f6ad-f2a31e3caeb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 189M Feb 16 02:27 Wav2Lip/checkpoints/lipsync_expert.pth\n",
            "-rw-r--r-- 1 root root 416M Feb 16 02:27 Wav2Lip/checkpoints/wav2lip.pth\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers librosa wandb\n",
        "!git clone https://github.com/Rudrabha/Wav2Lip.git 2>/dev/null || true\n",
        "!mkdir -p Wav2Lip/checkpoints\n",
        "\n",
        "!wget -q -nc \"https://huggingface.co/Nekochu/Wav2Lip/resolve/main/wav2lip.pth\" -O Wav2Lip/checkpoints/wav2lip.pth\n",
        "!wget -q -nc \"https://huggingface.co/Nekochu/Wav2Lip/resolve/main/lipsync_expert.pth\" -O Wav2Lip/checkpoints/lipsync_expert.pth\n",
        "!ls -lh Wav2Lip/checkpoints/*.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhwx7fo5RYE0",
        "outputId": "40513c8a-9b8e-4b82-fe56-3525148568c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content\")\n",
        "sys.path.insert(0, \"/content/Wav2Lip\")\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import wandb\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from emotion_utils import (\n",
        "    CrossModalEmotionLoss,\n",
        "    DifferentiableVideoPreprocess,\n",
        "    EmotionAgreementMetric,\n",
        "    load_frozen_audio_encoder,\n",
        "    load_frozen_video_encoder,\n",
        "    extract_audio_embedding,\n",
        "    extract_video_embedding,\n",
        ")\n",
        "from models.wav2lip import Wav2Lip as Wav2LipModel\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "METADATA = \"/content/processed_data/metadata.json\"\n",
        "WAV2LIP_CKPT = \"/content/Wav2Lip/checkpoints/wav2lip.pth\"\n",
        "BEST_AUDIO_PATH = \"/content/trained_encoders_v2/w2v2-lg-lr2e5\"\n",
        "BEST_VIDEO_PATH = \"/content/trained_encoders_v2/tsf-lr3e5-16f-nf\"\n",
        "OUT_DIR = Path(\"/content/wav2lip_finetuned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EXCLUDE = {0, 1, 3, 5, 7}\n",
        "REMAP = {2: 0, 4: 1, 6: 2}\n",
        "EMOTIONS = [\"happy\", \"angry\", \"disgust\"]\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "U1odWXrQRYE1"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 96\n",
        "MEL_STEP = 16\n",
        "SR = 16000\n",
        "FPS = 25\n",
        "\n",
        "def wav_to_mel(wav_path, sr=SR):\n",
        "    y, _ = librosa.load(wav_path, sr=sr)\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=y, sr=sr, n_mels=80, hop_length=200, win_length=800,\n",
        "        fmin=55, fmax=7600)\n",
        "    return librosa.power_to_db(mel, ref=np.max).astype(np.float32)\n",
        "\n",
        "\n",
        "class Wav2LipDataset(Dataset):\n",
        "    def __init__(self, metadata_path, split, T=5):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data\n",
        "                        if s[\"split\"] == split and s[\"emotion_idx\"] not in EXCLUDE]\n",
        "        self.T = T\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "\n",
        "        wav, sr = torchaudio.load(s[\"audio_path\"])\n",
        "        audio_1d = wav.squeeze(0)\n",
        "\n",
        "        mel = wav_to_mel(s[\"audio_path\"])\n",
        "\n",
        "        frames = np.load(s[\"frames_path\"]).astype(np.float32) / 255.0\n",
        "        n_frames = frames.shape[0]\n",
        "\n",
        "        start = np.random.randint(0, max(1, n_frames - self.T))\n",
        "        face_window = frames[start:start + self.T]\n",
        "        if face_window.shape[0] < self.T:\n",
        "            pad = np.repeat(face_window[-1:], self.T - face_window.shape[0], axis=0)\n",
        "            face_window = np.concatenate([face_window, pad], axis=0)\n",
        "\n",
        "        mel_start = int(start / FPS * SR / 200)\n",
        "        mel_end = mel_start + MEL_STEP * self.T\n",
        "        mel_window = mel[:, mel_start:mel_end]\n",
        "        if mel_window.shape[1] < MEL_STEP * self.T:\n",
        "            mel_window = np.pad(mel_window, ((0, 0), (0, MEL_STEP * self.T - mel_window.shape[1])))\n",
        "\n",
        "        gt = torch.from_numpy(face_window).permute(0, 3, 1, 2)\n",
        "        H, W = gt.shape[2], gt.shape[3]\n",
        "        if H != IMG_SIZE or W != IMG_SIZE:\n",
        "            gt = F.interpolate(gt, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        masked = gt.clone()\n",
        "        masked[:, :, IMG_SIZE // 2:, :] = 0.0\n",
        "\n",
        "        ref_idx = np.random.randint(0, n_frames)\n",
        "        ref = torch.from_numpy(frames[ref_idx]).permute(2, 0, 1).unsqueeze(0).expand(self.T, -1, -1, -1)\n",
        "        if ref.shape[2] != IMG_SIZE or ref.shape[3] != IMG_SIZE:\n",
        "            ref = F.interpolate(ref, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        face_input = torch.cat([ref, masked], dim=1)\n",
        "\n",
        "        mel_chunks = []\n",
        "        for t in range(self.T):\n",
        "            m = mel_window[:, t * MEL_STEP:(t + 1) * MEL_STEP]\n",
        "            mel_chunks.append(torch.from_numpy(m).unsqueeze(0))\n",
        "        mel_tensor = torch.stack(mel_chunks, dim=0)\n",
        "\n",
        "        return {\n",
        "            \"mel\": mel_tensor,\n",
        "            \"face_input\": face_input,\n",
        "            \"gt\": gt,\n",
        "            \"audio\": audio_1d,\n",
        "            \"emotion\": REMAP[s[\"emotion_idx\"]],\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_wav2lip(batch):\n",
        "    return {\n",
        "        \"mel\": torch.stack([b[\"mel\"] for b in batch]),\n",
        "        \"face_input\": torch.stack([b[\"face_input\"] for b in batch]),\n",
        "        \"gt\": torch.stack([b[\"gt\"] for b in batch]),\n",
        "        \"audio\": [b[\"audio\"] for b in batch],\n",
        "        \"emotion\": torch.tensor([b[\"emotion\"] for b in batch]),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SssQyRVdRYE1",
        "outputId": "a7b184ef-2783-4984-fad4-c6209bfb8883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wav2Lip loaded: 36.3M params\n",
            "Frozen encoders loaded. Video: 8 frames. Audio dim=1024, Video dim=768, Proj dim=256\n"
          ]
        }
      ],
      "source": [
        "def load_wav2lip(ckpt_path, device):\n",
        "    model = Wav2LipModel()\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
        "    state = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
        "    state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
        "    model.load_state_dict(state, strict=False)\n",
        "    return model.to(device)\n",
        "\n",
        "wav2lip = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "print(f\"Wav2Lip loaded: {sum(p.numel() for p in wav2lip.parameters()) / 1e6:.1f}M params\")\n",
        "\n",
        "audio_enc, audio_proc = load_frozen_audio_encoder(BEST_AUDIO_PATH, DEVICE)\n",
        "video_enc = load_frozen_video_encoder(BEST_VIDEO_PATH, DEVICE)\n",
        "video_preprocess = DifferentiableVideoPreprocess(224).to(DEVICE)\n",
        "\n",
        "VIDEO_ENC_FRAMES = getattr(video_enc.config, \"num_frames\", 8)\n",
        "AUDIO_DIM = audio_enc.config.hidden_size\n",
        "VIDEO_DIM = video_enc.config.hidden_size\n",
        "PROJ_DIM = 256\n",
        "print(f\"Frozen encoders loaded. Video: {VIDEO_ENC_FRAMES} frames. \"\n",
        "      f\"Audio dim={AUDIO_DIM}, Video dim={VIDEO_DIM}, Proj dim={PROJ_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrL9lg7bRYE1",
        "outputId": "610f37ac-1939-4e3d-a7c1-825e6e6a6101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ],
      "source": [
        "wandb.login()\n",
        "\n",
        "CONFIGS = [\n",
        "    {\"name\": \"wav2lip-baseline\", \"lambda_emo\": 0.0},\n",
        "    {\"name\": \"wav2lip-emo-001\",  \"lambda_emo\": 0.01},\n",
        "    {\"name\": \"wav2lip-emo-005\",  \"lambda_emo\": 0.05},\n",
        "    {\"name\": \"wav2lip-emo-01\",   \"lambda_emo\": 0.1},\n",
        "]\n",
        "\n",
        "LR = 1e-4\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 4\n",
        "PATIENCE = 5\n",
        "T_FRAMES = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "MMsiebgnRYE1"
      },
      "outputs": [],
      "source": [
        "def adapt_frames(frames, target_t):\n",
        "    \"\"\"Resample (B, T, C, H, W) to (B, target_t, C, H, W) via uniform index sampling.\"\"\"\n",
        "    B, T, C, H, W = frames.shape\n",
        "    if T == target_t:\n",
        "        return frames\n",
        "    idx = torch.linspace(0, T - 1, target_t).long()\n",
        "    return frames[:, idx]\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scaler, emotion_loss_fn, lambda_emo):\n",
        "    model.train()\n",
        "    total_recon, total_emo, total_loss = 0.0, 0.0, 0.0\n",
        "\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        B, T = mel.shape[0], mel.shape[1]\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        all_gen = []\n",
        "        recon = 0.0\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            for t in range(T):\n",
        "                gen = model(mel[:, t], face_in[:, t])\n",
        "                recon += F.l1_loss(gen, gt[:, t])\n",
        "                all_gen.append(gen)\n",
        "            recon = recon / T\n",
        "\n",
        "            emo = torch.tensor(0.0, device=DEVICE)\n",
        "            if lambda_emo > 0:\n",
        "                gen_video = torch.stack(all_gen, dim=1)\n",
        "                gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "                audio_emb = extract_audio_embedding(\n",
        "                    audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                video_emb = extract_video_embedding(\n",
        "                    video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                emo = emotion_loss_fn(audio_proj(audio_emb.detach()),\n",
        "                                      video_proj(video_emb))\n",
        "\n",
        "            loss = recon + lambda_emo * emo\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_recon += recon.item()\n",
        "        total_emo += emo.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {\"recon\": total_recon / n, \"emotion\": total_emo / n, \"total\": total_loss / n}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, emotion_loss_fn, lambda_emo):\n",
        "    model.eval()\n",
        "    total_recon, total_emo, total_loss = 0.0, 0.0, 0.0\n",
        "    metric = EmotionAgreementMetric()\n",
        "\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        B, T = mel.shape[0], mel.shape[1]\n",
        "\n",
        "        all_gen = []\n",
        "        recon = 0.0\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            for t in range(T):\n",
        "                gen = model(mel[:, t], face_in[:, t])\n",
        "                recon += F.l1_loss(gen, gt[:, t])\n",
        "                all_gen.append(gen)\n",
        "            recon = recon / T\n",
        "\n",
        "            emo = torch.tensor(0.0, device=DEVICE)\n",
        "            if lambda_emo > 0:\n",
        "                gen_video = torch.stack(all_gen, dim=1)\n",
        "                gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "                audio_emb = extract_audio_embedding(\n",
        "                    audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                video_emb = extract_video_embedding(\n",
        "                    video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                a_p, v_p = audio_proj(audio_emb), video_proj(video_emb)\n",
        "                emo = emotion_loss_fn(a_p, v_p)\n",
        "                metric.update(a_p, v_p)\n",
        "\n",
        "            loss = recon + lambda_emo * emo\n",
        "\n",
        "        total_recon += recon.item()\n",
        "        total_emo += emo.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    result = {\"recon\": total_recon / n, \"emotion\": total_emo / n, \"total\": total_loss / n}\n",
        "    if lambda_emo > 0:\n",
        "        result.update(metric.compute())\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sFEJ98kMRYE1",
        "outputId": "168d5f36-949a-4d35-ec80-58eaadc770e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 408, Val: 72\n",
            "\n",
            "============================================================\n",
            "wav2lip-baseline (lambda_emo=0.0)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wav2lip-emo-001</strong> at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/xyzaq6g2' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/xyzaq6g2</a><br> View project at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260216_030753-xyzaq6g2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260216_032119-r79w8msk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/r79w8msk' target=\"_blank\">wav2lip-baseline</a></strong> to <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/r79w8msk' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/r79w8msk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 1/20] t_loss=0.3554 v_loss=0.2947 v_recon=0.2947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 2/20] t_loss=0.2695 v_loss=0.2390 v_recon=0.2390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 3/20] t_loss=0.2162 v_loss=0.1924 v_recon=0.1924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 4/20] t_loss=0.1854 v_loss=0.1680 v_recon=0.1680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 5/20] t_loss=0.1505 v_loss=0.1374 v_recon=0.1374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 6/20] t_loss=0.1255 v_loss=0.1210 v_recon=0.1210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 7/20] t_loss=0.1119 v_loss=0.1091 v_recon=0.1091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 8/20] t_loss=0.1051 v_loss=0.0974 v_recon=0.0974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 9/20] t_loss=0.0941 v_loss=0.0895 v_recon=0.0895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [10/20] t_loss=0.0873 v_loss=0.0829 v_recon=0.0829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [11/20] t_loss=0.0840 v_loss=0.0788 v_recon=0.0788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [12/20] t_loss=0.0757 v_loss=0.0694 v_recon=0.0694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [13/20] t_loss=0.0687 v_loss=0.0673 v_recon=0.0673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [14/20] t_loss=0.0643 v_loss=0.0669 v_recon=0.0669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [15/20] t_loss=0.0604 v_loss=0.0677 v_recon=0.0677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [16/20] t_loss=0.0571 v_loss=0.0555 v_recon=0.0555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [17/20] t_loss=0.0538 v_loss=0.0581 v_recon=0.0581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [18/20] t_loss=0.0513 v_loss=0.0506 v_recon=0.0506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [19/20] t_loss=0.0493 v_loss=0.0502 v_recon=0.0502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [20/20] t_loss=0.0477 v_loss=0.0533 v_recon=0.0533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/emotion</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/recon</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/emotion</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/recon</td><td>█▆▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/total</td><td>█▆▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train/emotion</td><td>0</td></tr><tr><td>train/recon</td><td>0.04765</td></tr><tr><td>train/total</td><td>0.04765</td></tr><tr><td>val/emotion</td><td>0</td></tr><tr><td>val/recon</td><td>0.05332</td></tr><tr><td>val/total</td><td>0.05332</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wav2lip-baseline</strong> at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/r79w8msk' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/r79w8msk</a><br> View project at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260216_032119-r79w8msk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best val loss: 0.0502 -> /content/wav2lip_finetuned/wav2lip-baseline\n",
            "\n",
            "============================================================\n",
            "wav2lip-emo-001 (lambda_emo=0.01)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260216_033451-kxzrpc8h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/kxzrpc8h' target=\"_blank\">wav2lip-emo-001</a></strong> to <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/kxzrpc8h' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/kxzrpc8h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 1/20] t_loss=0.3581 v_loss=0.3065 v_recon=0.3049 cos_sim=0.837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 2/20] t_loss=0.2695 v_loss=0.2310 v_recon=0.2295 cos_sim=0.845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 3/20] t_loss=0.2197 v_loss=0.2044 v_recon=0.2029 cos_sim=0.848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 4/20] t_loss=0.1870 v_loss=0.1819 v_recon=0.1804 cos_sim=0.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 5/20] t_loss=0.1521 v_loss=0.1485 v_recon=0.1470 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 6/20] t_loss=0.1274 v_loss=0.1217 v_recon=0.1202 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 7/20] t_loss=0.1136 v_loss=0.1108 v_recon=0.1094 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 8/20] t_loss=0.1018 v_loss=0.0988 v_recon=0.0973 cos_sim=0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 9/20] t_loss=0.0947 v_loss=0.1003 v_recon=0.0989 cos_sim=0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [10/20] t_loss=0.0875 v_loss=0.0843 v_recon=0.0828 cos_sim=0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [11/20] t_loss=0.0812 v_loss=0.0845 v_recon=0.0831 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [12/20] t_loss=0.0754 v_loss=0.0762 v_recon=0.0748 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [13/20] t_loss=0.0696 v_loss=0.0728 v_recon=0.0713 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [14/20] t_loss=0.0645 v_loss=0.0653 v_recon=0.0638 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [15/20] t_loss=0.0603 v_loss=0.0602 v_recon=0.0587 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [16/20] t_loss=0.0574 v_loss=0.0647 v_recon=0.0632 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [17/20] t_loss=0.0550 v_loss=0.0557 v_recon=0.0542 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [18/20] t_loss=0.0522 v_loss=0.0519 v_recon=0.0504 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [19/20] t_loss=0.0502 v_loss=0.0546 v_recon=0.0531 cos_sim=0.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [20/20] t_loss=0.0476 v_loss=0.0501 v_recon=0.0486 cos_sim=0.850\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/emotion</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/recon</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/agreement_rate</td><td>▁▅▅█████████████████</td></tr><tr><td>val/avg_cosine_sim</td><td>▁▄▆▆▇███████▇▇▇▇▇▇▇▆</td></tr><tr><td>val/emotion</td><td>█▅▃▃▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃</td></tr><tr><td>val/recon</td><td>█▆▅▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val/total</td><td>█▆▅▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train/emotion</td><td>0.00087</td></tr><tr><td>train/recon</td><td>0.04755</td></tr><tr><td>train/total</td><td>0.04756</td></tr><tr><td>val/agreement_rate</td><td>0.90278</td></tr><tr><td>val/avg_cosine_sim</td><td>0.85016</td></tr><tr><td>val/emotion</td><td>0.14984</td></tr><tr><td>val/recon</td><td>0.04863</td></tr><tr><td>val/total</td><td>0.05013</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wav2lip-emo-001</strong> at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/kxzrpc8h' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/kxzrpc8h</a><br> View project at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260216_033451-kxzrpc8h/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best val loss: 0.0501 -> /content/wav2lip_finetuned/wav2lip-emo-001\n",
            "\n",
            "============================================================\n",
            "wav2lip-emo-005 (lambda_emo=0.05)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260216_035304-1jjdblc7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/1jjdblc7' target=\"_blank\">wav2lip-emo-005</a></strong> to <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/1jjdblc7' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/1jjdblc7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 1/20] t_loss=0.3701 v_loss=0.3289 v_recon=0.3208 cos_sim=0.837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 2/20] t_loss=0.2760 v_loss=0.2468 v_recon=0.2391 cos_sim=0.846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 3/20] t_loss=0.2236 v_loss=0.2092 v_recon=0.2017 cos_sim=0.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 4/20] t_loss=0.1886 v_loss=0.1851 v_recon=0.1777 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 5/20] t_loss=0.1563 v_loss=0.1402 v_recon=0.1328 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 6/20] t_loss=0.1283 v_loss=0.1298 v_recon=0.1224 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 7/20] t_loss=0.1133 v_loss=0.1164 v_recon=0.1090 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 8/20] t_loss=0.1045 v_loss=0.1040 v_recon=0.0967 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 9/20] t_loss=0.0958 v_loss=0.1008 v_recon=0.0934 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [10/20] t_loss=0.0887 v_loss=0.0957 v_recon=0.0884 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [11/20] t_loss=0.0830 v_loss=0.0883 v_recon=0.0809 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/102 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "train_ds = Wav2LipDataset(METADATA, \"train\", T=T_FRAMES)\n",
        "val_ds = Wav2LipDataset(METADATA, \"val\", T=T_FRAMES)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=0, collate_fn=collate_wav2lip)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=0, collate_fn=collate_wav2lip)\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for cfg in CONFIGS:\n",
        "    name = cfg[\"name\"]\n",
        "    lambda_emo = cfg[\"lambda_emo\"]\n",
        "    print(f\"\\n{'='*60}\\n{name} (lambda_emo={lambda_emo})\\n{'='*60}\")\n",
        "\n",
        "    wandb.init(project=\"uncanny-valley-wav2lip\", name=name,\n",
        "               config={**cfg, \"lr\": LR, \"epochs\": EPOCHS}, reinit=True)\n",
        "\n",
        "    model = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "    audio_proj = nn.Linear(AUDIO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "    video_proj = nn.Linear(VIDEO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "    params = list(model.parameters())\n",
        "    if lambda_emo > 0:\n",
        "        params += list(audio_proj.parameters()) + list(video_proj.parameters())\n",
        "    optimizer = torch.optim.AdamW(params, lr=LR)\n",
        "    scaler = GradScaler(enabled=DEVICE == \"cuda\")\n",
        "    emotion_loss_fn = CrossModalEmotionLoss(weight=1.0)\n",
        "\n",
        "    best_val, patience_cnt = float(\"inf\"), 0\n",
        "    save_path = OUT_DIR / name\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t = train_one_epoch(model, train_loader, optimizer, scaler,\n",
        "                            emotion_loss_fn, lambda_emo)\n",
        "        v = evaluate(model, val_loader, emotion_loss_fn, lambda_emo)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/recon\": t[\"recon\"], \"train/emotion\": t[\"emotion\"], \"train/total\": t[\"total\"],\n",
        "            \"val/recon\": v[\"recon\"], \"val/emotion\": v[\"emotion\"], \"val/total\": v[\"total\"],\n",
        "            **{f\"val/{k}\": v[k] for k in [\"avg_cosine_sim\", \"agreement_rate\"] if k in v},\n",
        "        })\n",
        "\n",
        "        print(f\"  [{epoch+1:2d}/{EPOCHS}] \"\n",
        "              f\"t_loss={t['total']:.4f} v_loss={v['total']:.4f} v_recon={v['recon']:.4f}\"\n",
        "              + (f\" cos_sim={v.get('avg_cosine_sim', 0):.3f}\" if lambda_emo > 0 else \"\"))\n",
        "\n",
        "        if v[\"total\"] < best_val:\n",
        "            best_val = v[\"total\"]\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(model.state_dict(), save_path / \"wav2lip.pth\")\n",
        "            torch.save({\"audio_proj\": audio_proj.state_dict(),\n",
        "                         \"video_proj\": video_proj.state_dict()},\n",
        "                        save_path / \"projections.pth\")\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= PATIENCE:\n",
        "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, scaler, audio_proj, video_proj\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    all_results.append({\"name\": name, \"lambda_emo\": lambda_emo, \"best_val\": best_val})\n",
        "    print(f\"  Best val loss: {best_val:.4f} -> {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE7_BkkGRYE1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(all_results).sort_values(\"best_val\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.bar(df[\"name\"], df[\"best_val\"], color=\"steelblue\")\n",
        "ax.set_ylabel(\"Best Val Loss\")\n",
        "ax.set_title(\"Wav2Lip Fine-tuning: λ_emo Ablation\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KdaP1RtRYE2"
      },
      "outputs": [],
      "source": [
        "best_name = df.iloc[0][\"name\"]\n",
        "best_model = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "best_model.load_state_dict(torch.load(OUT_DIR / best_name / \"wav2lip.pth\", map_location=DEVICE, weights_only=True))\n",
        "best_model.eval()\n",
        "\n",
        "audio_proj = nn.Linear(AUDIO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "video_proj = nn.Linear(VIDEO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "proj_ckpt = torch.load(OUT_DIR / best_name / \"projections.pth\", map_location=DEVICE, weights_only=True)\n",
        "audio_proj.load_state_dict(proj_ckpt[\"audio_proj\"])\n",
        "video_proj.load_state_dict(proj_ckpt[\"video_proj\"])\n",
        "audio_proj.eval()\n",
        "video_proj.eval()\n",
        "print(f\"Loaded best model: {best_name}\")\n",
        "\n",
        "metric = EmotionAgreementMetric()\n",
        "all_recon = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Evaluating best\"):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        T = mel.shape[1]\n",
        "\n",
        "        all_gen = []\n",
        "        for t in range(T):\n",
        "            gen = best_model(mel[:, t], face_in[:, t])\n",
        "            all_gen.append(gen)\n",
        "            all_recon.append(F.l1_loss(gen, gt[:, t]).item())\n",
        "\n",
        "        gen_video = torch.stack(all_gen, dim=1)\n",
        "        gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "        audio_emb = extract_audio_embedding(audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "        video_emb = extract_video_embedding(video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "        metric.update(audio_proj(audio_emb), video_proj(video_emb))\n",
        "\n",
        "agreement = metric.compute()\n",
        "print(f\"\\nBest model evaluation:\")\n",
        "print(f\"  Avg L1 recon:     {np.mean(all_recon):.4f}\")\n",
        "print(f\"  Avg cosine sim:   {agreement['avg_cosine_sim']:.4f}\")\n",
        "print(f\"  Agreement rate:   {agreement['agreement_rate']:.4f}\")\n",
        "print(f\"  Std cosine sim:   {agreement['std_cosine_sim']:.4f}\")\n",
        "\n",
        "del best_model\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}