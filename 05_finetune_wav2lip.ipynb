{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AcIdlpbRYE0",
        "outputId": "24a36ea4-8715-4fb2-f6ad-f2a31e3caeb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 189M Feb 16 02:27 Wav2Lip/checkpoints/lipsync_expert.pth\n",
            "-rw-r--r-- 1 root root 416M Feb 16 02:27 Wav2Lip/checkpoints/wav2lip.pth\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers librosa wandb\n",
        "!git clone https://github.com/Rudrabha/Wav2Lip.git 2>/dev/null || true\n",
        "!mkdir -p Wav2Lip/checkpoints\n",
        "\n",
        "!wget -q -nc \"https://huggingface.co/Nekochu/Wav2Lip/resolve/main/wav2lip.pth\" -O Wav2Lip/checkpoints/wav2lip.pth\n",
        "!wget -q -nc \"https://huggingface.co/Nekochu/Wav2Lip/resolve/main/lipsync_expert.pth\" -O Wav2Lip/checkpoints/lipsync_expert.pth\n",
        "!ls -lh Wav2Lip/checkpoints/*.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhwx7fo5RYE0",
        "outputId": "40513c8a-9b8e-4b82-fe56-3525148568c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content\")\n",
        "sys.path.insert(0, \"/content/Wav2Lip\")\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import wandb\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from emotion_utils import (\n",
        "    CrossModalEmotionLoss,\n",
        "    DifferentiableVideoPreprocess,\n",
        "    EmotionAgreementMetric,\n",
        "    load_frozen_audio_encoder,\n",
        "    load_frozen_video_encoder,\n",
        "    extract_audio_embedding,\n",
        "    extract_video_embedding,\n",
        ")\n",
        "from models.wav2lip import Wav2Lip as Wav2LipModel\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "METADATA = \"/content/processed_data/metadata.json\"\n",
        "WAV2LIP_CKPT = \"/content/Wav2Lip/checkpoints/wav2lip.pth\"\n",
        "BEST_AUDIO_PATH = \"/content/trained_encoders_v2/w2v2-lg-lr2e5\"\n",
        "BEST_VIDEO_PATH = \"/content/trained_encoders_v2/tsf-lr3e5-16f-nf\"\n",
        "OUT_DIR = Path(\"/content/wav2lip_finetuned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EXCLUDE = {0, 1, 3, 5, 7}\n",
        "REMAP = {2: 0, 4: 1, 6: 2}\n",
        "EMOTIONS = [\"happy\", \"angry\", \"disgust\"]\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "U1odWXrQRYE1"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 96\n",
        "MEL_STEP = 16\n",
        "SR = 16000\n",
        "FPS = 25\n",
        "\n",
        "def wav_to_mel(wav_path, sr=SR):\n",
        "    y, _ = librosa.load(wav_path, sr=sr)\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=y, sr=sr, n_mels=80, hop_length=200, win_length=800,\n",
        "        fmin=55, fmax=7600)\n",
        "    return librosa.power_to_db(mel, ref=np.max).astype(np.float32)\n",
        "\n",
        "\n",
        "class Wav2LipDataset(Dataset):\n",
        "    def __init__(self, metadata_path, split, T=5):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data\n",
        "                        if s[\"split\"] == split and s[\"emotion_idx\"] not in EXCLUDE]\n",
        "        self.T = T\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "\n",
        "        wav, sr = torchaudio.load(s[\"audio_path\"])\n",
        "        audio_1d = wav.squeeze(0)\n",
        "\n",
        "        mel = wav_to_mel(s[\"audio_path\"])\n",
        "\n",
        "        frames = np.load(s[\"frames_path\"]).astype(np.float32) / 255.0\n",
        "        n_frames = frames.shape[0]\n",
        "\n",
        "        start = np.random.randint(0, max(1, n_frames - self.T))\n",
        "        face_window = frames[start:start + self.T]\n",
        "        if face_window.shape[0] < self.T:\n",
        "            pad = np.repeat(face_window[-1:], self.T - face_window.shape[0], axis=0)\n",
        "            face_window = np.concatenate([face_window, pad], axis=0)\n",
        "\n",
        "        mel_start = int(start / FPS * SR / 200)\n",
        "        mel_end = mel_start + MEL_STEP * self.T\n",
        "        mel_window = mel[:, mel_start:mel_end]\n",
        "        if mel_window.shape[1] < MEL_STEP * self.T:\n",
        "            mel_window = np.pad(mel_window, ((0, 0), (0, MEL_STEP * self.T - mel_window.shape[1])))\n",
        "\n",
        "        gt = torch.from_numpy(face_window).permute(0, 3, 1, 2)\n",
        "        H, W = gt.shape[2], gt.shape[3]\n",
        "        if H != IMG_SIZE or W != IMG_SIZE:\n",
        "            gt = F.interpolate(gt, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        masked = gt.clone()\n",
        "        masked[:, :, IMG_SIZE // 2:, :] = 0.0\n",
        "\n",
        "        ref_idx = np.random.randint(0, n_frames)\n",
        "        ref = torch.from_numpy(frames[ref_idx]).permute(2, 0, 1).unsqueeze(0).expand(self.T, -1, -1, -1)\n",
        "        if ref.shape[2] != IMG_SIZE or ref.shape[3] != IMG_SIZE:\n",
        "            ref = F.interpolate(ref, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        face_input = torch.cat([ref, masked], dim=1)\n",
        "\n",
        "        mel_chunks = []\n",
        "        for t in range(self.T):\n",
        "            m = mel_window[:, t * MEL_STEP:(t + 1) * MEL_STEP]\n",
        "            mel_chunks.append(torch.from_numpy(m).unsqueeze(0))\n",
        "        mel_tensor = torch.stack(mel_chunks, dim=0)\n",
        "\n",
        "        return {\n",
        "            \"mel\": mel_tensor,\n",
        "            \"face_input\": face_input,\n",
        "            \"gt\": gt,\n",
        "            \"audio\": audio_1d,\n",
        "            \"emotion\": REMAP[s[\"emotion_idx\"]],\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_wav2lip(batch):\n",
        "    return {\n",
        "        \"mel\": torch.stack([b[\"mel\"] for b in batch]),\n",
        "        \"face_input\": torch.stack([b[\"face_input\"] for b in batch]),\n",
        "        \"gt\": torch.stack([b[\"gt\"] for b in batch]),\n",
        "        \"audio\": [b[\"audio\"] for b in batch],\n",
        "        \"emotion\": torch.tensor([b[\"emotion\"] for b in batch]),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SssQyRVdRYE1",
        "outputId": "a7b184ef-2783-4984-fad4-c6209bfb8883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wav2Lip loaded: 36.3M params\n",
            "Frozen encoders loaded. Video: 8 frames. Audio dim=1024, Video dim=768, Proj dim=256\n"
          ]
        }
      ],
      "source": [
        "def load_wav2lip(ckpt_path, device):\n",
        "    model = Wav2LipModel()\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
        "    state = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
        "    state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
        "    model.load_state_dict(state, strict=False)\n",
        "    return model.to(device)\n",
        "\n",
        "wav2lip = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "print(f\"Wav2Lip loaded: {sum(p.numel() for p in wav2lip.parameters()) / 1e6:.1f}M params\")\n",
        "\n",
        "audio_enc, audio_proc = load_frozen_audio_encoder(BEST_AUDIO_PATH, DEVICE)\n",
        "video_enc = load_frozen_video_encoder(BEST_VIDEO_PATH, DEVICE)\n",
        "video_preprocess = DifferentiableVideoPreprocess(224).to(DEVICE)\n",
        "\n",
        "VIDEO_ENC_FRAMES = getattr(video_enc.config, \"num_frames\", 8)\n",
        "AUDIO_DIM = audio_enc.config.hidden_size\n",
        "VIDEO_DIM = video_enc.config.hidden_size\n",
        "PROJ_DIM = 256\n",
        "print(f\"Frozen encoders loaded. Video: {VIDEO_ENC_FRAMES} frames. \"\n",
        "      f\"Audio dim={AUDIO_DIM}, Video dim={VIDEO_DIM}, Proj dim={PROJ_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrL9lg7bRYE1",
        "outputId": "610f37ac-1939-4e3d-a7c1-825e6e6a6101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ],
      "source": [
        "wandb.login()\n",
        "\n",
        "CONFIGS = [\n",
        "    {\"name\": \"wav2lip-baseline\", \"lambda_emo\": 0.0},\n",
        "    {\"name\": \"wav2lip-emo-001\",  \"lambda_emo\": 0.01},\n",
        "    {\"name\": \"wav2lip-emo-005\",  \"lambda_emo\": 0.05},\n",
        "    {\"name\": \"wav2lip-emo-01\",   \"lambda_emo\": 0.1},\n",
        "]\n",
        "\n",
        "LR = 1e-4\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 4\n",
        "PATIENCE = 5\n",
        "T_FRAMES = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "MMsiebgnRYE1"
      },
      "outputs": [],
      "source": [
        "def adapt_frames(frames, target_t):\n",
        "    \"\"\"Resample (B, T, C, H, W) to (B, target_t, C, H, W) via uniform index sampling.\"\"\"\n",
        "    B, T, C, H, W = frames.shape\n",
        "    if T == target_t:\n",
        "        return frames\n",
        "    idx = torch.linspace(0, T - 1, target_t).long()\n",
        "    return frames[:, idx]\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scaler, emotion_loss_fn, lambda_emo):\n",
        "    model.train()\n",
        "    total_recon, total_emo, total_loss = 0.0, 0.0, 0.0\n",
        "\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        B, T = mel.shape[0], mel.shape[1]\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        all_gen = []\n",
        "        recon = 0.0\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            for t in range(T):\n",
        "                gen = model(mel[:, t], face_in[:, t])\n",
        "                recon += F.l1_loss(gen, gt[:, t])\n",
        "                all_gen.append(gen)\n",
        "            recon = recon / T\n",
        "\n",
        "            emo = torch.tensor(0.0, device=DEVICE)\n",
        "            if lambda_emo > 0:\n",
        "                gen_video = torch.stack(all_gen, dim=1)\n",
        "                gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "                audio_emb = extract_audio_embedding(\n",
        "                    audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                video_emb = extract_video_embedding(\n",
        "                    video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                emo = emotion_loss_fn(audio_proj(audio_emb.detach()),\n",
        "                                      video_proj(video_emb))\n",
        "\n",
        "            loss = recon + lambda_emo * emo\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_recon += recon.item()\n",
        "        total_emo += emo.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {\"recon\": total_recon / n, \"emotion\": total_emo / n, \"total\": total_loss / n}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, emotion_loss_fn, lambda_emo):\n",
        "    model.eval()\n",
        "    total_recon, total_emo, total_loss = 0.0, 0.0, 0.0\n",
        "    metric = EmotionAgreementMetric()\n",
        "\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        B, T = mel.shape[0], mel.shape[1]\n",
        "\n",
        "        all_gen = []\n",
        "        recon = 0.0\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            for t in range(T):\n",
        "                gen = model(mel[:, t], face_in[:, t])\n",
        "                recon += F.l1_loss(gen, gt[:, t])\n",
        "                all_gen.append(gen)\n",
        "            recon = recon / T\n",
        "\n",
        "            emo = torch.tensor(0.0, device=DEVICE)\n",
        "            if lambda_emo > 0:\n",
        "                gen_video = torch.stack(all_gen, dim=1)\n",
        "                gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "                audio_emb = extract_audio_embedding(\n",
        "                    audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                video_emb = extract_video_embedding(\n",
        "                    video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                a_p, v_p = audio_proj(audio_emb), video_proj(video_emb)\n",
        "                emo = emotion_loss_fn(a_p, v_p)\n",
        "                metric.update(a_p, v_p)\n",
        "\n",
        "            loss = recon + lambda_emo * emo\n",
        "\n",
        "        total_recon += recon.item()\n",
        "        total_emo += emo.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    result = {\"recon\": total_recon / n, \"emotion\": total_emo / n, \"total\": total_loss / n}\n",
        "    if lambda_emo > 0:\n",
        "        result.update(metric.compute())\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sFEJ98kMRYE1",
        "outputId": "168d5f36-949a-4d35-ec80-58eaadc770e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 408, Val: 72\n",
            "\n",
            "============================================================\n",
            "wav2lip-baseline (lambda_emo=0.0)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wav2lip-emo-001</strong> at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/xyzaq6g2' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/xyzaq6g2</a><br> View project at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260216_030753-xyzaq6g2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260216_032119-r79w8msk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/r79w8msk' target=\"_blank\">wav2lip-baseline</a></strong> to <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/r79w8msk' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/r79w8msk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 1/20] t_loss=0.3554 v_loss=0.2947 v_recon=0.2947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 2/20] t_loss=0.2695 v_loss=0.2390 v_recon=0.2390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 3/20] t_loss=0.2162 v_loss=0.1924 v_recon=0.1924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 4/20] t_loss=0.1854 v_loss=0.1680 v_recon=0.1680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 5/20] t_loss=0.1505 v_loss=0.1374 v_recon=0.1374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 6/20] t_loss=0.1255 v_loss=0.1210 v_recon=0.1210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 7/20] t_loss=0.1119 v_loss=0.1091 v_recon=0.1091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 8/20] t_loss=0.1051 v_loss=0.0974 v_recon=0.0974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 9/20] t_loss=0.0941 v_loss=0.0895 v_recon=0.0895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [10/20] t_loss=0.0873 v_loss=0.0829 v_recon=0.0829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [11/20] t_loss=0.0840 v_loss=0.0788 v_recon=0.0788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [12/20] t_loss=0.0757 v_loss=0.0694 v_recon=0.0694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [13/20] t_loss=0.0687 v_loss=0.0673 v_recon=0.0673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [14/20] t_loss=0.0643 v_loss=0.0669 v_recon=0.0669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [15/20] t_loss=0.0604 v_loss=0.0677 v_recon=0.0677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [16/20] t_loss=0.0571 v_loss=0.0555 v_recon=0.0555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [17/20] t_loss=0.0538 v_loss=0.0581 v_recon=0.0581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [18/20] t_loss=0.0513 v_loss=0.0506 v_recon=0.0506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [19/20] t_loss=0.0493 v_loss=0.0502 v_recon=0.0502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [20/20] t_loss=0.0477 v_loss=0.0533 v_recon=0.0533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/emotion</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/recon</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/emotion</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/recon</td><td>█▆▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/total</td><td>█▆▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train/emotion</td><td>0</td></tr><tr><td>train/recon</td><td>0.04765</td></tr><tr><td>train/total</td><td>0.04765</td></tr><tr><td>val/emotion</td><td>0</td></tr><tr><td>val/recon</td><td>0.05332</td></tr><tr><td>val/total</td><td>0.05332</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wav2lip-baseline</strong> at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/r79w8msk' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/r79w8msk</a><br> View project at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260216_032119-r79w8msk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best val loss: 0.0502 -> /content/wav2lip_finetuned/wav2lip-baseline\n",
            "\n",
            "============================================================\n",
            "wav2lip-emo-001 (lambda_emo=0.01)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260216_033451-kxzrpc8h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/kxzrpc8h' target=\"_blank\">wav2lip-emo-001</a></strong> to <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/kxzrpc8h' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/kxzrpc8h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 1/20] t_loss=0.3581 v_loss=0.3065 v_recon=0.3049 cos_sim=0.837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 2/20] t_loss=0.2695 v_loss=0.2310 v_recon=0.2295 cos_sim=0.845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 3/20] t_loss=0.2197 v_loss=0.2044 v_recon=0.2029 cos_sim=0.848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 4/20] t_loss=0.1870 v_loss=0.1819 v_recon=0.1804 cos_sim=0.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 5/20] t_loss=0.1521 v_loss=0.1485 v_recon=0.1470 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 6/20] t_loss=0.1274 v_loss=0.1217 v_recon=0.1202 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 7/20] t_loss=0.1136 v_loss=0.1108 v_recon=0.1094 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 8/20] t_loss=0.1018 v_loss=0.0988 v_recon=0.0973 cos_sim=0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 9/20] t_loss=0.0947 v_loss=0.1003 v_recon=0.0989 cos_sim=0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [10/20] t_loss=0.0875 v_loss=0.0843 v_recon=0.0828 cos_sim=0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [11/20] t_loss=0.0812 v_loss=0.0845 v_recon=0.0831 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [12/20] t_loss=0.0754 v_loss=0.0762 v_recon=0.0748 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [13/20] t_loss=0.0696 v_loss=0.0728 v_recon=0.0713 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [14/20] t_loss=0.0645 v_loss=0.0653 v_recon=0.0638 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [15/20] t_loss=0.0603 v_loss=0.0602 v_recon=0.0587 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [16/20] t_loss=0.0574 v_loss=0.0647 v_recon=0.0632 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [17/20] t_loss=0.0550 v_loss=0.0557 v_recon=0.0542 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [18/20] t_loss=0.0522 v_loss=0.0519 v_recon=0.0504 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [19/20] t_loss=0.0502 v_loss=0.0546 v_recon=0.0531 cos_sim=0.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [20/20] t_loss=0.0476 v_loss=0.0501 v_recon=0.0486 cos_sim=0.850\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/emotion</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/recon</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/agreement_rate</td><td>▁▅▅█████████████████</td></tr><tr><td>val/avg_cosine_sim</td><td>▁▄▆▆▇███████▇▇▇▇▇▇▇▆</td></tr><tr><td>val/emotion</td><td>█▅▃▃▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃</td></tr><tr><td>val/recon</td><td>█▆▅▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val/total</td><td>█▆▅▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train/emotion</td><td>0.00087</td></tr><tr><td>train/recon</td><td>0.04755</td></tr><tr><td>train/total</td><td>0.04756</td></tr><tr><td>val/agreement_rate</td><td>0.90278</td></tr><tr><td>val/avg_cosine_sim</td><td>0.85016</td></tr><tr><td>val/emotion</td><td>0.14984</td></tr><tr><td>val/recon</td><td>0.04863</td></tr><tr><td>val/total</td><td>0.05013</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wav2lip-emo-001</strong> at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/kxzrpc8h' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/kxzrpc8h</a><br> View project at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260216_033451-kxzrpc8h/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best val loss: 0.0501 -> /content/wav2lip_finetuned/wav2lip-emo-001\n",
            "\n",
            "============================================================\n",
            "wav2lip-emo-005 (lambda_emo=0.05)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260216_035304-1jjdblc7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/1jjdblc7' target=\"_blank\">wav2lip-emo-005</a></strong> to <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/1jjdblc7' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/1jjdblc7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 1/20] t_loss=0.3701 v_loss=0.3289 v_recon=0.3208 cos_sim=0.837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 2/20] t_loss=0.2760 v_loss=0.2468 v_recon=0.2391 cos_sim=0.846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 3/20] t_loss=0.2236 v_loss=0.2092 v_recon=0.2017 cos_sim=0.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 4/20] t_loss=0.1886 v_loss=0.1851 v_recon=0.1777 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 5/20] t_loss=0.1563 v_loss=0.1402 v_recon=0.1328 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 6/20] t_loss=0.1283 v_loss=0.1298 v_recon=0.1224 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 7/20] t_loss=0.1133 v_loss=0.1164 v_recon=0.1090 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 8/20] t_loss=0.1045 v_loss=0.1040 v_recon=0.0967 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 9/20] t_loss=0.0958 v_loss=0.1008 v_recon=0.0934 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [10/20] t_loss=0.0887 v_loss=0.0957 v_recon=0.0884 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [11/20] t_loss=0.0830 v_loss=0.0883 v_recon=0.0809 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [12/20] t_loss=0.0757 v_loss=0.0796 v_recon=0.0722 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [13/20] t_loss=0.0691 v_loss=0.0899 v_recon=0.0825 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [14/20] t_loss=0.0657 v_loss=0.0698 v_recon=0.0624 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [15/20] t_loss=0.0605 v_loss=0.0731 v_recon=0.0656 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [16/20] t_loss=0.0579 v_loss=0.0644 v_recon=0.0570 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [17/20] t_loss=0.0547 v_loss=0.0625 v_recon=0.0550 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [18/20] t_loss=0.0531 v_loss=0.0609 v_recon=0.0534 cos_sim=0.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [19/20] t_loss=0.0505 v_loss=0.0565 v_recon=0.0490 cos_sim=0.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [20/20] t_loss=0.0480 v_loss=0.0589 v_recon=0.0513 cos_sim=0.849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/emotion</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/recon</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/agreement_rate</td><td>▁▅██████████████████</td></tr><tr><td>val/avg_cosine_sim</td><td>▁▅▇▇████████▇▇▇▇▇▇▆▆</td></tr><tr><td>val/emotion</td><td>█▄▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃</td></tr><tr><td>val/recon</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val/total</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train/emotion</td><td>0.00084</td></tr><tr><td>train/recon</td><td>0.04801</td></tr><tr><td>train/total</td><td>0.04805</td></tr><tr><td>val/agreement_rate</td><td>0.90278</td></tr><tr><td>val/avg_cosine_sim</td><td>0.84948</td></tr><tr><td>val/emotion</td><td>0.15052</td></tr><tr><td>val/recon</td><td>0.05135</td></tr><tr><td>val/total</td><td>0.05887</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wav2lip-emo-005</strong> at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/1jjdblc7' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/1jjdblc7</a><br> View project at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260216_035304-1jjdblc7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best val loss: 0.0565 -> /content/wav2lip_finetuned/wav2lip-emo-005\n",
            "\n",
            "============================================================\n",
            "wav2lip-emo-01 (lambda_emo=0.1)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260216_041117-eolwjfq5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/eolwjfq5' target=\"_blank\">wav2lip-emo-01</a></strong> to <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/eolwjfq5' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/eolwjfq5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 1/20] t_loss=0.3776 v_loss=0.3404 v_recon=0.3242 cos_sim=0.838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 2/20] t_loss=0.2831 v_loss=0.2645 v_recon=0.2491 cos_sim=0.847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 3/20] t_loss=0.2290 v_loss=0.2171 v_recon=0.2023 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 4/20] t_loss=0.1927 v_loss=0.1901 v_recon=0.1756 cos_sim=0.855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 5/20] t_loss=0.1601 v_loss=0.1637 v_recon=0.1492 cos_sim=0.855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 6/20] t_loss=0.1307 v_loss=0.1341 v_recon=0.1196 cos_sim=0.855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 7/20] t_loss=0.1153 v_loss=0.1234 v_recon=0.1090 cos_sim=0.856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 8/20] t_loss=0.1054 v_loss=0.1184 v_recon=0.1040 cos_sim=0.856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 9/20] t_loss=0.0972 v_loss=0.1074 v_recon=0.0929 cos_sim=0.855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [10/20] t_loss=0.0900 v_loss=0.1000 v_recon=0.0856 cos_sim=0.855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [11/20] t_loss=0.0833 v_loss=0.1021 v_recon=0.0875 cos_sim=0.855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [12/20] t_loss=0.0772 v_loss=0.0897 v_recon=0.0751 cos_sim=0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [13/20] t_loss=0.0691 v_loss=0.0832 v_recon=0.0685 cos_sim=0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [14/20] t_loss=0.0666 v_loss=0.0810 v_recon=0.0664 cos_sim=0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [15/20] t_loss=0.0612 v_loss=0.0744 v_recon=0.0597 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [16/20] t_loss=0.0600 v_loss=0.0733 v_recon=0.0586 cos_sim=0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [17/20] t_loss=0.0565 v_loss=0.0746 v_recon=0.0598 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [18/20] t_loss=0.0527 v_loss=0.0654 v_recon=0.0505 cos_sim=0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [19/20] t_loss=0.0505 v_loss=0.0673 v_recon=0.0524 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [20/20] t_loss=0.0507 v_loss=0.0648 v_recon=0.0499 cos_sim=0.851\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/emotion</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/recon</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/agreement_rate</td><td>▁▆▆█████████████████</td></tr><tr><td>val/avg_cosine_sim</td><td>▁▄▆▇██████▇▇▇▇▇▇▆▆▆▆</td></tr><tr><td>val/emotion</td><td>█▅▃▂▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃</td></tr><tr><td>val/recon</td><td>█▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/total</td><td>█▆▅▄▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train/emotion</td><td>0.0009</td></tr><tr><td>train/recon</td><td>0.0506</td></tr><tr><td>train/total</td><td>0.05069</td></tr><tr><td>val/agreement_rate</td><td>0.90278</td></tr><tr><td>val/avg_cosine_sim</td><td>0.85096</td></tr><tr><td>val/emotion</td><td>0.14904</td></tr><tr><td>val/recon</td><td>0.04987</td></tr><tr><td>val/total</td><td>0.06478</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wav2lip-emo-01</strong> at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/eolwjfq5' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/eolwjfq5</a><br> View project at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260216_041117-eolwjfq5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best val loss: 0.0648 -> /content/wav2lip_finetuned/wav2lip-emo-01\n"
          ]
        }
      ],
      "source": [
        "train_ds = Wav2LipDataset(METADATA, \"train\", T=T_FRAMES)\n",
        "val_ds = Wav2LipDataset(METADATA, \"val\", T=T_FRAMES)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=0, collate_fn=collate_wav2lip)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=0, collate_fn=collate_wav2lip)\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for cfg in CONFIGS:\n",
        "    name = cfg[\"name\"]\n",
        "    lambda_emo = cfg[\"lambda_emo\"]\n",
        "    print(f\"\\n{'='*60}\\n{name} (lambda_emo={lambda_emo})\\n{'='*60}\")\n",
        "\n",
        "    wandb.init(project=\"uncanny-valley-wav2lip\", name=name,\n",
        "               config={**cfg, \"lr\": LR, \"epochs\": EPOCHS}, reinit=True)\n",
        "\n",
        "    model = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "    audio_proj = nn.Linear(AUDIO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "    video_proj = nn.Linear(VIDEO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "    params = list(model.parameters())\n",
        "    if lambda_emo > 0:\n",
        "        params += list(audio_proj.parameters()) + list(video_proj.parameters())\n",
        "    optimizer = torch.optim.AdamW(params, lr=LR)\n",
        "    scaler = GradScaler(enabled=DEVICE == \"cuda\")\n",
        "    emotion_loss_fn = CrossModalEmotionLoss(weight=1.0)\n",
        "\n",
        "    best_val, patience_cnt = float(\"inf\"), 0\n",
        "    save_path = OUT_DIR / name\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t = train_one_epoch(model, train_loader, optimizer, scaler,\n",
        "                            emotion_loss_fn, lambda_emo)\n",
        "        v = evaluate(model, val_loader, emotion_loss_fn, lambda_emo)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/recon\": t[\"recon\"], \"train/emotion\": t[\"emotion\"], \"train/total\": t[\"total\"],\n",
        "            \"val/recon\": v[\"recon\"], \"val/emotion\": v[\"emotion\"], \"val/total\": v[\"total\"],\n",
        "            **{f\"val/{k}\": v[k] for k in [\"avg_cosine_sim\", \"agreement_rate\"] if k in v},\n",
        "        })\n",
        "\n",
        "        print(f\"  [{epoch+1:2d}/{EPOCHS}] \"\n",
        "              f\"t_loss={t['total']:.4f} v_loss={v['total']:.4f} v_recon={v['recon']:.4f}\"\n",
        "              + (f\" cos_sim={v.get('avg_cosine_sim', 0):.3f}\" if lambda_emo > 0 else \"\"))\n",
        "\n",
        "        if v[\"total\"] < best_val:\n",
        "            best_val = v[\"total\"]\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(model.state_dict(), save_path / \"wav2lip.pth\")\n",
        "            torch.save({\"audio_proj\": audio_proj.state_dict(),\n",
        "                         \"video_proj\": video_proj.state_dict()},\n",
        "                        save_path / \"projections.pth\")\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= PATIENCE:\n",
        "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, scaler, audio_proj, video_proj\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    all_results.append({\"name\": name, \"lambda_emo\": lambda_emo, \"best_val\": best_val})\n",
        "    print(f\"  Best val loss: {best_val:.4f} -> {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DE7_BkkGRYE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "outputId": "784af9b2-0869-41f3-c400-97da5a506dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            name  lambda_emo  best_val\n",
            " wav2lip-emo-001        0.01  0.050129\n",
            "wav2lip-baseline        0.00  0.050228\n",
            " wav2lip-emo-005        0.05  0.056458\n",
            "  wav2lip-emo-01        0.10  0.064778\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(all_results).sort_values(\"best_val\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.bar(df[\"name\"], df[\"best_val\"], color=\"steelblue\")\n",
        "ax.set_ylabel(\"Best Val Loss\")\n",
        "ax.set_title(\"Wav2Lip Fine-tuning: λ_emo Ablation\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7KdaP1RtRYE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f5adf3-4b1b-464c-88ef-99a883ec28a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best model: wav2lip-emo-001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating best: 100%|██████████| 18/18 [00:08<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best model evaluation:\n",
            "  Avg L1 recon:     0.0480\n",
            "  Avg cosine sim:   0.8504\n",
            "  Agreement rate:   0.9028\n",
            "  Std cosine sim:   0.4582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "best_name = df.iloc[0][\"name\"]\n",
        "best_model = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "best_model.load_state_dict(torch.load(OUT_DIR / best_name / \"wav2lip.pth\", map_location=DEVICE, weights_only=True))\n",
        "best_model.eval()\n",
        "\n",
        "audio_proj = nn.Linear(AUDIO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "video_proj = nn.Linear(VIDEO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "proj_ckpt = torch.load(OUT_DIR / best_name / \"projections.pth\", map_location=DEVICE, weights_only=True)\n",
        "audio_proj.load_state_dict(proj_ckpt[\"audio_proj\"])\n",
        "video_proj.load_state_dict(proj_ckpt[\"video_proj\"])\n",
        "audio_proj.eval()\n",
        "video_proj.eval()\n",
        "print(f\"Loaded best model: {best_name}\")\n",
        "\n",
        "metric = EmotionAgreementMetric()\n",
        "all_recon = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Evaluating best\"):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        T = mel.shape[1]\n",
        "\n",
        "        all_gen = []\n",
        "        for t in range(T):\n",
        "            gen = best_model(mel[:, t], face_in[:, t])\n",
        "            all_gen.append(gen)\n",
        "            all_recon.append(F.l1_loss(gen, gt[:, t]).item())\n",
        "\n",
        "        gen_video = torch.stack(all_gen, dim=1)\n",
        "        gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "        audio_emb = extract_audio_embedding(audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "        video_emb = extract_video_embedding(video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "        metric.update(audio_proj(audio_emb), video_proj(video_emb))\n",
        "\n",
        "agreement = metric.compute()\n",
        "print(f\"\\nBest model evaluation:\")\n",
        "print(f\"  Avg L1 recon:     {np.mean(all_recon):.4f}\")\n",
        "print(f\"  Avg cosine sim:   {agreement['avg_cosine_sim']:.4f}\")\n",
        "print(f\"  Agreement rate:   {agreement['agreement_rate']:.4f}\")\n",
        "print(f\"  Std cosine sim:   {agreement['std_cosine_sim']:.4f}\")\n",
        "\n",
        "del best_model\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}