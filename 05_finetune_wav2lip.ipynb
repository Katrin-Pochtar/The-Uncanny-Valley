{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AcIdlpbRYE0",
        "outputId": "0a488dbc-3ef1-4a88-ea1e-f93dcbc96f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 189M Feb 16 02:27 Wav2Lip/checkpoints/lipsync_expert.pth\n",
            "-rw-r--r-- 1 root root 416M Feb 16 02:27 Wav2Lip/checkpoints/wav2lip.pth\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers librosa wandb\n",
        "!git clone https://github.com/Rudrabha/Wav2Lip.git 2>/dev/null || true\n",
        "!mkdir -p Wav2Lip/checkpoints\n",
        "\n",
        "!wget -q -nc \"https://huggingface.co/Nekochu/Wav2Lip/resolve/main/wav2lip.pth\" -O Wav2Lip/checkpoints/wav2lip.pth\n",
        "!wget -q -nc \"https://huggingface.co/Nekochu/Wav2Lip/resolve/main/lipsync_expert.pth\" -O Wav2Lip/checkpoints/lipsync_expert.pth\n",
        "!ls -lh Wav2Lip/checkpoints/*.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhwx7fo5RYE0",
        "outputId": "4f6d8348-6564-48cb-9002-afe17ae5d20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content\")\n",
        "sys.path.insert(0, \"/content/Wav2Lip\")\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import wandb\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from emotion_utils import (\n",
        "    CrossModalEmotionLoss,\n",
        "    DifferentiableVideoPreprocess,\n",
        "    EmotionAgreementMetric,\n",
        "    load_frozen_audio_encoder,\n",
        "    load_frozen_video_encoder,\n",
        "    extract_audio_embedding,\n",
        "    extract_video_embedding,\n",
        ")\n",
        "from models.wav2lip import Wav2Lip as Wav2LipModel\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "METADATA = \"/content/processed_data/metadata.json\"\n",
        "WAV2LIP_CKPT = \"/content/Wav2Lip/checkpoints/wav2lip.pth\"\n",
        "BEST_AUDIO_PATH = \"/content/trained_encoders_v2/w2v2-lg-lr2e5\"\n",
        "BEST_VIDEO_PATH = \"/content/trained_encoders_v2/tsf-lr3e5-16f-nf\"\n",
        "OUT_DIR = Path(\"/content/wav2lip_finetuned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EXCLUDE = {0, 1, 3, 5, 7}\n",
        "REMAP = {2: 0, 4: 1, 6: 2}\n",
        "EMOTIONS = [\"happy\", \"angry\", \"disgust\"]\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "U1odWXrQRYE1"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 96\n",
        "MEL_STEP = 16\n",
        "SR = 16000\n",
        "FPS = 25\n",
        "\n",
        "def wav_to_mel(wav_path, sr=SR):\n",
        "    y, _ = librosa.load(wav_path, sr=sr)\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=y, sr=sr, n_mels=80, hop_length=200, win_length=800,\n",
        "        fmin=55, fmax=7600)\n",
        "    return librosa.power_to_db(mel, ref=np.max).astype(np.float32)\n",
        "\n",
        "\n",
        "class Wav2LipDataset(Dataset):\n",
        "    def __init__(self, metadata_path, split, T=5):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data\n",
        "                        if s[\"split\"] == split and s[\"emotion_idx\"] not in EXCLUDE]\n",
        "        self.T = T\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "\n",
        "        wav, sr = torchaudio.load(s[\"audio_path\"])\n",
        "        audio_1d = wav.squeeze(0)\n",
        "\n",
        "        mel = wav_to_mel(s[\"audio_path\"])\n",
        "\n",
        "        frames = np.load(s[\"frames_path\"]).astype(np.float32) / 255.0\n",
        "        n_frames = frames.shape[0]\n",
        "\n",
        "        start = np.random.randint(0, max(1, n_frames - self.T))\n",
        "        face_window = frames[start:start + self.T]\n",
        "        if face_window.shape[0] < self.T:\n",
        "            pad = np.repeat(face_window[-1:], self.T - face_window.shape[0], axis=0)\n",
        "            face_window = np.concatenate([face_window, pad], axis=0)\n",
        "\n",
        "        mel_start = int(start / FPS * SR / 200)\n",
        "        mel_end = mel_start + MEL_STEP * self.T\n",
        "        mel_window = mel[:, mel_start:mel_end]\n",
        "        if mel_window.shape[1] < MEL_STEP * self.T:\n",
        "            mel_window = np.pad(mel_window, ((0, 0), (0, MEL_STEP * self.T - mel_window.shape[1])))\n",
        "\n",
        "        gt = torch.from_numpy(face_window).permute(0, 3, 1, 2)\n",
        "        H, W = gt.shape[2], gt.shape[3]\n",
        "        if H != IMG_SIZE or W != IMG_SIZE:\n",
        "            gt = F.interpolate(gt, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        masked = gt.clone()\n",
        "        masked[:, :, IMG_SIZE // 2:, :] = 0.0\n",
        "\n",
        "        ref_idx = np.random.randint(0, n_frames)\n",
        "        ref = torch.from_numpy(frames[ref_idx]).permute(2, 0, 1).unsqueeze(0).expand(self.T, -1, -1, -1)\n",
        "        if ref.shape[2] != IMG_SIZE or ref.shape[3] != IMG_SIZE:\n",
        "            ref = F.interpolate(ref, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        face_input = torch.cat([ref, masked], dim=1)\n",
        "\n",
        "        mel_chunks = []\n",
        "        for t in range(self.T):\n",
        "            m = mel_window[:, t * MEL_STEP:(t + 1) * MEL_STEP]\n",
        "            mel_chunks.append(torch.from_numpy(m).unsqueeze(0))\n",
        "        mel_tensor = torch.stack(mel_chunks, dim=0)\n",
        "\n",
        "        return {\n",
        "            \"mel\": mel_tensor,\n",
        "            \"face_input\": face_input,\n",
        "            \"gt\": gt,\n",
        "            \"audio\": audio_1d,\n",
        "            \"emotion\": REMAP[s[\"emotion_idx\"]],\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_wav2lip(batch):\n",
        "    return {\n",
        "        \"mel\": torch.stack([b[\"mel\"] for b in batch]),\n",
        "        \"face_input\": torch.stack([b[\"face_input\"] for b in batch]),\n",
        "        \"gt\": torch.stack([b[\"gt\"] for b in batch]),\n",
        "        \"audio\": [b[\"audio\"] for b in batch],\n",
        "        \"emotion\": torch.tensor([b[\"emotion\"] for b in batch]),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SssQyRVdRYE1",
        "outputId": "1520364f-ceb1-4842-aba5-367144a385a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wav2Lip loaded: 36.3M params\n",
            "Frozen emotion encoders loaded.\n"
          ]
        }
      ],
      "source": [
        "def load_wav2lip(ckpt_path, device):\n",
        "    model = Wav2LipModel()\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
        "    state = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
        "    state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
        "    model.load_state_dict(state, strict=False)\n",
        "    return model.to(device)\n",
        "\n",
        "wav2lip = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "print(f\"Wav2Lip loaded: {sum(p.numel() for p in wav2lip.parameters()) / 1e6:.1f}M params\")\n",
        "\n",
        "audio_enc, audio_proc = load_frozen_audio_encoder(BEST_AUDIO_PATH, DEVICE)\n",
        "video_enc = load_frozen_video_encoder(BEST_VIDEO_PATH, DEVICE)\n",
        "video_preprocess = DifferentiableVideoPreprocess(224).to(DEVICE)\n",
        "print(\"Frozen emotion encoders loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qrL9lg7bRYE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cda88e9-b964-4df8-ed7c-c5cd317827cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkatrinpochtar\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "wandb.login()\n",
        "\n",
        "CONFIGS = [\n",
        "    {\"name\": \"wav2lip-baseline\", \"lambda_emo\": 0.0},\n",
        "    {\"name\": \"wav2lip-emo-001\",  \"lambda_emo\": 0.01},\n",
        "    {\"name\": \"wav2lip-emo-005\",  \"lambda_emo\": 0.05},\n",
        "    {\"name\": \"wav2lip-emo-01\",   \"lambda_emo\": 0.1},\n",
        "]\n",
        "\n",
        "LR = 1e-4\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 4\n",
        "PATIENCE = 5\n",
        "T_FRAMES = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MMsiebgnRYE1"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, scaler, emotion_loss_fn, lambda_emo):\n",
        "    model.train()\n",
        "    total_recon, total_emo, total_loss = 0.0, 0.0, 0.0\n",
        "\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        B, T = mel.shape[0], mel.shape[1]\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        all_gen = []\n",
        "        recon = 0.0\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            for t in range(T):\n",
        "                gen = model(mel[:, t], face_in[:, t])\n",
        "                recon += F.l1_loss(gen, gt[:, t])\n",
        "                all_gen.append(gen)\n",
        "            recon = recon / T\n",
        "\n",
        "            emo = torch.tensor(0.0, device=DEVICE)\n",
        "            if lambda_emo > 0:\n",
        "                gen_video = torch.stack(all_gen, dim=1)\n",
        "                audio_emb = extract_audio_embedding(\n",
        "                    audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                video_emb = extract_video_embedding(\n",
        "                    video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                emo = emotion_loss_fn(audio_emb.detach(), video_emb)\n",
        "\n",
        "            loss = recon + lambda_emo * emo\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_recon += recon.item()\n",
        "        total_emo += emo.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {\"recon\": total_recon / n, \"emotion\": total_emo / n, \"total\": total_loss / n}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, emotion_loss_fn, lambda_emo):\n",
        "    model.eval()\n",
        "    total_recon, total_emo, total_loss = 0.0, 0.0, 0.0\n",
        "    metric = EmotionAgreementMetric()\n",
        "\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        B, T = mel.shape[0], mel.shape[1]\n",
        "\n",
        "        all_gen = []\n",
        "        recon = 0.0\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            for t in range(T):\n",
        "                gen = model(mel[:, t], face_in[:, t])\n",
        "                recon += F.l1_loss(gen, gt[:, t])\n",
        "                all_gen.append(gen)\n",
        "            recon = recon / T\n",
        "\n",
        "            emo = torch.tensor(0.0, device=DEVICE)\n",
        "            if lambda_emo > 0:\n",
        "                gen_video = torch.stack(all_gen, dim=1)\n",
        "                audio_emb = extract_audio_embedding(\n",
        "                    audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                video_emb = extract_video_embedding(\n",
        "                    video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                emo = emotion_loss_fn(audio_emb, video_emb)\n",
        "                metric.update(audio_emb, video_emb)\n",
        "\n",
        "            loss = recon + lambda_emo * emo\n",
        "\n",
        "        total_recon += recon.item()\n",
        "        total_emo += emo.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    result = {\"recon\": total_recon / n, \"emotion\": total_emo / n, \"total\": total_loss / n}\n",
        "    if lambda_emo > 0:\n",
        "        result.update(metric.compute())\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sFEJ98kMRYE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58263ac7-d597-45b9-bfe8-88b364673962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 408, Val: 72\n",
            "\n",
            "============================================================\n",
            "wav2lip-baseline (lambda_emo=0.0)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260216_023248-b9vpt2al</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/b9vpt2al' target=\"_blank\">wav2lip-baseline</a></strong> to <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/b9vpt2al' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/b9vpt2al</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 1/20] t_loss=0.3573 v_loss=0.3032 v_recon=0.3032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 2/20] t_loss=0.2709 v_loss=0.2366 v_recon=0.2366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 3/20] t_loss=0.2199 v_loss=0.2045 v_recon=0.2045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 4/20] t_loss=0.1855 v_loss=0.1748 v_recon=0.1748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 5/20] t_loss=0.1559 v_loss=0.1299 v_recon=0.1299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 6/20] t_loss=0.1284 v_loss=0.1225 v_recon=0.1225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 7/20] t_loss=0.1132 v_loss=0.1077 v_recon=0.1077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 8/20] t_loss=0.1022 v_loss=0.0997 v_recon=0.0997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [ 9/20] t_loss=0.0961 v_loss=0.0921 v_recon=0.0921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [10/20] t_loss=0.0886 v_loss=0.0879 v_recon=0.0879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [11/20] t_loss=0.0825 v_loss=0.0807 v_recon=0.0807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [12/20] t_loss=0.0753 v_loss=0.0730 v_recon=0.0730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [13/20] t_loss=0.0697 v_loss=0.0692 v_recon=0.0692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [14/20] t_loss=0.0642 v_loss=0.0645 v_recon=0.0645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [15/20] t_loss=0.0595 v_loss=0.0609 v_recon=0.0609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [16/20] t_loss=0.0564 v_loss=0.0571 v_recon=0.0571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [17/20] t_loss=0.0545 v_loss=0.0508 v_recon=0.0508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [18/20] t_loss=0.0507 v_loss=0.0508 v_recon=0.0508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [19/20] t_loss=0.0503 v_loss=0.0507 v_recon=0.0507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [20/20] t_loss=0.0487 v_loss=0.0448 v_recon=0.0448\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/emotion</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/recon</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/emotion</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/recon</td><td>█▆▅▅▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val/total</td><td>█▆▅▅▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train/emotion</td><td>0</td></tr><tr><td>train/recon</td><td>0.04868</td></tr><tr><td>train/total</td><td>0.04868</td></tr><tr><td>val/emotion</td><td>0</td></tr><tr><td>val/recon</td><td>0.04485</td></tr><tr><td>val/total</td><td>0.04485</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wav2lip-baseline</strong> at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/b9vpt2al' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/b9vpt2al</a><br> View project at: <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260216_023248-b9vpt2al/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best val loss: 0.0448 -> /content/wav2lip_finetuned/wav2lip-baseline\n",
            "\n",
            "============================================================\n",
            "wav2lip-emo-001 (lambda_emo=0.01)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260216_024651-wdsf1xuk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/wdsf1xuk' target=\"_blank\">wav2lip-emo-001</a></strong> to <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/wdsf1xuk' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-wav2lip/runs/wdsf1xuk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[4, 8, 14, 8, 768]' is invalid for input of size 3010560",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1666332236.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         t = train_one_epoch(model, train_loader, optimizer, scaler,\n\u001b[0m\u001b[1;32m     29\u001b[0m                             emotion_loss_fn, lambda_emo)\n\u001b[1;32m     30\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_emo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-872860575.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, scaler, emotion_loss_fn, lambda_emo)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 audio_emb = extract_audio_embedding(\n\u001b[1;32m     26\u001b[0m                     audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n\u001b[0;32m---> 27\u001b[0;31m                 video_emb = extract_video_embedding(\n\u001b[0m\u001b[1;32m     28\u001b[0m                     video_enc, video_preprocess, gen_video, device=DEVICE)\n\u001b[1;32m     29\u001b[0m                 \u001b[0memo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memotion_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/emotion_utils.py\u001b[0m in \u001b[0;36mextract_video_embedding\u001b[0;34m(model, preprocess, frames, device)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_video_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mpv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/timesformer/modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         outputs = self.timesformer(\n\u001b[0m\u001b[1;32m    724\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/timesformer/modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    592\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/timesformer/modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/timesformer/modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;31m# Temporal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mtemporal_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             temporal_embedding = temporal_embedding.reshape(\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_patch_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_patch_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemporal_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             ).reshape(batch_size * num_patch_height * num_patch_width, num_frames, temporal_embedding.shape[2])\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 8, 14, 8, 768]' is invalid for input of size 3010560"
          ]
        }
      ],
      "source": [
        "train_ds = Wav2LipDataset(METADATA, \"train\", T=T_FRAMES)\n",
        "val_ds = Wav2LipDataset(METADATA, \"val\", T=T_FRAMES)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=0, collate_fn=collate_wav2lip)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=0, collate_fn=collate_wav2lip)\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for cfg in CONFIGS:\n",
        "    name = cfg[\"name\"]\n",
        "    lambda_emo = cfg[\"lambda_emo\"]\n",
        "    print(f\"\\n{'='*60}\\n{name} (lambda_emo={lambda_emo})\\n{'='*60}\")\n",
        "\n",
        "    wandb.init(project=\"uncanny-valley-wav2lip\", name=name,\n",
        "               config={**cfg, \"lr\": LR, \"epochs\": EPOCHS}, reinit=True)\n",
        "\n",
        "    model = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    scaler = GradScaler(enabled=DEVICE == \"cuda\")\n",
        "    emotion_loss_fn = CrossModalEmotionLoss(weight=1.0)\n",
        "\n",
        "    best_val, patience_cnt = float(\"inf\"), 0\n",
        "    save_path = OUT_DIR / name\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t = train_one_epoch(model, train_loader, optimizer, scaler,\n",
        "                            emotion_loss_fn, lambda_emo)\n",
        "        v = evaluate(model, val_loader, emotion_loss_fn, lambda_emo)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/recon\": t[\"recon\"], \"train/emotion\": t[\"emotion\"], \"train/total\": t[\"total\"],\n",
        "            \"val/recon\": v[\"recon\"], \"val/emotion\": v[\"emotion\"], \"val/total\": v[\"total\"],\n",
        "            **{f\"val/{k}\": v[k] for k in [\"avg_cosine_sim\", \"agreement_rate\"] if k in v},\n",
        "        })\n",
        "\n",
        "        print(f\"  [{epoch+1:2d}/{EPOCHS}] \"\n",
        "              f\"t_loss={t['total']:.4f} v_loss={v['total']:.4f} v_recon={v['recon']:.4f}\"\n",
        "              + (f\" cos_sim={v.get('avg_cosine_sim', 0):.3f}\" if lambda_emo > 0 else \"\"))\n",
        "\n",
        "        if v[\"total\"] < best_val:\n",
        "            best_val = v[\"total\"]\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(model.state_dict(), save_path / \"wav2lip.pth\")\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= PATIENCE:\n",
        "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, scaler\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    all_results.append({\"name\": name, \"lambda_emo\": lambda_emo, \"best_val\": best_val})\n",
        "    print(f\"  Best val loss: {best_val:.4f} -> {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE7_BkkGRYE1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(all_results).sort_values(\"best_val\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.bar(df[\"name\"], df[\"best_val\"], color=\"steelblue\")\n",
        "ax.set_ylabel(\"Best Val Loss\")\n",
        "ax.set_title(\"Wav2Lip Fine-tuning: λ_emo Ablation\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KdaP1RtRYE2"
      },
      "outputs": [],
      "source": [
        "best_name = df.iloc[0][\"name\"]\n",
        "best_model = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "best_model.load_state_dict(torch.load(OUT_DIR / best_name / \"wav2lip.pth\", map_location=DEVICE, weights_only=True))\n",
        "best_model.eval()\n",
        "print(f\"Loaded best model: {best_name}\")\n",
        "\n",
        "metric = EmotionAgreementMetric()\n",
        "all_recon = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Evaluating best\"):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        T = mel.shape[1]\n",
        "\n",
        "        all_gen = []\n",
        "        for t in range(T):\n",
        "            gen = best_model(mel[:, t], face_in[:, t])\n",
        "            all_gen.append(gen)\n",
        "            all_recon.append(F.l1_loss(gen, gt[:, t]).item())\n",
        "\n",
        "        gen_video = torch.stack(all_gen, dim=1)\n",
        "        audio_emb = extract_audio_embedding(audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "        video_emb = extract_video_embedding(video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "        metric.update(audio_emb, video_emb)\n",
        "\n",
        "agreement = metric.compute()\n",
        "print(f\"\\nBest model evaluation:\")\n",
        "print(f\"  Avg L1 recon:     {np.mean(all_recon):.4f}\")\n",
        "print(f\"  Avg cosine sim:   {agreement['avg_cosine_sim']:.4f}\")\n",
        "print(f\"  Agreement rate:   {agreement['agreement_rate']:.4f}\")\n",
        "print(f\"  Std cosine sim:   {agreement['std_cosine_sim']:.4f}\")\n",
        "\n",
        "del best_model\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}