{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AcIdlpbRYE0",
        "outputId": "8215d92e-d87b-4e86-8256-82763f7250a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 189M Feb 16 02:27 Wav2Lip/checkpoints/lipsync_expert.pth\n",
            "-rw-r--r-- 1 root root 416M Feb 16 02:27 Wav2Lip/checkpoints/wav2lip.pth\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers librosa wandb\n",
        "!git clone https://github.com/Rudrabha/Wav2Lip.git 2>/dev/null || true\n",
        "!mkdir -p Wav2Lip/checkpoints\n",
        "\n",
        "!wget -q -nc \"https://huggingface.co/Nekochu/Wav2Lip/resolve/main/wav2lip.pth\" -O Wav2Lip/checkpoints/wav2lip.pth\n",
        "!wget -q -nc \"https://huggingface.co/Nekochu/Wav2Lip/resolve/main/lipsync_expert.pth\" -O Wav2Lip/checkpoints/lipsync_expert.pth\n",
        "!ls -lh Wav2Lip/checkpoints/*.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhwx7fo5RYE0",
        "outputId": "2245af43-0563-47d2-c048-bc51e7306d76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content\")\n",
        "sys.path.insert(0, \"/content/Wav2Lip\")\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import wandb\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from emotion_utils import (\n",
        "    CrossModalEmotionLoss,\n",
        "    DifferentiableVideoPreprocess,\n",
        "    EmotionAgreementMetric,\n",
        "    load_frozen_audio_encoder,\n",
        "    load_frozen_video_encoder,\n",
        "    extract_audio_embedding,\n",
        "    extract_video_embedding,\n",
        ")\n",
        "from models.wav2lip import Wav2Lip as Wav2LipModel\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "METADATA = \"/content/processed_data/metadata.json\"\n",
        "WAV2LIP_CKPT = \"/content/Wav2Lip/checkpoints/wav2lip.pth\"\n",
        "BEST_AUDIO_PATH = \"/content/trained_encoders_v2/w2v2-lg-lr2e5\"\n",
        "BEST_VIDEO_PATH = \"/content/trained_encoders_v2/tsf-lr3e5-16f-nf\"\n",
        "OUT_DIR = Path(\"/content/wav2lip_finetuned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EXCLUDE = {0, 1, 3, 5, 7}\n",
        "REMAP = {2: 0, 4: 1, 6: 2}\n",
        "EMOTIONS = [\"happy\", \"angry\", \"disgust\"]\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "U1odWXrQRYE1"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 96\n",
        "MEL_STEP = 16\n",
        "SR = 16000\n",
        "FPS = 25\n",
        "\n",
        "def wav_to_mel(wav_path, sr=SR):\n",
        "    y, _ = librosa.load(wav_path, sr=sr)\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=y, sr=sr, n_mels=80, hop_length=200, win_length=800,\n",
        "        fmin=55, fmax=7600)\n",
        "    return librosa.power_to_db(mel, ref=np.max).astype(np.float32)\n",
        "\n",
        "\n",
        "class Wav2LipDataset(Dataset):\n",
        "    def __init__(self, metadata_path, split, T=5):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data\n",
        "                        if s[\"split\"] == split and s[\"emotion_idx\"] not in EXCLUDE]\n",
        "        self.T = T\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "\n",
        "        wav, sr = torchaudio.load(s[\"audio_path\"])\n",
        "        audio_1d = wav.squeeze(0)\n",
        "\n",
        "        mel = wav_to_mel(s[\"audio_path\"])\n",
        "\n",
        "        frames = np.load(s[\"frames_path\"]).astype(np.float32) / 255.0\n",
        "        n_frames = frames.shape[0]\n",
        "\n",
        "        start = np.random.randint(0, max(1, n_frames - self.T))\n",
        "        face_window = frames[start:start + self.T]\n",
        "        if face_window.shape[0] < self.T:\n",
        "            pad = np.repeat(face_window[-1:], self.T - face_window.shape[0], axis=0)\n",
        "            face_window = np.concatenate([face_window, pad], axis=0)\n",
        "\n",
        "        mel_start = int(start / FPS * SR / 200)\n",
        "        mel_end = mel_start + MEL_STEP * self.T\n",
        "        mel_window = mel[:, mel_start:mel_end]\n",
        "        if mel_window.shape[1] < MEL_STEP * self.T:\n",
        "            mel_window = np.pad(mel_window, ((0, 0), (0, MEL_STEP * self.T - mel_window.shape[1])))\n",
        "\n",
        "        gt = torch.from_numpy(face_window).permute(0, 3, 1, 2)\n",
        "        H, W = gt.shape[2], gt.shape[3]\n",
        "        if H != IMG_SIZE or W != IMG_SIZE:\n",
        "            gt = F.interpolate(gt, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        masked = gt.clone()\n",
        "        masked[:, :, IMG_SIZE // 2:, :] = 0.0\n",
        "\n",
        "        ref_idx = np.random.randint(0, n_frames)\n",
        "        ref = torch.from_numpy(frames[ref_idx]).permute(2, 0, 1).unsqueeze(0).expand(self.T, -1, -1, -1)\n",
        "        if ref.shape[2] != IMG_SIZE or ref.shape[3] != IMG_SIZE:\n",
        "            ref = F.interpolate(ref, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        face_input = torch.cat([ref, masked], dim=1)\n",
        "\n",
        "        mel_chunks = []\n",
        "        for t in range(self.T):\n",
        "            m = mel_window[:, t * MEL_STEP:(t + 1) * MEL_STEP]\n",
        "            mel_chunks.append(torch.from_numpy(m).unsqueeze(0))\n",
        "        mel_tensor = torch.stack(mel_chunks, dim=0)\n",
        "\n",
        "        return {\n",
        "            \"mel\": mel_tensor,\n",
        "            \"face_input\": face_input,\n",
        "            \"gt\": gt,\n",
        "            \"audio\": audio_1d,\n",
        "            \"emotion\": REMAP[s[\"emotion_idx\"]],\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_wav2lip(batch):\n",
        "    return {\n",
        "        \"mel\": torch.stack([b[\"mel\"] for b in batch]),\n",
        "        \"face_input\": torch.stack([b[\"face_input\"] for b in batch]),\n",
        "        \"gt\": torch.stack([b[\"gt\"] for b in batch]),\n",
        "        \"audio\": [b[\"audio\"] for b in batch],\n",
        "        \"emotion\": torch.tensor([b[\"emotion\"] for b in batch]),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "SssQyRVdRYE1",
        "outputId": "80daae93-b308-42e6-fe5e-dc143814af21"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Wav2Lip:\n\tMissing key(s) in state_dict: \"face_encoder_blocks.0.0.conv_block.0.weight\", \"face_encoder_blocks.0.0.conv_block.0.bias\", \"face_encoder_blocks.0.0.conv_block.1.weight\", \"face_encoder_blocks.0.0.conv_block.1.bias\", \"face_encoder_blocks.0.0.conv_block.1.running_mean\", \"face_encoder_blocks.0.0.conv_block.1.running_var\", \"face_encoder_blocks.1.0.conv_block.0.weight\", \"face_encoder_blocks.1.0.conv_block.0.bias\", \"face_encoder_blocks.1.0.conv_block.1.weight\", \"face_encoder_blocks.1.0.conv_block.1.bias\", \"face_encoder_blocks.1.0.conv_block.1.running_mean\", \"face_encoder_blocks.1.0.conv_block.1.running_var\", \"face_encoder_blocks.1.1.conv_block.0.weight\", \"face_encoder_blocks.1.1.conv_block.0.bias\", \"face_encoder_blocks.1.1.conv_block.1.weight\", \"face_encoder_blocks.1.1.conv_block.1.bias\", \"face_encoder_blocks.1.1.conv_block.1.running_mean\", \"face_encoder_blocks.1.1.conv_block.1.running_var\", \"face_encoder_blocks.1.2.conv_block.0.weight\", \"face_encoder_blocks.1.2.conv_block.0.bias\", \"face_encoder_blocks.1.2.conv_block.1.weight\", \"face_encoder_blocks.1.2.conv_block.1.bias\", \"face_encoder_blocks.1.2.conv_block.1.running_mean\", \"face_encoder_blocks.1.2.conv_block.1.running_var\", \"face_encoder_blocks.2.0.conv_block.0.weight\", \"face_encoder_blocks.2.0.conv_block.0.bias\", \"face_encoder_blocks.2.0.conv_block.1.weight\", \"face_encoder_blocks.2.0.conv_block.1.bias\", \"face_encoder_blocks.2.0.conv_block.1.running_mean\", \"face_encoder_blocks.2.0.conv_block.1.running_var\", \"face_encoder_blocks.2.1.conv_block.0.weight\", \"face_encoder_blocks.2.1.conv_block.0.bias\", \"face_encoder_blocks.2.1.conv_block.1.weight\", \"face_encoder_blocks.2.1.conv_block.1.bias\", \"face_encoder_blocks.2.1.conv_block.1.running_mean\", \"face_encoder_blocks.2.1.conv_block.1.running_var\", \"face_encoder_blocks.2.2.conv_block.0.weight\", \"face_encoder_blocks.2.2.conv_block.0.bias\", \"face_encoder_blocks.2.2.conv_block.1.weight\", \"face_encoder_blocks.2.2.conv_block.1.bias\", \"face_encoder_blocks.2.2.conv_block.1.running_mean\", \"face_encoder_blocks.2.2.conv_block.1.running_var\", \"face_encoder_blocks.2.3.conv_block.0.weight\", \"face_encoder_blocks.2.3.conv_block.0.bias\", \"face_encoder_blocks.2.3.conv_block.1.weight\", \"face_encoder_blocks.2.3.conv_block.1.bias\", \"face_encoder_blocks.2.3.conv_block.1.running_mean\", \"face_encoder_blocks.2.3.conv_block.1.running_var\", \"face_encoder_blocks.3.0.conv_block.0.weight\", \"face_encoder_blocks.3.0.conv_block.0.bias\", \"face_encoder_blocks.3.0.conv_block.1.weight\", \"face_encoder_blocks.3.0.conv_block.1.bias\", \"face_encoder_blocks.3.0.conv_block.1.running_mean\", \"face_encoder_blocks.3.0.conv_block.1.running_var\", \"face_encoder_blocks.3.1.conv_block.0.weight\", \"face_encoder_blocks.3.1.conv_block.0.bias\", \"face_encoder_blocks.3.1.conv_block.1.weight\", \"face_encoder_blocks.3.1.conv_block.1.bias\", \"face_encoder_blocks.3.1.conv_block.1.running_mean\", \"face_encoder_blocks.3.1.conv_block.1.running_var\", \"face_encoder_blocks.3.2.conv_block.0.weight\", \"face_encoder_blocks.3.2.conv_block.0.bias\", \"face_encoder_blocks.3.2.conv_block.1.weight\", \"face_encoder_blocks.3.2.conv_block.1.bias\", \"face_encoder_blocks.3.2.conv_block.1.running_mean\", \"face_encoder_blocks.3.2.conv_block.1.running_var\", \"face_encoder_blocks.4.0.conv_block.0.weight\", \"face_encoder_blocks.4.0.conv_block.0.bias\", \"face_encoder_blocks.4.0.conv_block.1.weight\", \"face_encoder_blocks.4.0.conv_block.1.bias\", \"face_encoder_blocks.4.0.conv_block.1.running_mean\", \"face_encoder_blocks.4.0.conv_block.1.running_var\", \"face_encoder_blocks.4.1.conv_block.0.weight\", \"face_encoder_blocks.4.1.conv_block.0.bias\", \"face_encoder_blocks.4.1.conv_block.1.weight\", \"face_encoder_blocks.4.1.conv_block.1.bias\", \"face_encoder_blocks.4.1.conv_block.1.running_mean\", \"face_encoder_blocks.4.1.conv_block.1.running_var\", \"face_encoder_blocks.4.2.conv_block.0.weight\", \"face_encoder_blocks.4.2.conv_block.0.bias\", \"face_encoder_blocks.4.2.conv_block.1.weight\", \"face_encoder_blocks.4.2.conv_block.1.bias\", \"face_encoder_blocks.4.2.conv_block.1.running_mean\", \"face_encoder_blocks.4.2.conv_block.1.running_var\", \"face_encoder_blocks.5.0.conv_block.0.weight\", \"face_encoder_blocks.5.0.conv_block.0.bias\", \"face_encoder_blocks.5.0.conv_block.1.weight\", \"face_encoder_blocks.5.0.conv_block.1.bias\", \"face_encoder_blocks.5.0.conv_block.1.running_mean\", \"face_encoder_blocks.5.0.conv_block.1.running_var\", \"face_encoder_blocks.5.1.conv_block.0.weight\", \"face_encoder_blocks.5.1.conv_block.0.bias\", \"face_encoder_blocks.5.1.conv_block.1.weight\", \"face_encoder_blocks.5.1.conv_block.1.bias\", \"face_encoder_blocks.5.1.conv_block.1.running_mean\", \"face_encoder_blocks.5.1.conv_block.1.running_var\", \"face_encoder_blocks.6.0.conv_block.0.weight\", \"face_encoder_blocks.6.0.conv_block.0.bias\", \"face_encoder_blocks.6.0.conv_block.1.weight\", \"face_encoder_blocks.6.0.conv_block.1.bias\", \"face_encoder_blocks.6.0.conv_block.1.running_mean\", \"face_encoder_blocks.6.0.conv_block.1.running_var\", \"face_encoder_blocks.6.1.conv_block.0.weight\", \"face_encoder_blocks.6.1.conv_block.0.bias\", \"face_encoder_blocks.6.1.conv_block.1.weight\", \"face_encoder_blocks.6.1.conv_block.1.bias\", \"face_encoder_blocks.6.1.conv_block.1.running_mean\", \"face_encoder_blocks.6.1.conv_block.1.running_var\", \"audio_encoder.0.conv_block.0.weight\", \"audio_encoder.0.conv_block.0.bias\", \"audio_encoder.0.conv_block.1.weight\", \"audio_encoder.0.conv_block.1.bias\", \"audio_encoder.0.conv_block.1.running_mean\", \"audio_encoder.0.conv_block.1.running_var\", \"audio_encoder.1.conv_block.0.weight\", \"audio_encoder.1.conv_block.0.bias\", \"audio_encoder.1.conv_block.1.weight\", \"audio_encoder.1.conv_block.1.bias\", \"audio_encoder.1.conv_block.1.running_mean\", \"audio_encoder.1.conv_block.1.running_var\", \"audio_encoder.2.conv_block.0.weight\", \"audio_encoder.2.conv_block.0.bias\", \"audio_encoder.2.conv_block.1.weight\", \"audio_encoder.2.conv_block.1.bias\", \"audio_encoder.2.conv_block.1.running_mean\", \"audio_encoder.2.conv_block.1.running_var\", \"audio_encoder.3.conv_block.0.weight\", \"audio_encoder.3.conv_block.0.bias\", \"audio_encoder.3.conv_block.1.weight\", \"audio_encoder.3.conv_block.1.bias\", \"audio_encoder.3.conv_block.1.running_mean\", \"audio_encoder.3.conv_block.1.running_var\", \"audio_encoder.4.conv_block.0.weight\", \"audio_encoder.4.conv_block.0.bias\", \"audio_encoder.4.conv_block.1.weight\", \"audio_encoder.4.conv_block.1.bias\", \"audio_encoder.4.conv_block.1.running_mean\", \"audio_encoder.4.conv_block.1.running_var\", \"audio_encoder.5.conv_block.0.weight\", \"audio_encoder.5.conv_block.0.bias\", \"audio_encoder.5.conv_block.1.weight\", \"audio_encoder.5.conv_block.1.bias\", \"audio_encoder.5.conv_block.1.running_mean\", \"audio_encoder.5.conv_block.1.running_var\", \"audio_encoder.6.conv_block.0.weight\", \"audio_encoder.6.conv_block.0.bias\", \"audio_encoder.6.conv_block.1.weight\", \"audio_encoder.6.conv_block.1.bias\", \"audio_encoder.6.conv_block.1.running_mean\", \"audio_encoder.6.conv_block.1.running_var\", \"audio_encoder.7.conv_block.0.weight\", \"audio_encoder.7.conv_block.0.bias\", \"audio_encoder.7.conv_block.1.weight\", \"audio_encoder.7.conv_block.1.bias\", \"audio_encoder.7.conv_block.1.running_mean\", \"audio_encoder.7.conv_block.1.running_var\", \"audio_encoder.8.conv_block.0.weight\", \"audio_encoder.8.conv_block.0.bias\", \"audio_encoder.8.conv_block.1.weight\", \"audio_encoder.8.conv_block.1.bias\", \"audio_encoder.8.conv_block.1.running_mean\", \"audio_encoder.8.conv_block.1.running_var\", \"audio_encoder.9.conv_block.0.weight\", \"audio_encoder.9.conv_block.0.bias\", \"audio_encoder.9.conv_block.1.weight\", \"audio_encoder.9.conv_block.1.bias\", \"audio_encoder.9.conv_block.1.running_mean\", \"audio_encoder.9.conv_block.1.running_var\", \"audio_encoder.10.conv_block.0.weight\", \"audio_encoder.10.conv_block.0.bias\", \"audio_encoder.10.conv_block.1.weight\", \"audio_encoder.10.conv_block.1.bias\", \"audio_encoder.10.conv_block.1.running_mean\", \"audio_encoder.10.conv_block.1.running_var\", \"audio_encoder.11.conv_block.0.weight\", \"audio_encoder.11.conv_block.0.bias\", \"audio_encoder.11.conv_block.1.weight\", \"audio_encoder.11.conv_block.1.bias\", \"audio_encoder.11.conv_block.1.running_mean\", \"audio_encoder.11.conv_block.1.running_var\", \"audio_encoder.12.conv_block.0.weight\", \"audio_encoder.12.conv_block.0.bias\", \"audio_encoder.12.conv_block.1.weight\", \"audio_encoder.12.conv_block.1.bias\", \"audio_encoder.12.conv_block.1.running_mean\", \"audio_encoder.12.conv_block.1.running_var\", \"face_decoder_blocks.0.0.conv_block.0.weight\", \"face_decoder_blocks.0.0.conv_block.0.bias\", \"face_decoder_blocks.0.0.conv_block.1.weight\", \"face_decoder_blocks.0.0.conv_block.1.bias\", \"face_decoder_blocks.0.0.conv_block.1.running_mean\", \"face_decoder_blocks.0.0.conv_block.1.running_var\", \"face_decoder_blocks.1.0.conv_block.0.weight\", \"face_decoder_blocks.1.0.conv_block.0.bias\", \"face_decoder_blocks.1.0.conv_block.1.weight\", \"face_decoder_blocks.1.0.conv_block.1.bias\", \"face_decoder_blocks.1.0.conv_block.1.running_mean\", \"face_decoder_blocks.1.0.conv_block.1.running_var\", \"face_decoder_blocks.1.1.conv_block.0.weight\", \"face_decoder_blocks.1.1.conv_block.0.bias\", \"face_decoder_blocks.1.1.conv_block.1.weight\", \"face_decoder_blocks.1.1.conv_block.1.bias\", \"face_decoder_blocks.1.1.conv_block.1.running_mean\", \"face_decoder_blocks.1.1.conv_block.1.running_var\", \"face_decoder_blocks.2.0.conv_block.0.weight\", \"face_decoder_blocks.2.0.conv_block.0.bias\", \"face_decoder_blocks.2.0.conv_block.1.weight\", \"face_decoder_blocks.2.0.conv_block.1.bias\", \"face_decoder_blocks.2.0.conv_block.1.running_mean\", \"face_decoder_blocks.2.0.conv_block.1.running_var\", \"face_decoder_blocks.2.1.conv_block.0.weight\", \"face_decoder_blocks.2.1.conv_block.0.bias\", \"face_decoder_blocks.2.1.conv_block.1.weight\", \"face_decoder_blocks.2.1.conv_block.1.bias\", \"face_decoder_blocks.2.1.conv_block.1.running_mean\", \"face_decoder_blocks.2.1.conv_block.1.running_var\", \"face_decoder_blocks.2.2.conv_block.0.weight\", \"face_decoder_blocks.2.2.conv_block.0.bias\", \"face_decoder_blocks.2.2.conv_block.1.weight\", \"face_decoder_blocks.2.2.conv_block.1.bias\", \"face_decoder_blocks.2.2.conv_block.1.running_mean\", \"face_decoder_blocks.2.2.conv_block.1.running_var\", \"face_decoder_blocks.3.0.conv_block.0.weight\", \"face_decoder_blocks.3.0.conv_block.0.bias\", \"face_decoder_blocks.3.0.conv_block.1.weight\", \"face_decoder_blocks.3.0.conv_block.1.bias\", \"face_decoder_blocks.3.0.conv_block.1.running_mean\", \"face_decoder_blocks.3.0.conv_block.1.running_var\", \"face_decoder_blocks.3.1.conv_block.0.weight\", \"face_decoder_blocks.3.1.conv_block.0.bias\", \"face_decoder_blocks.3.1.conv_block.1.weight\", \"face_decoder_blocks.3.1.conv_block.1.bias\", \"face_decoder_blocks.3.1.conv_block.1.running_mean\", \"face_decoder_blocks.3.1.conv_block.1.running_var\", \"face_decoder_blocks.3.2.conv_block.0.weight\", \"face_decoder_blocks.3.2.conv_block.0.bias\", \"face_decoder_blocks.3.2.conv_block.1.weight\", \"face_decoder_blocks.3.2.conv_block.1.bias\", \"face_decoder_blocks.3.2.conv_block.1.running_mean\", \"face_decoder_blocks.3.2.conv_block.1.running_var\", \"face_decoder_blocks.4.0.conv_block.0.weight\", \"face_decoder_blocks.4.0.conv_block.0.bias\", \"face_decoder_blocks.4.0.conv_block.1.weight\", \"face_decoder_blocks.4.0.conv_block.1.bias\", \"face_decoder_blocks.4.0.conv_block.1.running_mean\", \"face_decoder_blocks.4.0.conv_block.1.running_var\", \"face_decoder_blocks.4.1.conv_block.0.weight\", \"face_decoder_blocks.4.1.conv_block.0.bias\", \"face_decoder_blocks.4.1.conv_block.1.weight\", \"face_decoder_blocks.4.1.conv_block.1.bias\", \"face_decoder_blocks.4.1.conv_block.1.running_mean\", \"face_decoder_blocks.4.1.conv_block.1.running_var\", \"face_decoder_blocks.4.2.conv_block.0.weight\", \"face_decoder_blocks.4.2.conv_block.0.bias\", \"face_decoder_blocks.4.2.conv_block.1.weight\", \"face_decoder_blocks.4.2.conv_block.1.bias\", \"face_decoder_blocks.4.2.conv_block.1.running_mean\", \"face_decoder_blocks.4.2.conv_block.1.running_var\", \"face_decoder_blocks.5.0.conv_block.0.weight\", \"face_decoder_blocks.5.0.conv_block.0.bias\", \"face_decoder_blocks.5.0.conv_block.1.weight\", \"face_decoder_blocks.5.0.conv_block.1.bias\", \"face_decoder_blocks.5.0.conv_block.1.running_mean\", \"face_decoder_blocks.5.0.conv_block.1.running_var\", \"face_decoder_blocks.5.1.conv_block.0.weight\", \"face_decoder_blocks.5.1.conv_block.0.bias\", \"face_decoder_blocks.5.1.conv_block.1.weight\", \"face_decoder_blocks.5.1.conv_block.1.bias\", \"face_decoder_blocks.5.1.conv_block.1.running_mean\", \"face_decoder_blocks.5.1.conv_block.1.running_var\", \"face_decoder_blocks.5.2.conv_block.0.weight\", \"face_decoder_blocks.5.2.conv_block.0.bias\", \"face_decoder_blocks.5.2.conv_block.1.weight\", \"face_decoder_blocks.5.2.conv_block.1.bias\", \"face_decoder_blocks.5.2.conv_block.1.running_mean\", \"face_decoder_blocks.5.2.conv_block.1.running_var\", \"face_decoder_blocks.6.0.conv_block.0.weight\", \"face_decoder_blocks.6.0.conv_block.0.bias\", \"face_decoder_blocks.6.0.conv_block.1.weight\", \"face_decoder_blocks.6.0.conv_block.1.bias\", \"face_decoder_blocks.6.0.conv_block.1.running_mean\", \"face_decoder_blocks.6.0.conv_block.1.running_var\", \"face_decoder_blocks.6.1.conv_block.0.weight\", \"face_decoder_blocks.6.1.conv_block.0.bias\", \"face_decoder_blocks.6.1.conv_block.1.weight\", \"face_decoder_blocks.6.1.conv_block.1.bias\", \"face_decoder_blocks.6.1.conv_block.1.running_mean\", \"face_decoder_blocks.6.1.conv_block.1.running_var\", \"face_decoder_blocks.6.2.conv_block.0.weight\", \"face_decoder_blocks.6.2.conv_block.0.bias\", \"face_decoder_blocks.6.2.conv_block.1.weight\", \"face_decoder_blocks.6.2.conv_block.1.bias\", \"face_decoder_blocks.6.2.conv_block.1.running_mean\", \"face_decoder_blocks.6.2.conv_block.1.running_var\", \"output_block.0.conv_block.0.weight\", \"output_block.0.conv_block.0.bias\", \"output_block.0.conv_block.1.weight\", \"output_block.0.conv_block.1.bias\", \"output_block.0.conv_block.1.running_mean\", \"output_block.0.conv_block.1.running_var\", \"output_block.1.weight\", \"output_block.1.bias\". \n\tUnexpected key(s) in state_dict: \"module.face_encoder_blocks.0.0.conv_block.0.weight\", \"module.face_encoder_blocks.0.0.conv_block.0.bias\", \"module.face_encoder_blocks.0.0.conv_block.1.weight\", \"module.face_encoder_blocks.0.0.conv_block.1.bias\", \"module.face_encoder_blocks.0.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.0.0.conv_block.1.running_var\", \"module.face_encoder_blocks.0.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.0.conv_block.0.weight\", \"module.face_encoder_blocks.1.0.conv_block.0.bias\", \"module.face_encoder_blocks.1.0.conv_block.1.weight\", \"module.face_encoder_blocks.1.0.conv_block.1.bias\", \"module.face_encoder_blocks.1.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.1.0.conv_block.1.running_var\", \"module.face_encoder_blocks.1.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.1.conv_block.0.weight\", \"module.face_encoder_blocks.1.1.conv_block.0.bias\", \"module.face_encoder_blocks.1.1.conv_block.1.weight\", \"module.face_encoder_blocks.1.1.conv_block.1.bias\", \"module.face_encoder_blocks.1.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.1.1.conv_block.1.running_var\", \"module.face_encoder_blocks.1.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.2.conv_block.0.weight\", \"module.face_encoder_blocks.1.2.conv_block.0.bias\", \"module.face_encoder_blocks.1.2.conv_block.1.weight\", \"module.face_encoder_blocks.1.2.conv_block.1.bias\", \"module.face_encoder_blocks.1.2.conv_block.1.running_mean\", \"module.face_encoder_blocks.1.2.conv_block.1.running_var\", \"module.face_encoder_blocks.1.2.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.2.0.conv_block.0.weight\", \"module.face_encoder_blocks.2.0.conv_block.0.bias\", \"module.face_encoder_blocks.2.0.conv_block.1.weight\", \"module.face_encoder_blocks.2.0.conv_block.1.bias\", \"module.face_encoder_blocks.2.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.2.0.conv_block.1.running_var\", \"module.face_encoder_blocks.2.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.2.1.conv_block.0.weight\", \"module.face_encoder_blocks.2.1.conv_block.0.bias\", \"module.face_encoder_blocks.2.1.conv_block.1.weight\", \"module.face_encoder_blocks.2.1.conv_block.1.bias\", \"module.face_encoder_blocks.2.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.2.1.conv_block.1.running_var\", \"module.face_encoder_blocks.2.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.2.2.conv_block.0.weight\", \"module.face_encoder_blocks.2.2.conv_block.0.bias\", \"module.face_encoder_blocks.2.2.conv_block.1.weight\", \"module.face_encoder_blocks.2.2.conv_block.1.bias\", \"module.face_encoder_blocks.2.2.conv_block.1.running_mean\", \"module.face_encoder_blocks.2.2.conv_block.1.running_var\", \"module.face_encoder_blocks.2.2.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.2.3.conv_block.0.weight\", \"module.face_encoder_blocks.2.3.conv_block.0.bias\", \"module.face_encoder_blocks.2.3.conv_block.1.weight\", \"module.face_encoder_blocks.2.3.conv_block.1.bias\", \"module.face_encoder_blocks.2.3.conv_block.1.running_mean\", \"module.face_encoder_blocks.2.3.conv_block.1.running_var\", \"module.face_encoder_blocks.2.3.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.3.0.conv_block.0.weight\", \"module.face_encoder_blocks.3.0.conv_block.0.bias\", \"module.face_encoder_blocks.3.0.conv_block.1.weight\", \"module.face_encoder_blocks.3.0.conv_block.1.bias\", \"module.face_encoder_blocks.3.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.3.0.conv_block.1.running_var\", \"module.face_encoder_blocks.3.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.3.1.conv_block.0.weight\", \"module.face_encoder_blocks.3.1.conv_block.0.bias\", \"module.face_encoder_blocks.3.1.conv_block.1.weight\", \"module.face_encoder_blocks.3.1.conv_block.1.bias\", \"module.face_encoder_blocks.3.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.3.1.conv_block.1.running_var\", \"module.face_encoder_blocks.3.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.3.2.conv_block.0.weight\", \"module.face_encoder_blocks.3.2.conv_block.0.bias\", \"module.face_encoder_blocks.3.2.conv_block.1.weight\", \"module.face_encoder_blocks.3.2.conv_block.1.bias\", \"module.face_encoder_blocks.3.2.conv_block.1.running_mean\", \"module.face_encoder_blocks.3.2.conv_block.1.running_var\", \"module.face_encoder_blocks.3.2.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.4.0.conv_block.0.weight\", \"module.face_encoder_blocks.4.0.conv_block.0.bias\", \"module.face_encoder_blocks.4.0.conv_block.1.weight\", \"module.face_encoder_blocks.4.0.conv_block.1.bias\", \"module.face_encoder_blocks.4.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.4.0.conv_block.1.running_var\", \"module.face_encoder_blocks.4.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.4.1.conv_block.0.weight\", \"module.face_encoder_blocks.4.1.conv_block.0.bias\", \"module.face_encoder_blocks.4.1.conv_block.1.weight\", \"module.face_encoder_blocks.4.1.conv_block.1.bias\", \"module.face_encoder_blocks.4.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.4.1.conv_block.1.running_var\", \"module.face_encoder_blocks.4.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.4.2.conv_block.0.weight\", \"module.face_encoder_blocks.4.2.conv_block.0.bias\", \"module.face_encoder_blocks.4.2.conv_block.1.weight\", \"module.face_encoder_blocks.4.2.conv_block.1.bias\", \"module.face_encoder_blocks.4.2.conv_block.1.running_mean\", \"module.face_encoder_blocks.4.2.conv_block.1.running_var\", \"module.face_encoder_blocks.4.2.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.5.0.conv_block.0.weight\", \"module.face_encoder_blocks.5.0.conv_block.0.bias\", \"module.face_encoder_blocks.5.0.conv_block.1.weight\", \"module.face_encoder_blocks.5.0.conv_block.1.bias\", \"module.face_encoder_blocks.5.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.5.0.conv_block.1.running_var\", \"module.face_encoder_blocks.5.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.5.1.conv_block.0.weight\", \"module.face_encoder_blocks.5.1.conv_block.0.bias\", \"module.face_encoder_blocks.5.1.conv_block.1.weight\", \"module.face_encoder_blocks.5.1.conv_block.1.bias\", \"module.face_encoder_blocks.5.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.5.1.conv_block.1.running_var\", \"module.face_encoder_blocks.5.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.6.0.conv_block.0.weight\", \"module.face_encoder_blocks.6.0.conv_block.0.bias\", \"module.face_encoder_blocks.6.0.conv_block.1.weight\", \"module.face_encoder_blocks.6.0.conv_block.1.bias\", \"module.face_encoder_blocks.6.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.6.0.conv_block.1.running_var\", \"module.face_encoder_blocks.6.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.6.1.conv_block.0.weight\", \"module.face_encoder_blocks.6.1.conv_block.0.bias\", \"module.face_encoder_blocks.6.1.conv_block.1.weight\", \"module.face_encoder_blocks.6.1.conv_block.1.bias\", \"module.face_encoder_blocks.6.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.6.1.conv_block.1.running_var\", \"module.face_encoder_blocks.6.1.conv_block.1.num_batches_tracked\", \"module.audio_encoder.0.conv_block.0.weight\", \"module.audio_encoder.0.conv_block.0.bias\", \"module.audio_encoder.0.conv_block.1.weight\", \"module.audio_encoder.0.conv_block.1.bias\", \"module.audio_encoder.0.conv_block.1.running_mean\", \"module.audio_encoder.0.conv_block.1.running_var\", \"module.audio_encoder.0.conv_block.1.num_batches_tracked\", \"module.audio_encoder.1.conv_block.0.weight\", \"module.audio_encoder.1.conv_block.0.bias\", \"module.audio_encoder.1.conv_block.1.weight\", \"module.audio_encoder.1.conv_block.1.bias\", \"module.audio_encoder.1.conv_block.1.running_mean\", \"module.audio_encoder.1.conv_block.1.running_var\", \"module.audio_encoder.1.conv_block.1.num_batches_tracked\", \"module.audio_encoder.2.conv_block.0.weight\", \"module.audio_encoder.2.conv_block.0.bias\", \"module.audio_encoder.2.conv_block.1.weight\", \"module.audio_encoder.2.conv_block.1.bias\", \"module.audio_encoder.2.conv_block.1.running_mean\", \"module.audio_encoder.2.conv_block.1.running_var\", \"module.audio_encoder.2.conv_block.1.num_batches_tracked\", \"module.audio_encoder.3.conv_block.0.weight\", \"module.audio_encoder.3.conv_block.0.bias\", \"module.audio_encoder.3.conv_block.1.weight\", \"module.audio_encoder.3.conv_block.1.bias\", \"module.audio_encoder.3.conv_block.1.running_mean\", \"module.audio_encoder.3.conv_block.1.running_var\", \"module.audio_encoder.3.conv_block.1.num_batches_tracked\", \"module.audio_encoder.4.conv_block.0.weight\", \"module.audio_encoder.4.conv_block.0.bias\", \"module.audio_encoder.4.conv_block.1.weight\", \"module.audio_encoder.4.conv_block.1.bias\", \"module.audio_encoder.4.conv_block.1.running_mean\", \"module.audio_encoder.4.conv_block.1.running_var\", \"module.audio_encoder.4.conv_block.1.num_batches_tracked\", \"module.audio_encoder.5.conv_block.0.weight\", \"module.audio_encoder.5.conv_block.0.bias\", \"module.audio_encoder.5.conv_block.1.weight\", \"module.audio_encoder.5.conv_block.1.bias\", \"module.audio_encoder.5.conv_block.1.running_mean\", \"module.audio_encoder.5.conv_block.1.running_var\", \"module.audio_encoder.5.conv_block.1.num_batches_tracked\", \"module.audio_encoder.6.conv_block.0.weight\", \"module.audio_encoder.6.conv_block.0.bias\", \"module.audio_encoder.6.conv_block.1.weight\", \"module.audio_encoder.6.conv_block.1.bias\", \"module.audio_encoder.6.conv_block.1.running_mean\", \"module.audio_encoder.6.conv_block.1.running_var\", \"module.audio_encoder.6.conv_block.1.num_batches_tracked\", \"module.audio_encoder.7.conv_block.0.weight\", \"module.audio_encoder.7.conv_block.0.bias\", \"module.audio_encoder.7.conv_block.1.weight\", \"module.audio_encoder.7.conv_block.1.bias\", \"module.audio_encoder.7.conv_block.1.running_mean\", \"module.audio_encoder.7.conv_block.1.running_var\", \"module.audio_encoder.7.conv_block.1.num_batches_tracked\", \"module.audio_encoder.8.conv_block.0.weight\", \"module.audio_encoder.8.conv_block.0.bias\", \"module.audio_encoder.8.conv_block.1.weight\", \"module.audio_encoder.8.conv_block.1.bias\", \"module.audio_encoder.8.conv_block.1.running_mean\", \"module.audio_encoder.8.conv_block.1.running_var\", \"module.audio_encoder.8.conv_block.1.num_batches_tracked\", \"module.audio_encoder.9.conv_block.0.weight\", \"module.audio_encoder.9.conv_block.0.bias\", \"module.audio_encoder.9.conv_block.1.weight\", \"module.audio_encoder.9.conv_block.1.bias\", \"module.audio_encoder.9.conv_block.1.running_mean\", \"module.audio_encoder.9.conv_block.1.running_var\", \"module.audio_encoder.9.conv_block.1.num_batches_tracked\", \"module.audio_encoder.10.conv_block.0.weight\", \"module.audio_encoder.10.conv_block.0.bias\", \"module.audio_encoder.10.conv_block.1.weight\", \"module.audio_encoder.10.conv_block.1.bias\", \"module.audio_encoder.10.conv_block.1.running_mean\", \"module.audio_encoder.10.conv_block.1.running_var\", \"module.audio_encoder.10.conv_block.1.num_batches_tracked\", \"module.audio_encoder.11.conv_block.0.weight\", \"module.audio_encoder.11.conv_block.0.bias\", \"module.audio_encoder.11.conv_block.1.weight\", \"module.audio_encoder.11.conv_block.1.bias\", \"module.audio_encoder.11.conv_block.1.running_mean\", \"module.audio_encoder.11.conv_block.1.running_var\", \"module.audio_encoder.11.conv_block.1.num_batches_tracked\", \"module.audio_encoder.12.conv_block.0.weight\", \"module.audio_encoder.12.conv_block.0.bias\", \"module.audio_encoder.12.conv_block.1.weight\", \"module.audio_encoder.12.conv_block.1.bias\", \"module.audio_encoder.12.conv_block.1.running_mean\", \"module.audio_encoder.12.conv_block.1.running_var\", \"module.audio_encoder.12.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.0.0.conv_block.0.weight\", \"module.face_decoder_blocks.0.0.conv_block.0.bias\", \"module.face_decoder_blocks.0.0.conv_block.1.weight\", \"module.face_decoder_blocks.0.0.conv_block.1.bias\", \"module.face_decoder_blocks.0.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.0.0.conv_block.1.running_var\", \"module.face_decoder_blocks.0.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.1.0.conv_block.0.weight\", \"module.face_decoder_blocks.1.0.conv_block.0.bias\", \"module.face_decoder_blocks.1.0.conv_block.1.weight\", \"module.face_decoder_blocks.1.0.conv_block.1.bias\", \"module.face_decoder_blocks.1.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.1.0.conv_block.1.running_var\", \"module.face_decoder_blocks.1.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.1.1.conv_block.0.weight\", \"module.face_decoder_blocks.1.1.conv_block.0.bias\", \"module.face_decoder_blocks.1.1.conv_block.1.weight\", \"module.face_decoder_blocks.1.1.conv_block.1.bias\", \"module.face_decoder_blocks.1.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.1.1.conv_block.1.running_var\", \"module.face_decoder_blocks.1.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.2.0.conv_block.0.weight\", \"module.face_decoder_blocks.2.0.conv_block.0.bias\", \"module.face_decoder_blocks.2.0.conv_block.1.weight\", \"module.face_decoder_blocks.2.0.conv_block.1.bias\", \"module.face_decoder_blocks.2.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.2.0.conv_block.1.running_var\", \"module.face_decoder_blocks.2.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.2.1.conv_block.0.weight\", \"module.face_decoder_blocks.2.1.conv_block.0.bias\", \"module.face_decoder_blocks.2.1.conv_block.1.weight\", \"module.face_decoder_blocks.2.1.conv_block.1.bias\", \"module.face_decoder_blocks.2.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.2.1.conv_block.1.running_var\", \"module.face_decoder_blocks.2.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.2.2.conv_block.0.weight\", \"module.face_decoder_blocks.2.2.conv_block.0.bias\", \"module.face_decoder_blocks.2.2.conv_block.1.weight\", \"module.face_decoder_blocks.2.2.conv_block.1.bias\", \"module.face_decoder_blocks.2.2.conv_block.1.running_mean\", \"module.face_decoder_blocks.2.2.conv_block.1.running_var\", \"module.face_decoder_blocks.2.2.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.3.0.conv_block.0.weight\", \"module.face_decoder_blocks.3.0.conv_block.0.bias\", \"module.face_decoder_blocks.3.0.conv_block.1.weight\", \"module.face_decoder_blocks.3.0.conv_block.1.bias\", \"module.face_decoder_blocks.3.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.3.0.conv_block.1.running_var\", \"module.face_decoder_blocks.3.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.3.1.conv_block.0.weight\", \"module.face_decoder_blocks.3.1.conv_block.0.bias\", \"module.face_decoder_blocks.3.1.conv_block.1.weight\", \"module.face_decoder_blocks.3.1.conv_block.1.bias\", \"module.face_decoder_blocks.3.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.3.1.conv_block.1.running_var\", \"module.face_decoder_blocks.3.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.3.2.conv_block.0.weight\", \"module.face_decoder_blocks.3.2.conv_block.0.bias\", \"module.face_decoder_blocks.3.2.conv_block.1.weight\", \"module.face_decoder_blocks.3.2.conv_block.1.bias\", \"module.face_decoder_blocks.3.2.conv_block.1.running_mean\", \"module.face_decoder_blocks.3.2.conv_block.1.running_var\", \"module.face_decoder_blocks.3.2.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.4.0.conv_block.0.weight\", \"module.face_decoder_blocks.4.0.conv_block.0.bias\", \"module.face_decoder_blocks.4.0.conv_block.1.weight\", \"module.face_decoder_blocks.4.0.conv_block.1.bias\", \"module.face_decoder_blocks.4.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.4.0.conv_block.1.running_var\", \"module.face_decoder_blocks.4.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.4.1.conv_block.0.weight\", \"module.face_decoder_blocks.4.1.conv_block.0.bias\", \"module.face_decoder_blocks.4.1.conv_block.1.weight\", \"module.face_decoder_blocks.4.1.conv_block.1.bias\", \"module.face_decoder_blocks.4.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.4.1.conv_block.1.running_var\", \"module.face_decoder_blocks.4.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.4.2.conv_block.0.weight\", \"module.face_decoder_blocks.4.2.conv_block.0.bias\", \"module.face_decoder_blocks.4.2.conv_block.1.weight\", \"module.face_decoder_blocks.4.2.conv_block.1.bias\", \"module.face_decoder_blocks.4.2.conv_block.1.running_mean\", \"module.face_decoder_blocks.4.2.conv_block.1.running_var\", \"module.face_decoder_blocks.4.2.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.5.0.conv_block.0.weight\", \"module.face_decoder_blocks.5.0.conv_block.0.bias\", \"module.face_decoder_blocks.5.0.conv_block.1.weight\", \"module.face_decoder_blocks.5.0.conv_block.1.bias\", \"module.face_decoder_blocks.5.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.5.0.conv_block.1.running_var\", \"module.face_decoder_blocks.5.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.5.1.conv_block.0.weight\", \"module.face_decoder_blocks.5.1.conv_block.0.bias\", \"module.face_decoder_blocks.5.1.conv_block.1.weight\", \"module.face_decoder_blocks.5.1.conv_block.1.bias\", \"module.face_decoder_blocks.5.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.5.1.conv_block.1.running_var\", \"module.face_decoder_blocks.5.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.5.2.conv_block.0.weight\", \"module.face_decoder_blocks.5.2.conv_block.0.bias\", \"module.face_decoder_blocks.5.2.conv_block.1.weight\", \"module.face_decoder_blocks.5.2.conv_block.1.bias\", \"module.face_decoder_blocks.5.2.conv_block.1.running_mean\", \"module.face_decoder_blocks.5.2.conv_block.1.running_var\", \"module.face_decoder_blocks.5.2.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.6.0.conv_block.0.weight\", \"module.face_decoder_blocks.6.0.conv_block.0.bias\", \"module.face_decoder_blocks.6.0.conv_block.1.weight\", \"module.face_decoder_blocks.6.0.conv_block.1.bias\", \"module.face_decoder_blocks.6.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.6.0.conv_block.1.running_var\", \"module.face_decoder_blocks.6.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.6.1.conv_block.0.weight\", \"module.face_decoder_blocks.6.1.conv_block.0.bias\", \"module.face_decoder_blocks.6.1.conv_block.1.weight\", \"module.face_decoder_blocks.6.1.conv_block.1.bias\", \"module.face_decoder_blocks.6.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.6.1.conv_block.1.running_var\", \"module.face_decoder_blocks.6.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.6.2.conv_block.0.weight\", \"module.face_decoder_blocks.6.2.conv_block.0.bias\", \"module.face_decoder_blocks.6.2.conv_block.1.weight\", \"module.face_decoder_blocks.6.2.conv_block.1.bias\", \"module.face_decoder_blocks.6.2.conv_block.1.running_mean\", \"module.face_decoder_blocks.6.2.conv_block.1.running_var\", \"module.face_decoder_blocks.6.2.conv_block.1.num_batches_tracked\", \"module.output_block.0.conv_block.0.weight\", \"module.output_block.0.conv_block.0.bias\", \"module.output_block.0.conv_block.1.weight\", \"module.output_block.0.conv_block.1.bias\", \"module.output_block.0.conv_block.1.running_mean\", \"module.output_block.0.conv_block.1.running_var\", \"module.output_block.0.conv_block.1.num_batches_tracked\", \"module.output_block.1.weight\", \"module.output_block.1.bias\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1411129896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mwav2lip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wav2lip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWAV2LIP_CKPT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Wav2Lip loaded: {sum(p.numel() for p in wav2lip.parameters()) / 1e6:.1f}M params\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1411129896.py\u001b[0m in \u001b[0;36mload_wav2lip\u001b[0;34m(ckpt_path, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"state_dict\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2624\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2625\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2626\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Wav2Lip:\n\tMissing key(s) in state_dict: \"face_encoder_blocks.0.0.conv_block.0.weight\", \"face_encoder_blocks.0.0.conv_block.0.bias\", \"face_encoder_blocks.0.0.conv_block.1.weight\", \"face_encoder_blocks.0.0.conv_block.1.bias\", \"face_encoder_blocks.0.0.conv_block.1.running_mean\", \"face_encoder_blocks.0.0.conv_block.1.running_var\", \"face_encoder_blocks.1.0.conv_block.0.weight\", \"face_encoder_blocks.1.0.conv_block.0.bias\", \"face_encoder_blocks.1.0.conv_block.1.weight\", \"face_encoder_blocks.1.0.conv_block.1.bias\", \"face_encoder_blocks.1.0.conv_block.1.running_mean\", \"face_encoder_blocks.1.0.conv_block.1.running_var\", \"face_encoder_blocks.1.1.conv_block.0.weight\", \"face_encoder_blocks.1.1.conv_block.0.bias\", \"face_encoder_blocks.1.1.conv_block.1.weight\", \"face_encoder_blocks.1.1.conv_block.1.bias\", \"face_encoder_blocks.1.1.conv_block.1.running_mean\", \"face_encoder_blocks.1.1.conv_block.1.running_var\", \"face_encoder_blocks.1.2.conv_block.0.weight\", \"face_encoder_blocks.1.2.conv_block.0.bias\", \"face_encoder_blocks.1.2.conv_block.1.weight\", \"face_encoder_blocks.1.2.conv_block.1.bias\", \"face_encoder_blocks.1.2.conv_block.1.running_mean\", \"face_encoder_blocks.1.2.conv_block.1.running_var\", \"face_encoder_blocks.2.0.conv_block.0.weight\", \"face_encoder_blocks.2.0.conv_block.0.bias\", \"face_encoder_blocks.2.0.conv_block.1.weight\", \"face_encoder_blocks.2.0.conv_block.1.bias\", \"face_encoder_blocks.2.0.conv_block.1.running_mean\", \"face_encoder_blocks.2.0.conv_block.1.running_var\", \"face_encoder_blocks....\n\tUnexpected key(s) in state_dict: \"module.face_encoder_blocks.0.0.conv_block.0.weight\", \"module.face_encoder_blocks.0.0.conv_block.0.bias\", \"module.face_encoder_blocks.0.0.conv_block.1.weight\", \"module.face_encoder_blocks.0.0.conv_block.1.bias\", \"module.face_encoder_blocks.0.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.0.0.conv_block.1.running_var\", \"module.face_encoder_blocks.0.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.0.conv_block.0.weight\", \"module.face_encoder_blocks.1.0.conv_block.0.bias\", \"module.face_encoder_blocks.1.0.conv_block.1.weight\", \"module.face_encoder_blocks.1.0.conv_block.1.bias\", \"module.face_encoder_blocks.1.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.1.0.conv_block.1.running_var\", \"module.face_encoder_blocks.1.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.1.conv_block.0.weight\", \"module.face_encoder_blocks.1.1.conv_block.0.bias\", \"module.face_encoder_blocks.1.1.conv_block.1.weight\", \"module.face_encoder_blocks.1.1.conv_block.1.bias\", \"module.face_encoder_blocks.1.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.1.1.conv_block.1.running_var\", \"module.face_encoder_blocks.1.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.2.conv_block.0.weight\", \"module.face_encoder_blocks.1.2.conv_block.0.bias\", \"module.face_encoder_blocks.1.2.conv_block.1.weight\", \"module.face_encoder_blocks.1.2.conv_block.1.bias\", \"module.face_encoder_blocks.1.2.conv_block.1.running_mean..."
          ]
        }
      ],
      "source": [
        "def load_wav2lip(ckpt_path, device):\n",
        "    model = Wav2LipModel()\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
        "    state = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
        "    state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
        "    model.load_state_dict(state, strict=False)\n",
        "    return model.to(device)\n",
        "\n",
        "wav2lip = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "print(f\"Wav2Lip loaded: {sum(p.numel() for p in wav2lip.parameters()) / 1e6:.1f}M params\")\n",
        "\n",
        "audio_enc, audio_proc = load_frozen_audio_encoder(BEST_AUDIO_PATH, DEVICE)\n",
        "video_enc = load_frozen_video_encoder(BEST_VIDEO_PATH, DEVICE)\n",
        "video_preprocess = DifferentiableVideoPreprocess(224).to(DEVICE)\n",
        "print(\"Frozen emotion encoders loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrL9lg7bRYE1"
      },
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "\n",
        "CONFIGS = [\n",
        "    {\"name\": \"wav2lip-baseline\", \"lambda_emo\": 0.0},\n",
        "    {\"name\": \"wav2lip-emo-001\",  \"lambda_emo\": 0.01},\n",
        "    {\"name\": \"wav2lip-emo-005\",  \"lambda_emo\": 0.05},\n",
        "    {\"name\": \"wav2lip-emo-01\",   \"lambda_emo\": 0.1},\n",
        "]\n",
        "\n",
        "LR = 1e-4\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 4\n",
        "PATIENCE = 5\n",
        "T_FRAMES = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMsiebgnRYE1"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, scaler, emotion_loss_fn, lambda_emo):\n",
        "    model.train()\n",
        "    total_recon, total_emo, total_loss = 0.0, 0.0, 0.0\n",
        "\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        B, T = mel.shape[0], mel.shape[1]\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        all_gen = []\n",
        "        recon = 0.0\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            for t in range(T):\n",
        "                gen = model(mel[:, t], face_in[:, t])\n",
        "                recon += F.l1_loss(gen, gt[:, t])\n",
        "                all_gen.append(gen)\n",
        "            recon = recon / T\n",
        "\n",
        "            emo = torch.tensor(0.0, device=DEVICE)\n",
        "            if lambda_emo > 0:\n",
        "                gen_video = torch.stack(all_gen, dim=1)\n",
        "                audio_emb = extract_audio_embedding(\n",
        "                    audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                video_emb = extract_video_embedding(\n",
        "                    video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                emo = emotion_loss_fn(audio_emb.detach(), video_emb)\n",
        "\n",
        "            loss = recon + lambda_emo * emo\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_recon += recon.item()\n",
        "        total_emo += emo.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {\"recon\": total_recon / n, \"emotion\": total_emo / n, \"total\": total_loss / n}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, emotion_loss_fn, lambda_emo):\n",
        "    model.eval()\n",
        "    total_recon, total_emo, total_loss = 0.0, 0.0, 0.0\n",
        "    metric = EmotionAgreementMetric()\n",
        "\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        B, T = mel.shape[0], mel.shape[1]\n",
        "\n",
        "        all_gen = []\n",
        "        recon = 0.0\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            for t in range(T):\n",
        "                gen = model(mel[:, t], face_in[:, t])\n",
        "                recon += F.l1_loss(gen, gt[:, t])\n",
        "                all_gen.append(gen)\n",
        "            recon = recon / T\n",
        "\n",
        "            emo = torch.tensor(0.0, device=DEVICE)\n",
        "            if lambda_emo > 0:\n",
        "                gen_video = torch.stack(all_gen, dim=1)\n",
        "                audio_emb = extract_audio_embedding(\n",
        "                    audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                video_emb = extract_video_embedding(\n",
        "                    video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                emo = emotion_loss_fn(audio_emb, video_emb)\n",
        "                metric.update(audio_emb, video_emb)\n",
        "\n",
        "            loss = recon + lambda_emo * emo\n",
        "\n",
        "        total_recon += recon.item()\n",
        "        total_emo += emo.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    n = len(loader)\n",
        "    result = {\"recon\": total_recon / n, \"emotion\": total_emo / n, \"total\": total_loss / n}\n",
        "    if lambda_emo > 0:\n",
        "        result.update(metric.compute())\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFEJ98kMRYE1"
      },
      "outputs": [],
      "source": [
        "train_ds = Wav2LipDataset(METADATA, \"train\", T=T_FRAMES)\n",
        "val_ds = Wav2LipDataset(METADATA, \"val\", T=T_FRAMES)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=0, collate_fn=collate_wav2lip)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=0, collate_fn=collate_wav2lip)\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for cfg in CONFIGS:\n",
        "    name = cfg[\"name\"]\n",
        "    lambda_emo = cfg[\"lambda_emo\"]\n",
        "    print(f\"\\n{'='*60}\\n{name} (lambda_emo={lambda_emo})\\n{'='*60}\")\n",
        "\n",
        "    wandb.init(project=\"uncanny-valley-wav2lip\", name=name,\n",
        "               config={**cfg, \"lr\": LR, \"epochs\": EPOCHS}, reinit=True)\n",
        "\n",
        "    model = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    scaler = GradScaler(enabled=DEVICE == \"cuda\")\n",
        "    emotion_loss_fn = CrossModalEmotionLoss(weight=1.0)\n",
        "\n",
        "    best_val, patience_cnt = float(\"inf\"), 0\n",
        "    save_path = OUT_DIR / name\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t = train_one_epoch(model, train_loader, optimizer, scaler,\n",
        "                            emotion_loss_fn, lambda_emo)\n",
        "        v = evaluate(model, val_loader, emotion_loss_fn, lambda_emo)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/recon\": t[\"recon\"], \"train/emotion\": t[\"emotion\"], \"train/total\": t[\"total\"],\n",
        "            \"val/recon\": v[\"recon\"], \"val/emotion\": v[\"emotion\"], \"val/total\": v[\"total\"],\n",
        "            **{f\"val/{k}\": v[k] for k in [\"avg_cosine_sim\", \"agreement_rate\"] if k in v},\n",
        "        })\n",
        "\n",
        "        print(f\"  [{epoch+1:2d}/{EPOCHS}] \"\n",
        "              f\"t_loss={t['total']:.4f} v_loss={v['total']:.4f} v_recon={v['recon']:.4f}\"\n",
        "              + (f\" cos_sim={v.get('avg_cosine_sim', 0):.3f}\" if lambda_emo > 0 else \"\"))\n",
        "\n",
        "        if v[\"total\"] < best_val:\n",
        "            best_val = v[\"total\"]\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(model.state_dict(), save_path / \"wav2lip.pth\")\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= PATIENCE:\n",
        "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, scaler\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    all_results.append({\"name\": name, \"lambda_emo\": lambda_emo, \"best_val\": best_val})\n",
        "    print(f\"  Best val loss: {best_val:.4f} -> {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE7_BkkGRYE1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(all_results).sort_values(\"best_val\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.bar(df[\"name\"], df[\"best_val\"], color=\"steelblue\")\n",
        "ax.set_ylabel(\"Best Val Loss\")\n",
        "ax.set_title(\"Wav2Lip Fine-tuning: _emo Ablation\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KdaP1RtRYE2"
      },
      "outputs": [],
      "source": [
        "best_name = df.iloc[0][\"name\"]\n",
        "best_model = load_wav2lip(WAV2LIP_CKPT, DEVICE)\n",
        "best_model.load_state_dict(torch.load(OUT_DIR / best_name / \"wav2lip.pth\", map_location=DEVICE, weights_only=True))\n",
        "best_model.eval()\n",
        "print(f\"Loaded best model: {best_name}\")\n",
        "\n",
        "metric = EmotionAgreementMetric()\n",
        "all_recon = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Evaluating best\"):\n",
        "        mel = batch[\"mel\"].to(DEVICE)\n",
        "        face_in = batch[\"face_input\"].to(DEVICE)\n",
        "        gt = batch[\"gt\"].to(DEVICE)\n",
        "        T = mel.shape[1]\n",
        "\n",
        "        all_gen = []\n",
        "        for t in range(T):\n",
        "            gen = best_model(mel[:, t], face_in[:, t])\n",
        "            all_gen.append(gen)\n",
        "            all_recon.append(F.l1_loss(gen, gt[:, t]).item())\n",
        "\n",
        "        gen_video = torch.stack(all_gen, dim=1)\n",
        "        audio_emb = extract_audio_embedding(audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "        video_emb = extract_video_embedding(video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "        metric.update(audio_emb, video_emb)\n",
        "\n",
        "agreement = metric.compute()\n",
        "print(f\"\\nBest model evaluation:\")\n",
        "print(f\"  Avg L1 recon:     {np.mean(all_recon):.4f}\")\n",
        "print(f\"  Avg cosine sim:   {agreement['avg_cosine_sim']:.4f}\")\n",
        "print(f\"  Agreement rate:   {agreement['agreement_rate']:.4f}\")\n",
        "print(f\"  Std cosine sim:   {agreement['std_cosine_sim']:.4f}\")\n",
        "\n",
        "del best_model\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
