{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RWgNve4IdUEu"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers librosa wandb face-alignment dlib yacs pydub gfpgan\n",
        "\n",
        "# Fix basicsr's deprecated torchvision import (functional_tensor -> functional)\n",
        "!python -c \"\n",
        "import importlib, pathlib\n",
        "spec = importlib.util.find_spec('basicsr')\n",
        "if spec and spec.origin:\n",
        "    deg = pathlib.Path(spec.origin).parent / 'data' / 'degradations.py'\n",
        "    if deg.exists():\n",
        "        txt = deg.read_text()\n",
        "        if 'functional_tensor' in txt:\n",
        "            deg.write_text(txt.replace(\n",
        "                'from torchvision.transforms.functional_tensor import rgb_to_grayscale',\n",
        "                'from torchvision.transforms.functional import rgb_to_grayscale'))\n",
        "            print(f'Patched {deg}')\n",
        "        else:\n",
        "            print('Already patched')\n",
        "    else:\n",
        "        print(f'File not found: {deg}')\n",
        "else:\n",
        "    print('basicsr not found')\n",
        "\"\n",
        "\n",
        "!git clone https://github.com/OpenTalker/SadTalker.git 2>/dev/null || true\n",
        "\n",
        "!mkdir -p SadTalker/checkpoints\n",
        "!wget -q -nc \"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/mapping_00109-model.pth.tar\" -O SadTalker/checkpoints/mapping.pth.tar\n",
        "!wget -q -nc \"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/SadTalker_V0.0.2_256.safetensors\" -O SadTalker/checkpoints/sadtalker_256.safetensors\n",
        "!wget -q -nc \"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/BFM_Fitting.zip\" -O /tmp/BFM_Fitting.zip && unzip -qo /tmp/BFM_Fitting.zip -d SadTalker/checkpoints/ 2>/dev/null || true\n",
        "!ls -lh SadTalker/checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf_XjYhEdUEv",
        "outputId": "b87edba5-8d03-4ef0-d5eb-04f69c593ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content\")\n",
        "sys.path.insert(0, \"/content/SadTalker\")\n",
        "sys.path.insert(0, \"/content/SadTalker/src\")\n",
        "\n",
        "import numpy as np\n",
        "if not hasattr(np, \"VisibleDeprecationWarning\"):\n",
        "    np.VisibleDeprecationWarning = DeprecationWarning\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import wandb\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from emotion_utils import (\n",
        "    CrossModalEmotionLoss,\n",
        "    DifferentiableVideoPreprocess,\n",
        "    EmotionAgreementMetric,\n",
        "    load_frozen_audio_encoder,\n",
        "    load_frozen_video_encoder,\n",
        "    extract_audio_embedding,\n",
        "    extract_video_embedding,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "METADATA = \"/content/processed_data/metadata.json\"\n",
        "BEST_AUDIO_PATH = \"/content/trained_encoders_v2/w2v2-lg-lr2e5\"\n",
        "BEST_VIDEO_PATH = \"/content/trained_encoders_v2/tsf-lr3e5-16f-nf\"\n",
        "OUT_DIR = Path(\"/content/sadtalker_finetuned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EXCLUDE = {0, 1, 3, 5, 7}\n",
        "REMAP = {2: 0, 4: 1, 6: 2}\n",
        "EMOTIONS = [\"happy\", \"angry\", \"disgust\"]\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "oUPJR26FdUEv",
        "outputId": "a140d96e-8030-4249-c575-0adb27ead567"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchvision.transforms.functional_tensor'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-459930977.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio2exp_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio2exp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio2Exp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio2exp_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleWrapperV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfacerender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manimate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnimateFromCoeff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_facerender_batch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_facerender_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SadTalker/src/facerender/animate.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_enhancer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menhancer_generator_with_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menhancer_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaste_pic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaste_pic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideoio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_video_with_watermark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SadTalker/src/utils/face_enhancer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgfpgan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGFPGANer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gfpgan/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marchs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gfpgan/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdataset_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_dataset.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# import all the dataset modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0m_dataset_modules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'gfpgan.data.{file_name}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_filenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gfpgan/data/ffhq_degradation_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdegradations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdegradations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaths_from_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/basicsr/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marchs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/basicsr/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdataset_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_dataset.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# import all the dataset modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0m_dataset_modules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'basicsr.data.{file_name}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_filenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/basicsr/data/realesrgan_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegradations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcircular_lowpass_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_mixed_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_root_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimfrombytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# -------------------------------------------------------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "SadTalker pipeline overview:\n",
        "  Audio -> AudioEncoder -> ExpNet -> expression coefficients (3DMM)\n",
        "  Expression coefficients + source face -> FaceRenderer -> generated video\n",
        "\n",
        "We fine-tune the ExpNet (expression mapper) so that generated expressions\n",
        "carry the same emotion as detected in the audio by our frozen encoders.\n",
        "\n",
        "Gradient path: emotion_loss -> generated_frames -> renderer -> expression_coeffs -> ExpNet\n",
        "\"\"\"\n",
        "\n",
        "from src.audio2pose_models.audio2pose import Audio2Pose\n",
        "from src.audio2exp_models.audio2exp import Audio2Exp\n",
        "from src.audio2exp_models.networks import SimpleWrapperV2\n",
        "from src.facerender.animate import AnimateFromCoeff\n",
        "from src.generate_batch import get_data\n",
        "from src.generate_facerender_batch import get_facerender_data\n",
        "from src.utils.preprocess import CropAndExtract\n",
        "\n",
        "SADTALKER_CKPT = Path(\"/content/SadTalker/checkpoints\")\n",
        "\n",
        "print(\"SadTalker modules loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3OdDUabdUEv"
      },
      "outputs": [],
      "source": [
        "class ExpNetWrapper(nn.Module):\n",
        "    \"\"\"Wraps SadTalker's Audio2Exp for fine-tuning.\n",
        "    Only expression mapper is trainable; audio backbone stays frozen.\"\"\"\n",
        "\n",
        "    def __init__(self, audio2exp_model):\n",
        "        super().__init__()\n",
        "        self.model = audio2exp_model\n",
        "\n",
        "    def freeze_audio_backbone(self):\n",
        "        for name, p in self.model.named_parameters():\n",
        "            if \"netG\" not in name:\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, batch):\n",
        "        return self.model(batch)\n",
        "\n",
        "\n",
        "class EmotionAwareFaceRenderer(nn.Module):\n",
        "    \"\"\"Wraps SadTalker face renderer so output is differentiable for emotion loss.\"\"\"\n",
        "\n",
        "    def __init__(self, renderer):\n",
        "        super().__init__()\n",
        "        self.renderer = renderer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def render(self, source_img, coeffs_dict):\n",
        "        \"\"\"Render face given 3DMM coefficients. Frozen -- no gradients.\"\"\"\n",
        "        return self.renderer.generate(source_img, coeffs_dict)\n",
        "\n",
        "\n",
        "def load_sadtalker_components(ckpt_dir, device):\n",
        "    ckpt_dir = Path(ckpt_dir)\n",
        "\n",
        "    from src.utils.init_path import init_path\n",
        "    sadtalker_paths = init_path(\n",
        "        str(ckpt_dir), \"/content/SadTalker/src/config\",\n",
        "        \"256\", False, \"crop\"\n",
        "    )\n",
        "\n",
        "    preprocess_model = CropAndExtract(sadtalker_paths, device)\n",
        "\n",
        "    from src.test_audio2coeff import Audio2Coeff\n",
        "    audio2coeff = Audio2Coeff(sadtalker_paths, device)\n",
        "\n",
        "    from src.facerender.animate import AnimateFromCoeff\n",
        "    animate = AnimateFromCoeff(sadtalker_paths, device)\n",
        "\n",
        "    return preprocess_model, audio2coeff, animate\n",
        "\n",
        "\n",
        "try:\n",
        "    preprocess_model, audio2coeff, animate = load_sadtalker_components(SADTALKER_CKPT, DEVICE)\n",
        "    exp_net = ExpNetWrapper(audio2coeff.audio2exp)\n",
        "    exp_net.freeze_audio_backbone()\n",
        "    trainable = sum(p.numel() for p in exp_net.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in exp_net.parameters())\n",
        "    print(f\"ExpNet: {trainable/1e6:.1f}M trainable / {total/1e6:.1f}M total\")\n",
        "except Exception as e:\n",
        "    print(f\"SadTalker loading issue (expected on first run): {e}\")\n",
        "    print(\"Adjust checkpoint paths in cell 0 and re-run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXfzFP1AdUEw"
      },
      "outputs": [],
      "source": [
        "audio_enc, audio_proc = load_frozen_audio_encoder(BEST_AUDIO_PATH, DEVICE)\n",
        "video_enc = load_frozen_video_encoder(BEST_VIDEO_PATH, DEVICE)\n",
        "video_preprocess = DifferentiableVideoPreprocess(224).to(DEVICE)\n",
        "\n",
        "VIDEO_ENC_FRAMES = getattr(video_enc.config, \"num_frames\", 8)\n",
        "AUDIO_DIM = audio_enc.config.hidden_size\n",
        "VIDEO_DIM = video_enc.config.hidden_size\n",
        "PROJ_DIM = 256\n",
        "print(f\"Frozen encoders loaded. Video: {VIDEO_ENC_FRAMES} frames. \"\n",
        "      f\"Audio dim={AUDIO_DIM}, Video dim={VIDEO_DIM}, Proj dim={PROJ_DIM}\")\n",
        "\n",
        "\n",
        "def adapt_frames(frames, target_t):\n",
        "    \"\"\"Resample (B, T, C, H, W) to (B, target_t, C, H, W) via uniform index sampling.\"\"\"\n",
        "    B, T, C, H, W = frames.shape\n",
        "    if T == target_t:\n",
        "        return frames\n",
        "    idx = torch.linspace(0, T - 1, target_t).long()\n",
        "    return frames[:, idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCVTHLBYdUEw"
      },
      "outputs": [],
      "source": [
        "SR = 16000\n",
        "IMG_SIZE = 256\n",
        "\n",
        "class SadTalkerDataset(Dataset):\n",
        "    def __init__(self, metadata_path, split, n_frames=8):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data\n",
        "                        if s[\"split\"] == split and s[\"emotion_idx\"] not in EXCLUDE]\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "\n",
        "        wav, sr = torchaudio.load(s[\"audio_path\"])\n",
        "        audio_1d = wav.squeeze(0)\n",
        "\n",
        "        frames = np.load(s[\"frames_path\"]).astype(np.float32) / 255.0\n",
        "        n = frames.shape[0]\n",
        "\n",
        "        source_idx = 0\n",
        "        source = torch.from_numpy(frames[source_idx]).permute(2, 0, 1)\n",
        "        if source.shape[1] != IMG_SIZE or source.shape[2] != IMG_SIZE:\n",
        "            source = F.interpolate(source.unsqueeze(0), size=(IMG_SIZE, IMG_SIZE),\n",
        "                                   mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "\n",
        "        indices = np.linspace(0, n - 1, self.n_frames).astype(int)\n",
        "        gt_frames = torch.from_numpy(frames[indices]).permute(0, 3, 1, 2)\n",
        "        if gt_frames.shape[2] != IMG_SIZE or gt_frames.shape[3] != IMG_SIZE:\n",
        "            gt_frames = F.interpolate(gt_frames, size=(IMG_SIZE, IMG_SIZE),\n",
        "                                      mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        return {\n",
        "            \"audio\": audio_1d,\n",
        "            \"audio_path\": s[\"audio_path\"],\n",
        "            \"source\": source,\n",
        "            \"gt_frames\": gt_frames,\n",
        "            \"emotion\": REMAP[s[\"emotion_idx\"]],\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_sadtalker(batch):\n",
        "    return {\n",
        "        \"audio\": [b[\"audio\"] for b in batch],\n",
        "        \"audio_path\": [b[\"audio_path\"] for b in batch],\n",
        "        \"source\": torch.stack([b[\"source\"] for b in batch]),\n",
        "        \"gt_frames\": torch.stack([b[\"gt_frames\"] for b in batch]),\n",
        "        \"emotion\": torch.tensor([b[\"emotion\"] for b in batch]),\n",
        "    }\n",
        "\n",
        "\n",
        "train_ds = SadTalkerDataset(METADATA, \"train\", n_frames=8)\n",
        "val_ds = SadTalkerDataset(METADATA, \"val\", n_frames=8)\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dfp2iIzLdUEw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "SadTalker fine-tuning strategy:\n",
        "  1. Freeze: audio backbone, pose network, face renderer\n",
        "  2. Train: expression network (ExpNet / netG)\n",
        "  3. Loss: expression_coeff_loss + lambda_emo * cross_modal_emotion_loss\n",
        "\n",
        "Two loss paths:\n",
        "  (a) Coefficient loss -- L1 between predicted and GT 3DMM expression coefficients\n",
        "      (requires extracting GT coefficients from real faces)\n",
        "  (b) Emotion loss -- cosine distance between audio and video emotion embeddings\n",
        "      (requires rendering -> differentiable preprocess -> frozen emotion encoder)\n",
        "\n",
        "For efficiency, path (b) is computed every N steps (rendering is expensive).\n",
        "\"\"\"\n",
        "\n",
        "def coeff_loss_fn(pred_coeffs, gt_coeffs):\n",
        "    return F.l1_loss(pred_coeffs, gt_coeffs)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_3dmm_coeffs(preprocess_model, source_img, device):\n",
        "    \"\"\"Extract 3DMM coefficients from a face image using SadTalker's preprocessor.\"\"\"\n",
        "    if isinstance(source_img, torch.Tensor):\n",
        "        img = (source_img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    return preprocess_model.generate(img, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmhXv5Y1dUEw"
      },
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "\n",
        "CONFIGS = [\n",
        "    {\"name\": \"sadtalker-baseline\", \"lambda_emo\": 0.0},\n",
        "    {\"name\": \"sadtalker-emo-001\",  \"lambda_emo\": 0.01},\n",
        "    {\"name\": \"sadtalker-emo-005\",  \"lambda_emo\": 0.05},\n",
        "    {\"name\": \"sadtalker-emo-01\",   \"lambda_emo\": 0.1},\n",
        "]\n",
        "\n",
        "LR = 5e-5\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 2\n",
        "PATIENCE = 5\n",
        "EMO_EVAL_EVERY = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XSVk_RudUEw"
      },
      "outputs": [],
      "source": [
        "def generate_with_sadtalker(audio2coeff, animate, audio_path, source_img, device):\n",
        "    \"\"\"Full SadTalker inference for a single sample. Returns generated frames (T, C, H, W).\"\"\"\n",
        "    from src.generate_batch import get_data as get_batch_data\n",
        "\n",
        "    batch = get_batch_data(audio2coeff, audio_path, device)\n",
        "    coeff_dict = audio2coeff.generate(batch, save_path=None, return_coeffs=True)\n",
        "    rendered = animate.generate(source_img, coeff_dict)\n",
        "    return rendered\n",
        "\n",
        "\n",
        "def train_one_epoch(exp_net, train_loader, optimizer, scaler, emotion_loss_fn,\n",
        "                    lambda_emo, audio2coeff, animate, step_counter):\n",
        "    exp_net.model.netG.train()\n",
        "    total_loss, total_emo = 0.0, 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, leave=False):\n",
        "        source = batch[\"source\"].to(DEVICE)\n",
        "        B = source.shape[0]\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        emo_loss = torch.tensor(0.0, device=DEVICE)\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            if lambda_emo > 0 and step_counter[0] % EMO_EVAL_EVERY == 0:\n",
        "                gen_frames_list = []\n",
        "                for i in range(B):\n",
        "                    try:\n",
        "                        frames = generate_with_sadtalker(\n",
        "                            audio2coeff, animate,\n",
        "                            batch[\"audio_path\"][i], source[i], DEVICE)\n",
        "                        if isinstance(frames, torch.Tensor) and frames.dim() == 4:\n",
        "                            gen_frames_list.append(frames)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "                if gen_frames_list:\n",
        "                    n_frames = min(f.shape[0] for f in gen_frames_list)\n",
        "                    gen_video = torch.stack([f[:n_frames] for f in gen_frames_list])\n",
        "                    gen_video = gen_video.float() / 255.0 if gen_video.max() > 1 else gen_video.float()\n",
        "                    gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "\n",
        "                    audio_emb = extract_audio_embedding(\n",
        "                        audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                    video_emb = extract_video_embedding(\n",
        "                        video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                    emo_loss = emotion_loss_fn(audio_proj(audio_emb.detach()),\n",
        "                                              video_proj(video_emb))\n",
        "\n",
        "            loss = lambda_emo * emo_loss if lambda_emo > 0 else torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "        if loss.requires_grad:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(\n",
        "                [p for p in exp_net.parameters() if p.requires_grad], 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_emo += emo_loss.item()\n",
        "        step_counter[0] += 1\n",
        "        n += 1\n",
        "\n",
        "    return {\"total\": total_loss / max(n, 1), \"emotion\": total_emo / max(n, 1)}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(exp_net, val_loader, emotion_loss_fn, lambda_emo, audio2coeff, animate):\n",
        "    exp_net.model.netG.eval()\n",
        "    metric = EmotionAgreementMetric()\n",
        "    total_emo = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch in tqdm(val_loader, leave=False):\n",
        "        source = batch[\"source\"].to(DEVICE)\n",
        "        B = source.shape[0]\n",
        "\n",
        "        gen_frames_list = []\n",
        "        for i in range(B):\n",
        "            try:\n",
        "                frames = generate_with_sadtalker(\n",
        "                    audio2coeff, animate,\n",
        "                    batch[\"audio_path\"][i], source[i], DEVICE)\n",
        "                if isinstance(frames, torch.Tensor) and frames.dim() == 4:\n",
        "                    gen_frames_list.append(frames)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if not gen_frames_list:\n",
        "            continue\n",
        "\n",
        "        n_frames = min(f.shape[0] for f in gen_frames_list)\n",
        "        gen_video = torch.stack([f[:n_frames] for f in gen_frames_list])\n",
        "        gen_video = gen_video.float() / 255.0 if gen_video.max() > 1 else gen_video.float()\n",
        "        gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "\n",
        "        audio_emb = extract_audio_embedding(\n",
        "            audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "        video_emb = extract_video_embedding(\n",
        "            video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "\n",
        "        a_p, v_p = audio_proj(audio_emb), video_proj(video_emb)\n",
        "        emo_loss = emotion_loss_fn(a_p, v_p)\n",
        "        total_emo += emo_loss.item()\n",
        "        metric.update(a_p, v_p)\n",
        "        n += 1\n",
        "\n",
        "    result = {\"emotion\": total_emo / max(n, 1)}\n",
        "    result.update(metric.compute())\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWxXLKyFdUEw"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=0, collate_fn=collate_sadtalker)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=0, collate_fn=collate_sadtalker)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for cfg in CONFIGS:\n",
        "    name = cfg[\"name\"]\n",
        "    lambda_emo = cfg[\"lambda_emo\"]\n",
        "    print(f\"\\n{'='*60}\\n{name} (lambda_emo={lambda_emo})\\n{'='*60}\")\n",
        "\n",
        "    wandb.init(project=\"uncanny-valley-sadtalker\", name=name,\n",
        "               config={**cfg, \"lr\": LR, \"epochs\": EPOCHS}, reinit=True)\n",
        "\n",
        "    preprocess_model, audio2coeff, animate = load_sadtalker_components(SADTALKER_CKPT, DEVICE)\n",
        "    exp_net = ExpNetWrapper(audio2coeff.audio2exp)\n",
        "    exp_net.freeze_audio_backbone()\n",
        "\n",
        "    audio_proj = nn.Linear(AUDIO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "    video_proj = nn.Linear(VIDEO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "    trainable_params = [p for p in exp_net.parameters() if p.requires_grad]\n",
        "    if lambda_emo > 0:\n",
        "        trainable_params += list(audio_proj.parameters()) + list(video_proj.parameters())\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=LR)\n",
        "    scaler = GradScaler(enabled=DEVICE == \"cuda\")\n",
        "    emotion_loss_fn = CrossModalEmotionLoss(weight=1.0)\n",
        "\n",
        "    best_val, patience_cnt = float(\"inf\"), 0\n",
        "    save_path = OUT_DIR / name\n",
        "    step_counter = [0]\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t = train_one_epoch(exp_net, train_loader, optimizer, scaler,\n",
        "                            emotion_loss_fn, lambda_emo, audio2coeff, animate, step_counter)\n",
        "        v = evaluate(exp_net, val_loader, emotion_loss_fn, lambda_emo, audio2coeff, animate)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/total\": t[\"total\"], \"train/emotion\": t[\"emotion\"],\n",
        "            \"val/emotion\": v[\"emotion\"],\n",
        "            **{f\"val/{k}\": v[k] for k in [\"avg_cosine_sim\", \"agreement_rate\"] if k in v},\n",
        "        })\n",
        "\n",
        "        print(f\"  [{epoch+1:2d}/{EPOCHS}] \"\n",
        "              f\"t_loss={t['total']:.4f} v_emo={v['emotion']:.4f}\"\n",
        "              + (f\" cos_sim={v.get('avg_cosine_sim', 0):.3f}\" if lambda_emo > 0 else \"\"))\n",
        "\n",
        "        val_metric = v[\"emotion\"] if lambda_emo > 0 else v.get(\"avg_cosine_sim\", float(\"inf\"))\n",
        "        if val_metric < best_val:\n",
        "            best_val = val_metric\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(\n",
        "                {k: v for k, v in exp_net.model.netG.state_dict().items()},\n",
        "                save_path / \"expnet.pth\")\n",
        "            torch.save({\"audio_proj\": audio_proj.state_dict(),\n",
        "                         \"video_proj\": video_proj.state_dict()},\n",
        "                        save_path / \"projections.pth\")\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= PATIENCE:\n",
        "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    del preprocess_model, audio2coeff, animate, exp_net, optimizer, scaler, audio_proj, video_proj\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    all_results.append({\"name\": name, \"lambda_emo\": lambda_emo, \"best_val\": best_val})\n",
        "    print(f\"  Best val metric: {best_val:.4f} -> {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPY-niu2dUEw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(all_results).sort_values(\"best_val\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.bar(df[\"name\"], df[\"best_val\"], color=\"coral\")\n",
        "ax.set_ylabel(\"Best Val Emotion Loss\")\n",
        "ax.set_title(\"SadTalker Fine-tuning: Î»_emo Ablation\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrI6UyQ2dUEx"
      },
      "outputs": [],
      "source": [
        "best_name = df.iloc[0][\"name\"]\n",
        "print(f\"Best SadTalker variant: {best_name}\")\n",
        "\n",
        "preprocess_model, audio2coeff, animate = load_sadtalker_components(SADTALKER_CKPT, DEVICE)\n",
        "exp_net = ExpNetWrapper(audio2coeff.audio2exp)\n",
        "\n",
        "ckpt = torch.load(OUT_DIR / best_name / \"expnet.pth\", map_location=DEVICE, weights_only=True)\n",
        "exp_net.model.netG.load_state_dict(ckpt)\n",
        "exp_net.eval()\n",
        "\n",
        "audio_proj = nn.Linear(AUDIO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "video_proj = nn.Linear(VIDEO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "proj_ckpt = torch.load(OUT_DIR / best_name / \"projections.pth\", map_location=DEVICE, weights_only=True)\n",
        "audio_proj.load_state_dict(proj_ckpt[\"audio_proj\"])\n",
        "video_proj.load_state_dict(proj_ckpt[\"video_proj\"])\n",
        "audio_proj.eval()\n",
        "video_proj.eval()\n",
        "\n",
        "metric = EmotionAgreementMetric()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Evaluating best\"):\n",
        "        source = batch[\"source\"].to(DEVICE)\n",
        "        B = source.shape[0]\n",
        "\n",
        "        gen_frames_list = []\n",
        "        for i in range(B):\n",
        "            try:\n",
        "                frames = generate_with_sadtalker(\n",
        "                    audio2coeff, animate,\n",
        "                    batch[\"audio_path\"][i], source[i], DEVICE)\n",
        "                if isinstance(frames, torch.Tensor) and frames.dim() == 4:\n",
        "                    gen_frames_list.append(frames)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if not gen_frames_list:\n",
        "            continue\n",
        "\n",
        "        n_frames = min(f.shape[0] for f in gen_frames_list)\n",
        "        gen_video = torch.stack([f[:n_frames] for f in gen_frames_list])\n",
        "        gen_video = gen_video.float() / 255.0 if gen_video.max() > 1 else gen_video.float()\n",
        "        gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "\n",
        "        audio_emb = extract_audio_embedding(\n",
        "            audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "        video_emb = extract_video_embedding(\n",
        "            video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "        metric.update(audio_proj(audio_emb), video_proj(video_emb))\n",
        "\n",
        "agreement = metric.compute()\n",
        "print(f\"\\nBest model evaluation:\")\n",
        "print(f\"  Avg cosine sim:   {agreement['avg_cosine_sim']:.4f}\")\n",
        "print(f\"  Agreement rate:   {agreement['agreement_rate']:.4f}\")\n",
        "print(f\"  Std cosine sim:   {agreement['std_cosine_sim']:.4f}\")\n",
        "\n",
        "del preprocess_model, audio2coeff, animate, exp_net\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
