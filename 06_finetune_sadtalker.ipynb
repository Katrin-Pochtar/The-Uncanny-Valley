{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers librosa wandb face-alignment dlib yacs\n",
        "!git clone https://github.com/OpenTalker/SadTalker.git 2>/dev/null || true\n",
        "\n",
        "# Download pretrained checkpoints\n",
        "!mkdir -p SadTalker/checkpoints\n",
        "!wget -q -nc \"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/mapping_00109-model.pth.tar\" -O SadTalker/checkpoints/mapping.pth.tar\n",
        "!wget -q -nc \"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/SadTalker_V0.0.2_256.safetensors\" -O SadTalker/checkpoints/sadtalker_256.safetensors\n",
        "!wget -q -nc \"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/BFM_Fitting.zip\" -O /tmp/BFM_Fitting.zip && unzip -qo /tmp/BFM_Fitting.zip -d SadTalker/checkpoints/ 2>/dev/null || true\n",
        "\n",
        "echo \"If downloads fail, see: https://github.com/OpenTalker/SadTalker#-2-download-trained-models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content\")\n",
        "sys.path.insert(0, \"/content/SadTalker\")\n",
        "sys.path.insert(0, \"/content/SadTalker/src\")\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import wandb\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from emotion_utils import (\n",
        "    CrossModalEmotionLoss,\n",
        "    DifferentiableVideoPreprocess,\n",
        "    EmotionAgreementMetric,\n",
        "    load_frozen_audio_encoder,\n",
        "    load_frozen_video_encoder,\n",
        "    extract_audio_embedding,\n",
        "    extract_video_embedding,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "METADATA = \"/content/processed_data/metadata.json\"\n",
        "BEST_AUDIO_PATH = \"/content/trained_encoders_v2/hubert-er-lr5e5\"\n",
        "BEST_VIDEO_PATH = \"/content/trained_encoders_v2/tsf-lr3e5-16f\"\n",
        "OUT_DIR = Path(\"/content/sadtalker_finetuned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EXCLUDE = {0, 7}\n",
        "REMAP = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5}\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "SadTalker pipeline overview:\n",
        "  Audio -> AudioEncoder -> ExpNet -> expression coefficients (3DMM)\n",
        "  Expression coefficients + source face -> FaceRenderer -> generated video\n",
        "\n",
        "We fine-tune the ExpNet (expression mapper) so that generated expressions\n",
        "carry the same emotion as detected in the audio by our frozen encoders.\n",
        "\n",
        "Gradient path: emotion_loss -> generated_frames -> renderer -> expression_coeffs -> ExpNet\n",
        "\"\"\"\n",
        "\n",
        "from src.audio2pose_models.audio2pose import Audio2Pose\n",
        "from src.audio2exp_models.audio2exp import Audio2Exp\n",
        "from src.audio2exp_models.networks import SimpleWrapperV2\n",
        "from src.facerender.animate import AnimateFromCoeff\n",
        "from src.generate_batch import get_data\n",
        "from src.generate_facerender_batch import get_facerender_data\n",
        "from src.utils.preprocess import CropAndExtract\n",
        "\n",
        "SADTALKER_CKPT = Path(\"/content/SadTalker/checkpoints\")\n",
        "\n",
        "print(\"SadTalker modules loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpNetWrapper(nn.Module):\n",
        "    \"\"\"Wraps SadTalker's Audio2Exp for fine-tuning.\n",
        "    Only expression mapper is trainable; audio backbone stays frozen.\"\"\"\n",
        "\n",
        "    def __init__(self, audio2exp_model):\n",
        "        super().__init__()\n",
        "        self.model = audio2exp_model\n",
        "\n",
        "    def freeze_audio_backbone(self):\n",
        "        for name, p in self.model.named_parameters():\n",
        "            if \"netG\" not in name:\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, batch):\n",
        "        return self.model(batch)\n",
        "\n",
        "\n",
        "class EmotionAwareFaceRenderer(nn.Module):\n",
        "    \"\"\"Wraps SadTalker face renderer so output is differentiable for emotion loss.\"\"\"\n",
        "\n",
        "    def __init__(self, renderer):\n",
        "        super().__init__()\n",
        "        self.renderer = renderer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def render(self, source_img, coeffs_dict):\n",
        "        \"\"\"Render face given 3DMM coefficients. Frozen -- no gradients.\"\"\"\n",
        "        return self.renderer.generate(source_img, coeffs_dict)\n",
        "\n",
        "\n",
        "def load_sadtalker_components(ckpt_dir, device):\n",
        "    ckpt_dir = Path(ckpt_dir)\n",
        "\n",
        "    from src.utils.init_path import init_path\n",
        "    sadtalker_paths = init_path(\n",
        "        str(ckpt_dir), \"/content/SadTalker/src/config\",\n",
        "        \"256\", False, \"crop\"\n",
        "    )\n",
        "\n",
        "    preprocess_model = CropAndExtract(sadtalker_paths, device)\n",
        "\n",
        "    from src.test_audio2coeff import Audio2Coeff\n",
        "    audio2coeff = Audio2Coeff(sadtalker_paths, device)\n",
        "\n",
        "    from src.facerender.animate import AnimateFromCoeff\n",
        "    animate = AnimateFromCoeff(sadtalker_paths, device)\n",
        "\n",
        "    return preprocess_model, audio2coeff, animate\n",
        "\n",
        "\n",
        "try:\n",
        "    preprocess_model, audio2coeff, animate = load_sadtalker_components(SADTALKER_CKPT, DEVICE)\n",
        "    exp_net = ExpNetWrapper(audio2coeff.audio2exp)\n",
        "    exp_net.freeze_audio_backbone()\n",
        "    trainable = sum(p.numel() for p in exp_net.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in exp_net.parameters())\n",
        "    print(f\"ExpNet: {trainable/1e6:.1f}M trainable / {total/1e6:.1f}M total\")\n",
        "except Exception as e:\n",
        "    print(f\"SadTalker loading issue (expected on first run): {e}\")\n",
        "    print(\"Adjust checkpoint paths in cell 0 and re-run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_enc, audio_proc = load_frozen_audio_encoder(BEST_AUDIO_PATH, DEVICE)\n",
        "video_enc = load_frozen_video_encoder(BEST_VIDEO_PATH, DEVICE)\n",
        "video_preprocess = DifferentiableVideoPreprocess(224).to(DEVICE)\n",
        "print(\"Frozen emotion encoders loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SR = 16000\n",
        "IMG_SIZE = 256\n",
        "\n",
        "class SadTalkerDataset(Dataset):\n",
        "    def __init__(self, metadata_path, split, n_frames=8):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data\n",
        "                        if s[\"split\"] == split and s[\"emotion_idx\"] not in EXCLUDE]\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "\n",
        "        wav, sr = torchaudio.load(s[\"audio_path\"])\n",
        "        audio_1d = wav.squeeze(0)\n",
        "\n",
        "        frames = np.load(s[\"frames_path\"]).astype(np.float32) / 255.0\n",
        "        n = frames.shape[0]\n",
        "\n",
        "        source_idx = 0\n",
        "        source = torch.from_numpy(frames[source_idx]).permute(2, 0, 1)\n",
        "        if source.shape[1] != IMG_SIZE or source.shape[2] != IMG_SIZE:\n",
        "            source = F.interpolate(source.unsqueeze(0), size=(IMG_SIZE, IMG_SIZE),\n",
        "                                   mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "\n",
        "        indices = np.linspace(0, n - 1, self.n_frames).astype(int)\n",
        "        gt_frames = torch.from_numpy(frames[indices]).permute(0, 3, 1, 2)\n",
        "        if gt_frames.shape[2] != IMG_SIZE or gt_frames.shape[3] != IMG_SIZE:\n",
        "            gt_frames = F.interpolate(gt_frames, size=(IMG_SIZE, IMG_SIZE),\n",
        "                                      mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        return {\n",
        "            \"audio\": audio_1d,\n",
        "            \"audio_path\": s[\"audio_path\"],\n",
        "            \"source\": source,\n",
        "            \"gt_frames\": gt_frames,\n",
        "            \"emotion\": REMAP[s[\"emotion_idx\"]],\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_sadtalker(batch):\n",
        "    return {\n",
        "        \"audio\": [b[\"audio\"] for b in batch],\n",
        "        \"audio_path\": [b[\"audio_path\"] for b in batch],\n",
        "        \"source\": torch.stack([b[\"source\"] for b in batch]),\n",
        "        \"gt_frames\": torch.stack([b[\"gt_frames\"] for b in batch]),\n",
        "        \"emotion\": torch.tensor([b[\"emotion\"] for b in batch]),\n",
        "    }\n",
        "\n",
        "\n",
        "train_ds = SadTalkerDataset(METADATA, \"train\", n_frames=8)\n",
        "val_ds = SadTalkerDataset(METADATA, \"val\", n_frames=8)\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "SadTalker fine-tuning strategy:\n",
        "  1. Freeze: audio backbone, pose network, face renderer\n",
        "  2. Train: expression network (ExpNet / netG)\n",
        "  3. Loss: expression_coeff_loss + lambda_emo * cross_modal_emotion_loss\n",
        "\n",
        "Two loss paths:\n",
        "  (a) Coefficient loss -- L1 between predicted and GT 3DMM expression coefficients\n",
        "      (requires extracting GT coefficients from real faces)\n",
        "  (b) Emotion loss -- cosine distance between audio and video emotion embeddings\n",
        "      (requires rendering -> differentiable preprocess -> frozen emotion encoder)\n",
        "\n",
        "For efficiency, path (b) is computed every N steps (rendering is expensive).\n",
        "\"\"\"\n",
        "\n",
        "def coeff_loss_fn(pred_coeffs, gt_coeffs):\n",
        "    return F.l1_loss(pred_coeffs, gt_coeffs)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_3dmm_coeffs(preprocess_model, source_img, device):\n",
        "    \"\"\"Extract 3DMM coefficients from a face image using SadTalker's preprocessor.\"\"\"\n",
        "    if isinstance(source_img, torch.Tensor):\n",
        "        img = (source_img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    return preprocess_model.generate(img, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "\n",
        "CONFIGS = [\n",
        "    {\"name\": \"sadtalker-baseline\", \"lambda_emo\": 0.0},\n",
        "    {\"name\": \"sadtalker-emo-001\",  \"lambda_emo\": 0.01},\n",
        "    {\"name\": \"sadtalker-emo-005\",  \"lambda_emo\": 0.05},\n",
        "    {\"name\": \"sadtalker-emo-01\",   \"lambda_emo\": 0.1},\n",
        "]\n",
        "\n",
        "LR = 5e-5\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 2\n",
        "PATIENCE = 5\n",
        "EMO_EVAL_EVERY = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_with_sadtalker(audio2coeff, animate, audio_path, source_img, device):\n",
        "    \"\"\"Full SadTalker inference for a single sample. Returns generated frames (T, C, H, W).\"\"\"\n",
        "    from src.generate_batch import get_data as get_batch_data\n",
        "\n",
        "    batch = get_batch_data(audio2coeff, audio_path, device)\n",
        "    coeff_dict = audio2coeff.generate(batch, save_path=None, return_coeffs=True)\n",
        "    rendered = animate.generate(source_img, coeff_dict)\n",
        "    return rendered\n",
        "\n",
        "\n",
        "def train_one_epoch(exp_net, train_loader, optimizer, scaler, emotion_loss_fn,\n",
        "                    lambda_emo, audio2coeff, animate, step_counter):\n",
        "    exp_net.model.netG.train()\n",
        "    total_loss, total_emo = 0.0, 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, leave=False):\n",
        "        source = batch[\"source\"].to(DEVICE)\n",
        "        B = source.shape[0]\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        emo_loss = torch.tensor(0.0, device=DEVICE)\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            if lambda_emo > 0 and step_counter[0] % EMO_EVAL_EVERY == 0:\n",
        "                gen_frames_list = []\n",
        "                for i in range(B):\n",
        "                    try:\n",
        "                        frames = generate_with_sadtalker(\n",
        "                            audio2coeff, animate,\n",
        "                            batch[\"audio_path\"][i], source[i], DEVICE)\n",
        "                        if isinstance(frames, torch.Tensor) and frames.dim() == 4:\n",
        "                            gen_frames_list.append(frames)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "                if gen_frames_list:\n",
        "                    n_frames = min(f.shape[0] for f in gen_frames_list)\n",
        "                    gen_video = torch.stack([f[:n_frames] for f in gen_frames_list])\n",
        "                    gen_video = gen_video.float() / 255.0 if gen_video.max() > 1 else gen_video.float()\n",
        "\n",
        "                    audio_emb = extract_audio_embedding(\n",
        "                        audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                    video_emb = extract_video_embedding(\n",
        "                        video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                    emo_loss = emotion_loss_fn(audio_emb.detach(), video_emb)\n",
        "\n",
        "            loss = lambda_emo * emo_loss if lambda_emo > 0 else torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "        if loss.requires_grad:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(\n",
        "                [p for p in exp_net.parameters() if p.requires_grad], 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_emo += emo_loss.item()\n",
        "        step_counter[0] += 1\n",
        "        n += 1\n",
        "\n",
        "    return {\"total\": total_loss / max(n, 1), \"emotion\": total_emo / max(n, 1)}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(exp_net, val_loader, emotion_loss_fn, lambda_emo, audio2coeff, animate):\n",
        "    exp_net.model.netG.eval()\n",
        "    metric = EmotionAgreementMetric()\n",
        "    total_emo = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch in tqdm(val_loader, leave=False):\n",
        "        source = batch[\"source\"].to(DEVICE)\n",
        "        B = source.shape[0]\n",
        "\n",
        "        gen_frames_list = []\n",
        "        for i in range(B):\n",
        "            try:\n",
        "                frames = generate_with_sadtalker(\n",
        "                    audio2coeff, animate,\n",
        "                    batch[\"audio_path\"][i], source[i], DEVICE)\n",
        "                if isinstance(frames, torch.Tensor) and frames.dim() == 4:\n",
        "                    gen_frames_list.append(frames)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if not gen_frames_list:\n",
        "            continue\n",
        "\n",
        "        n_frames = min(f.shape[0] for f in gen_frames_list)\n",
        "        gen_video = torch.stack([f[:n_frames] for f in gen_frames_list])\n",
        "        gen_video = gen_video.float() / 255.0 if gen_video.max() > 1 else gen_video.float()\n",
        "\n",
        "        audio_emb = extract_audio_embedding(\n",
        "            audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "        video_emb = extract_video_embedding(\n",
        "            video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "\n",
        "        emo_loss = emotion_loss_fn(audio_emb, video_emb)\n",
        "        total_emo += emo_loss.item()\n",
        "        metric.update(audio_emb, video_emb)\n",
        "        n += 1\n",
        "\n",
        "    result = {\"emotion\": total_emo / max(n, 1)}\n",
        "    result.update(metric.compute())\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=0, collate_fn=collate_sadtalker)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=0, collate_fn=collate_sadtalker)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for cfg in CONFIGS:\n",
        "    name = cfg[\"name\"]\n",
        "    lambda_emo = cfg[\"lambda_emo\"]\n",
        "    print(f\"\\n{'='*60}\\n{name} (lambda_emo={lambda_emo})\\n{'='*60}\")\n",
        "\n",
        "    wandb.init(project=\"uncanny-valley-sadtalker\", name=name,\n",
        "               config={**cfg, \"lr\": LR, \"epochs\": EPOCHS}, reinit=True)\n",
        "\n",
        "    preprocess_model, audio2coeff, animate = load_sadtalker_components(SADTALKER_CKPT, DEVICE)\n",
        "    exp_net = ExpNetWrapper(audio2coeff.audio2exp)\n",
        "    exp_net.freeze_audio_backbone()\n",
        "\n",
        "    trainable_params = [p for p in exp_net.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=LR)\n",
        "    scaler = GradScaler(enabled=DEVICE == \"cuda\")\n",
        "    emotion_loss_fn = CrossModalEmotionLoss(weight=1.0)\n",
        "\n",
        "    best_val, patience_cnt = float(\"inf\"), 0\n",
        "    save_path = OUT_DIR / name\n",
        "    step_counter = [0]\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t = train_one_epoch(exp_net, train_loader, optimizer, scaler,\n",
        "                            emotion_loss_fn, lambda_emo, audio2coeff, animate, step_counter)\n",
        "        v = evaluate(exp_net, val_loader, emotion_loss_fn, lambda_emo, audio2coeff, animate)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/total\": t[\"total\"], \"train/emotion\": t[\"emotion\"],\n",
        "            \"val/emotion\": v[\"emotion\"],\n",
        "            **{f\"val/{k}\": v[k] for k in [\"avg_cosine_sim\", \"agreement_rate\"] if k in v},\n",
        "        })\n",
        "\n",
        "        print(f\"  [{epoch+1:2d}/{EPOCHS}] \"\n",
        "              f\"t_loss={t['total']:.4f} v_emo={v['emotion']:.4f}\"\n",
        "              + (f\" cos_sim={v.get('avg_cosine_sim', 0):.3f}\" if lambda_emo > 0 else \"\"))\n",
        "\n",
        "        val_metric = v[\"emotion\"] if lambda_emo > 0 else v.get(\"avg_cosine_sim\", float(\"inf\"))\n",
        "        if val_metric < best_val:\n",
        "            best_val = val_metric\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(\n",
        "                {k: v for k, v in exp_net.model.netG.state_dict().items()},\n",
        "                save_path / \"expnet.pth\")\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= PATIENCE:\n",
        "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    del preprocess_model, audio2coeff, animate, exp_net, optimizer, scaler\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    all_results.append({\"name\": name, \"lambda_emo\": lambda_emo, \"best_val\": best_val})\n",
        "    print(f\"  Best val metric: {best_val:.4f} -> {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(all_results).sort_values(\"best_val\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.bar(df[\"name\"], df[\"best_val\"], color=\"coral\")\n",
        "ax.set_ylabel(\"Best Val Emotion Loss\")\n",
        "ax.set_title(\"SadTalker Fine-tuning: Î»_emo Ablation\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_name = df.iloc[0][\"name\"]\n",
        "print(f\"Best SadTalker variant: {best_name}\")\n",
        "\n",
        "preprocess_model, audio2coeff, animate = load_sadtalker_components(SADTALKER_CKPT, DEVICE)\n",
        "exp_net = ExpNetWrapper(audio2coeff.audio2exp)\n",
        "\n",
        "ckpt = torch.load(OUT_DIR / best_name / \"expnet.pth\", map_location=DEVICE, weights_only=True)\n",
        "exp_net.model.netG.load_state_dict(ckpt)\n",
        "exp_net.eval()\n",
        "\n",
        "metric = EmotionAgreementMetric()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Evaluating best\"):\n",
        "        source = batch[\"source\"].to(DEVICE)\n",
        "        B = source.shape[0]\n",
        "\n",
        "        gen_frames_list = []\n",
        "        for i in range(B):\n",
        "            try:\n",
        "                frames = generate_with_sadtalker(\n",
        "                    audio2coeff, animate,\n",
        "                    batch[\"audio_path\"][i], source[i], DEVICE)\n",
        "                if isinstance(frames, torch.Tensor) and frames.dim() == 4:\n",
        "                    gen_frames_list.append(frames)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if not gen_frames_list:\n",
        "            continue\n",
        "\n",
        "        n_frames = min(f.shape[0] for f in gen_frames_list)\n",
        "        gen_video = torch.stack([f[:n_frames] for f in gen_frames_list])\n",
        "        gen_video = gen_video.float() / 255.0 if gen_video.max() > 1 else gen_video.float()\n",
        "\n",
        "        audio_emb = extract_audio_embedding(\n",
        "            audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "        video_emb = extract_video_embedding(\n",
        "            video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "        metric.update(audio_emb, video_emb)\n",
        "\n",
        "agreement = metric.compute()\n",
        "print(f\"\\nBest model evaluation:\")\n",
        "print(f\"  Avg cosine sim:   {agreement['avg_cosine_sim']:.4f}\")\n",
        "print(f\"  Agreement rate:   {agreement['agreement_rate']:.4f}\")\n",
        "print(f\"  Std cosine sim:   {agreement['std_cosine_sim']:.4f}\")\n",
        "\n",
        "del preprocess_model, audio2coeff, animate, exp_net\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
