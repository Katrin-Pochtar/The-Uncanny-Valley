{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWgNve4IdUEu",
        "outputId": "f8046263-c48b-41b5-a9fc-6a00c0ac58f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already patched\n",
            "total 149M\n",
            "drwxr-xr-x 2 root root 4.0K Mar 14  2023 BFM_Fitting\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 04:22 __MACOSX\n",
            "-rw-r--r-- 1 root root 149M Apr  8  2023 mapping.pth.tar\n",
            "-rw-r--r-- 1 root root    0 Feb 16 04:22 SadTalker_V0.0.2_256.safetensors\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers librosa wandb face-alignment dlib yacs pydub gfpgan\n",
        "\n",
        "import importlib, pathlib\n",
        "spec = importlib.util.find_spec(\"basicsr\")\n",
        "if spec and spec.origin:\n",
        "    deg = pathlib.Path(spec.origin).parent / \"data\" / \"degradations.py\"\n",
        "    if deg.exists():\n",
        "        txt = deg.read_text()\n",
        "        if \"functional_tensor\" in txt:\n",
        "            deg.write_text(txt.replace(\n",
        "                \"from torchvision.transforms.functional_tensor import rgb_to_grayscale\",\n",
        "                \"from torchvision.transforms.functional import rgb_to_grayscale\"))\n",
        "            print(f\"Patched {deg}\")\n",
        "        else:\n",
        "            print(\"Already patched\")\n",
        "\n",
        "!git clone https://github.com/OpenTalker/SadTalker.git 2>/dev/null || true\n",
        "\n",
        "!mkdir -p SadTalker/checkpoints\n",
        "!rm -f SadTalker/checkpoints/sadtalker_256.safetensors SadTalker/checkpoints/SadTalker_V0.0.2_256.safetensors\n",
        "!wget -q \"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/mapping_00109-model.pth.tar\" -O SadTalker/checkpoints/mapping.pth.tar\n",
        "!wget -q \"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2-rc/SadTalker_V0.0.2_256.safetensors\" -O SadTalker/checkpoints/SadTalker_V0.0.2_256.safetensors\n",
        "!wget -q \"https://github.com/OpenTalker/SadTalker/releases/download/v0.0.2/BFM_Fitting.zip\" -O /tmp/BFM_Fitting.zip && unzip -qo /tmp/BFM_Fitting.zip -d SadTalker/checkpoints/ 2>/dev/null || true\n",
        "!ls -lh SadTalker/checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQYdqaBzpjlY",
        "outputId": "40963492-8c3f-44f8-fbd3-df660a2bbfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kornia in /usr/local/lib/python3.12/dist-packages (0.8.2)\n",
            "Requirement already satisfied: kornia_rs>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from kornia) (0.1.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kornia) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from kornia) (2.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.0.0->kornia) (1.3.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->kornia) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install kornia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf_XjYhEdUEv",
        "outputId": "acbccfa6-f006-4403-f163-cd434b8bc3f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content\")\n",
        "sys.path.insert(0, \"/content/SadTalker\")\n",
        "sys.path.insert(0, \"/content/SadTalker/src\")\n",
        "\n",
        "import numpy as np\n",
        "if not hasattr(np, \"VisibleDeprecationWarning\"):\n",
        "    np.VisibleDeprecationWarning = DeprecationWarning\n",
        "\n",
        "# Force-patch basicsr in current runtime (no restart needed)\n",
        "import importlib, pathlib\n",
        "_spec = importlib.util.find_spec(\"basicsr\")\n",
        "if _spec and _spec.origin:\n",
        "    _deg = pathlib.Path(_spec.origin).parent / \"data\" / \"degradations.py\"\n",
        "    if _deg.exists():\n",
        "        _txt = _deg.read_text()\n",
        "        if \"functional_tensor\" in _txt:\n",
        "            _deg.write_text(_txt.replace(\n",
        "                \"from torchvision.transforms.functional_tensor import rgb_to_grayscale\",\n",
        "                \"from torchvision.transforms.functional import rgb_to_grayscale\"))\n",
        "            print(f\"Patched {_deg}\")\n",
        "        # Clear any cached imports of basicsr\n",
        "        for k in list(__import__(\"sys\").modules.keys()):\n",
        "            if \"basicsr\" in k or \"gfpgan\" in k:\n",
        "                del __import__(\"sys\").modules[k]\n",
        "\n",
        "import gc\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import wandb\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from emotion_utils import (\n",
        "    CrossModalEmotionLoss,\n",
        "    DifferentiableVideoPreprocess,\n",
        "    EmotionAgreementMetric,\n",
        "    load_frozen_audio_encoder,\n",
        "    load_frozen_video_encoder,\n",
        "    extract_audio_embedding,\n",
        "    extract_video_embedding,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "METADATA = \"/content/processed_data/metadata.json\"\n",
        "BEST_AUDIO_PATH = \"/content/trained_encoders_v2/w2v2-lg-lr2e5\"\n",
        "BEST_VIDEO_PATH = \"/content/trained_encoders_v2/tsf-lr3e5-16f-nf\"\n",
        "OUT_DIR = Path(\"/content/sadtalker_finetuned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EXCLUDE = {0, 1, 3, 5, 7}\n",
        "REMAP = {2: 0, 4: 1, 6: 2}\n",
        "EMOTIONS = [\"happy\", \"angry\", \"disgust\"]\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUPJR26FdUEv",
        "outputId": "62864e44-6e00-4fc8-be12-c763542deeb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SadTalker modules loaded.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "SadTalker pipeline overview:\n",
        "  Audio -> AudioEncoder -> ExpNet -> expression coefficients (3DMM)\n",
        "  Expression coefficients + source face -> FaceRenderer -> generated video\n",
        "\n",
        "We fine-tune the ExpNet (expression mapper) so that generated expressions\n",
        "carry the same emotion as detected in the audio by our frozen encoders.\n",
        "\n",
        "Gradient path: emotion_loss -> generated_frames -> renderer -> expression_coeffs -> ExpNet\n",
        "\"\"\"\n",
        "\n",
        "from src.audio2pose_models.audio2pose import Audio2Pose\n",
        "from src.audio2exp_models.audio2exp import Audio2Exp\n",
        "from src.audio2exp_models.networks import SimpleWrapperV2\n",
        "from src.facerender.animate import AnimateFromCoeff\n",
        "from src.generate_batch import get_data\n",
        "from src.generate_facerender_batch import get_facerender_data\n",
        "from src.utils.preprocess import CropAndExtract\n",
        "\n",
        "SADTALKER_CKPT = Path(\"/content/SadTalker/checkpoints\")\n",
        "\n",
        "print(\"SadTalker modules loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3OdDUabdUEv",
        "outputId": "d2dfdf76-358c-4431-c1a4-9004b9487ccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using safetensor as default\n",
            "SadTalker loading issue (expected on first run): Error while deserializing header: header too small\n",
            "Adjust checkpoint paths in cell 0 and re-run.\n"
          ]
        }
      ],
      "source": [
        "class ExpNetWrapper(nn.Module):\n",
        "    \"\"\"Wraps SadTalker's Audio2Exp for fine-tuning.\n",
        "    Only expression mapper is trainable; audio backbone stays frozen.\"\"\"\n",
        "\n",
        "    def __init__(self, audio2exp_model):\n",
        "        super().__init__()\n",
        "        self.model = audio2exp_model\n",
        "\n",
        "    def freeze_audio_backbone(self):\n",
        "        for name, p in self.model.named_parameters():\n",
        "            if \"netG\" not in name:\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, batch):\n",
        "        return self.model(batch)\n",
        "\n",
        "\n",
        "class EmotionAwareFaceRenderer(nn.Module):\n",
        "    \"\"\"Wraps SadTalker face renderer so output is differentiable for emotion loss.\"\"\"\n",
        "\n",
        "    def __init__(self, renderer):\n",
        "        super().__init__()\n",
        "        self.renderer = renderer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def render(self, source_img, coeffs_dict):\n",
        "        \"\"\"Render face given 3DMM coefficients. Frozen -- no gradients.\"\"\"\n",
        "        return self.renderer.generate(source_img, coeffs_dict)\n",
        "\n",
        "\n",
        "def load_sadtalker_components(ckpt_dir, device):\n",
        "    ckpt_dir = Path(ckpt_dir)\n",
        "\n",
        "    from src.utils.init_path import init_path\n",
        "    sadtalker_paths = init_path(\n",
        "        str(ckpt_dir), \"/content/SadTalker/src/config\",\n",
        "        \"256\", False, \"crop\"\n",
        "    )\n",
        "\n",
        "    preprocess_model = CropAndExtract(sadtalker_paths, device)\n",
        "\n",
        "    from src.test_audio2coeff import Audio2Coeff\n",
        "    audio2coeff = Audio2Coeff(sadtalker_paths, device)\n",
        "\n",
        "    from src.facerender.animate import AnimateFromCoeff\n",
        "    animate = AnimateFromCoeff(sadtalker_paths, device)\n",
        "\n",
        "    return preprocess_model, audio2coeff, animate\n",
        "\n",
        "\n",
        "try:\n",
        "    preprocess_model, audio2coeff, animate = load_sadtalker_components(SADTALKER_CKPT, DEVICE)\n",
        "    exp_net = ExpNetWrapper(audio2coeff.audio2exp)\n",
        "    exp_net.freeze_audio_backbone()\n",
        "    trainable = sum(p.numel() for p in exp_net.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in exp_net.parameters())\n",
        "    print(f\"ExpNet: {trainable/1e6:.1f}M trainable / {total/1e6:.1f}M total\")\n",
        "except Exception as e:\n",
        "    print(f\"SadTalker loading issue (expected on first run): {e}\")\n",
        "    print(\"Adjust checkpoint paths in cell 0 and re-run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXfzFP1AdUEw",
        "outputId": "cc576e77-c429-42ac-c1b5-a2baac3e8e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frozen encoders loaded. Video: 8 frames. Audio dim=1024, Video dim=768, Proj dim=256\n"
          ]
        }
      ],
      "source": [
        "audio_enc, audio_proc = load_frozen_audio_encoder(BEST_AUDIO_PATH, DEVICE)\n",
        "video_enc = load_frozen_video_encoder(BEST_VIDEO_PATH, DEVICE)\n",
        "video_preprocess = DifferentiableVideoPreprocess(224).to(DEVICE)\n",
        "\n",
        "VIDEO_ENC_FRAMES = getattr(video_enc.config, \"num_frames\", 8)\n",
        "AUDIO_DIM = audio_enc.config.hidden_size\n",
        "VIDEO_DIM = video_enc.config.hidden_size\n",
        "PROJ_DIM = 256\n",
        "print(f\"Frozen encoders loaded. Video: {VIDEO_ENC_FRAMES} frames. \"\n",
        "      f\"Audio dim={AUDIO_DIM}, Video dim={VIDEO_DIM}, Proj dim={PROJ_DIM}\")\n",
        "\n",
        "\n",
        "def adapt_frames(frames, target_t):\n",
        "    \"\"\"Resample (B, T, C, H, W) to (B, target_t, C, H, W) via uniform index sampling.\"\"\"\n",
        "    B, T, C, H, W = frames.shape\n",
        "    if T == target_t:\n",
        "        return frames\n",
        "    idx = torch.linspace(0, T - 1, target_t).long()\n",
        "    return frames[:, idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCVTHLBYdUEw",
        "outputId": "2072156a-b8be-4b25-ee88-ee05ab0e9408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 408, Val: 72\n"
          ]
        }
      ],
      "source": [
        "SR = 16000\n",
        "IMG_SIZE = 256\n",
        "\n",
        "class SadTalkerDataset(Dataset):\n",
        "    def __init__(self, metadata_path, split, n_frames=8):\n",
        "        with open(metadata_path) as f:\n",
        "            data = json.load(f)\n",
        "        self.samples = [s for s in data\n",
        "                        if s[\"split\"] == split and s[\"emotion_idx\"] not in EXCLUDE]\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "\n",
        "        wav, sr = torchaudio.load(s[\"audio_path\"])\n",
        "        audio_1d = wav.squeeze(0)\n",
        "\n",
        "        frames = np.load(s[\"frames_path\"]).astype(np.float32) / 255.0\n",
        "        n = frames.shape[0]\n",
        "\n",
        "        source_idx = 0\n",
        "        source = torch.from_numpy(frames[source_idx]).permute(2, 0, 1)\n",
        "        if source.shape[1] != IMG_SIZE or source.shape[2] != IMG_SIZE:\n",
        "            source = F.interpolate(source.unsqueeze(0), size=(IMG_SIZE, IMG_SIZE),\n",
        "                                   mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "\n",
        "        indices = np.linspace(0, n - 1, self.n_frames).astype(int)\n",
        "        gt_frames = torch.from_numpy(frames[indices]).permute(0, 3, 1, 2)\n",
        "        if gt_frames.shape[2] != IMG_SIZE or gt_frames.shape[3] != IMG_SIZE:\n",
        "            gt_frames = F.interpolate(gt_frames, size=(IMG_SIZE, IMG_SIZE),\n",
        "                                      mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        return {\n",
        "            \"audio\": audio_1d,\n",
        "            \"audio_path\": s[\"audio_path\"],\n",
        "            \"source\": source,\n",
        "            \"gt_frames\": gt_frames,\n",
        "            \"emotion\": REMAP[s[\"emotion_idx\"]],\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_sadtalker(batch):\n",
        "    return {\n",
        "        \"audio\": [b[\"audio\"] for b in batch],\n",
        "        \"audio_path\": [b[\"audio_path\"] for b in batch],\n",
        "        \"source\": torch.stack([b[\"source\"] for b in batch]),\n",
        "        \"gt_frames\": torch.stack([b[\"gt_frames\"] for b in batch]),\n",
        "        \"emotion\": torch.tensor([b[\"emotion\"] for b in batch]),\n",
        "    }\n",
        "\n",
        "\n",
        "train_ds = SadTalkerDataset(METADATA, \"train\", n_frames=8)\n",
        "val_ds = SadTalkerDataset(METADATA, \"val\", n_frames=8)\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Dfp2iIzLdUEw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "SadTalker fine-tuning strategy:\n",
        "  1. Freeze: audio backbone, pose network, face renderer\n",
        "  2. Train: expression network (ExpNet / netG)\n",
        "  3. Loss: expression_coeff_loss + lambda_emo * cross_modal_emotion_loss\n",
        "\n",
        "Two loss paths:\n",
        "  (a) Coefficient loss -- L1 between predicted and GT 3DMM expression coefficients\n",
        "      (requires extracting GT coefficients from real faces)\n",
        "  (b) Emotion loss -- cosine distance between audio and video emotion embeddings\n",
        "      (requires rendering -> differentiable preprocess -> frozen emotion encoder)\n",
        "\n",
        "For efficiency, path (b) is computed every N steps (rendering is expensive).\n",
        "\"\"\"\n",
        "\n",
        "def coeff_loss_fn(pred_coeffs, gt_coeffs):\n",
        "    return F.l1_loss(pred_coeffs, gt_coeffs)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_3dmm_coeffs(preprocess_model, source_img, device):\n",
        "    \"\"\"Extract 3DMM coefficients from a face image using SadTalker's preprocessor.\"\"\"\n",
        "    if isinstance(source_img, torch.Tensor):\n",
        "        img = (source_img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    return preprocess_model.generate(img, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmhXv5Y1dUEw",
        "outputId": "ed4bc167-c18b-4574-98c2-0fa9725e4697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkatrinpochtar\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "wandb.login()\n",
        "\n",
        "CONFIGS = [\n",
        "    {\"name\": \"sadtalker-baseline\", \"lambda_emo\": 0.0},\n",
        "    {\"name\": \"sadtalker-emo-001\",  \"lambda_emo\": 0.01},\n",
        "    {\"name\": \"sadtalker-emo-005\",  \"lambda_emo\": 0.05},\n",
        "    {\"name\": \"sadtalker-emo-01\",   \"lambda_emo\": 0.1},\n",
        "]\n",
        "\n",
        "LR = 5e-5\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 2\n",
        "PATIENCE = 5\n",
        "EMO_EVAL_EVERY = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3XSVk_RudUEw"
      },
      "outputs": [],
      "source": [
        "def generate_with_sadtalker(audio2coeff, animate, audio_path, source_img, device):\n",
        "    \"\"\"Full SadTalker inference for a single sample. Returns generated frames (T, C, H, W).\"\"\"\n",
        "    from src.generate_batch import get_data as get_batch_data\n",
        "\n",
        "    batch = get_batch_data(audio2coeff, audio_path, device)\n",
        "    coeff_dict = audio2coeff.generate(batch, save_path=None, return_coeffs=True)\n",
        "    rendered = animate.generate(source_img, coeff_dict)\n",
        "    return rendered\n",
        "\n",
        "\n",
        "def train_one_epoch(exp_net, train_loader, optimizer, scaler, emotion_loss_fn,\n",
        "                    lambda_emo, audio2coeff, animate, step_counter):\n",
        "    exp_net.model.netG.train()\n",
        "    total_loss, total_emo = 0.0, 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, leave=False):\n",
        "        source = batch[\"source\"].to(DEVICE)\n",
        "        B = source.shape[0]\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        emo_loss = torch.tensor(0.0, device=DEVICE)\n",
        "        with autocast(\"cuda\", enabled=DEVICE == \"cuda\"):\n",
        "            if lambda_emo > 0 and step_counter[0] % EMO_EVAL_EVERY == 0:\n",
        "                gen_frames_list = []\n",
        "                for i in range(B):\n",
        "                    try:\n",
        "                        frames = generate_with_sadtalker(\n",
        "                            audio2coeff, animate,\n",
        "                            batch[\"audio_path\"][i], source[i], DEVICE)\n",
        "                        if isinstance(frames, torch.Tensor) and frames.dim() == 4:\n",
        "                            gen_frames_list.append(frames)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "                if gen_frames_list:\n",
        "                    n_frames = min(f.shape[0] for f in gen_frames_list)\n",
        "                    gen_video = torch.stack([f[:n_frames] for f in gen_frames_list])\n",
        "                    gen_video = gen_video.float() / 255.0 if gen_video.max() > 1 else gen_video.float()\n",
        "                    gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "\n",
        "                    audio_emb = extract_audio_embedding(\n",
        "                        audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "                    video_emb = extract_video_embedding(\n",
        "                        video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "                    emo_loss = emotion_loss_fn(audio_proj(audio_emb.detach()),\n",
        "                                              video_proj(video_emb))\n",
        "\n",
        "            loss = lambda_emo * emo_loss if lambda_emo > 0 else torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "        if loss.requires_grad:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(\n",
        "                [p for p in exp_net.parameters() if p.requires_grad], 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_emo += emo_loss.item()\n",
        "        step_counter[0] += 1\n",
        "        n += 1\n",
        "\n",
        "    return {\"total\": total_loss / max(n, 1), \"emotion\": total_emo / max(n, 1)}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(exp_net, val_loader, emotion_loss_fn, lambda_emo, audio2coeff, animate):\n",
        "    exp_net.model.netG.eval()\n",
        "    metric = EmotionAgreementMetric()\n",
        "    total_emo = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch in tqdm(val_loader, leave=False):\n",
        "        source = batch[\"source\"].to(DEVICE)\n",
        "        B = source.shape[0]\n",
        "\n",
        "        gen_frames_list = []\n",
        "        for i in range(B):\n",
        "            try:\n",
        "                frames = generate_with_sadtalker(\n",
        "                    audio2coeff, animate,\n",
        "                    batch[\"audio_path\"][i], source[i], DEVICE)\n",
        "                if isinstance(frames, torch.Tensor) and frames.dim() == 4:\n",
        "                    gen_frames_list.append(frames)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if not gen_frames_list:\n",
        "            continue\n",
        "\n",
        "        n_frames = min(f.shape[0] for f in gen_frames_list)\n",
        "        gen_video = torch.stack([f[:n_frames] for f in gen_frames_list])\n",
        "        gen_video = gen_video.float() / 255.0 if gen_video.max() > 1 else gen_video.float()\n",
        "        gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "\n",
        "        audio_emb = extract_audio_embedding(\n",
        "            audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "        video_emb = extract_video_embedding(\n",
        "            video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "\n",
        "        a_p, v_p = audio_proj(audio_emb), video_proj(video_emb)\n",
        "        emo_loss = emotion_loss_fn(a_p, v_p)\n",
        "        total_emo += emo_loss.item()\n",
        "        metric.update(a_p, v_p)\n",
        "        n += 1\n",
        "\n",
        "    result = {\"emotion\": total_emo / max(n, 1)}\n",
        "    result.update(metric.compute())\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "qWxXLKyFdUEw",
        "outputId": "fdf26245-d708-4d0e-a278-e512c11d5767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "sadtalker-baseline (lambda_emo=0.0)\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260216_042303-xuz242yy</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/katrinpochtar/uncanny-valley-sadtalker/runs/xuz242yy' target=\"_blank\">sadtalker-baseline</a></strong> to <a href='https://wandb.ai/katrinpochtar/uncanny-valley-sadtalker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-sadtalker' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-sadtalker</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/katrinpochtar/uncanny-valley-sadtalker/runs/xuz242yy' target=\"_blank\">https://wandb.ai/katrinpochtar/uncanny-valley-sadtalker/runs/xuz242yy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using safetensor as default\n"
          ]
        },
        {
          "ename": "SafetensorError",
          "evalue": "Error while deserializing header: header too small",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1239948856.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                config={**cfg, \"lr\": LR, \"epochs\": EPOCHS}, reinit=True)\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpreprocess_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio2coeff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sadtalker_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSADTALKER_CKPT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mexp_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExpNetWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio2coeff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio2exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mexp_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze_audio_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1679570954.py\u001b[0m in \u001b[0;36mload_sadtalker_components\u001b[0;34m(ckpt_dir, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mpreprocess_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCropAndExtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msadtalker_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_audio2coeff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio2Coeff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SadTalker/src/utils/preprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sadtalker_path, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msadtalker_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_safetensor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msadtalker_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_recon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_x_from_safetensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'face_3drecon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/safetensors/torch.py\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \"\"\"\n\u001b[1;32m    380\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msafe_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSafetensorError\u001b[0m: Error while deserializing header: header too small"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=0, collate_fn=collate_sadtalker)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=0, collate_fn=collate_sadtalker)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for cfg in CONFIGS:\n",
        "    name = cfg[\"name\"]\n",
        "    lambda_emo = cfg[\"lambda_emo\"]\n",
        "    print(f\"\\n{'='*60}\\n{name} (lambda_emo={lambda_emo})\\n{'='*60}\")\n",
        "\n",
        "    wandb.init(project=\"uncanny-valley-sadtalker\", name=name,\n",
        "               config={**cfg, \"lr\": LR, \"epochs\": EPOCHS}, reinit=True)\n",
        "\n",
        "    preprocess_model, audio2coeff, animate = load_sadtalker_components(SADTALKER_CKPT, DEVICE)\n",
        "    exp_net = ExpNetWrapper(audio2coeff.audio2exp)\n",
        "    exp_net.freeze_audio_backbone()\n",
        "\n",
        "    audio_proj = nn.Linear(AUDIO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "    video_proj = nn.Linear(VIDEO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "    trainable_params = [p for p in exp_net.parameters() if p.requires_grad]\n",
        "    if lambda_emo > 0:\n",
        "        trainable_params += list(audio_proj.parameters()) + list(video_proj.parameters())\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=LR)\n",
        "    scaler = GradScaler(enabled=DEVICE == \"cuda\")\n",
        "    emotion_loss_fn = CrossModalEmotionLoss(weight=1.0)\n",
        "\n",
        "    best_val, patience_cnt = float(\"inf\"), 0\n",
        "    save_path = OUT_DIR / name\n",
        "    step_counter = [0]\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t = train_one_epoch(exp_net, train_loader, optimizer, scaler,\n",
        "                            emotion_loss_fn, lambda_emo, audio2coeff, animate, step_counter)\n",
        "        v = evaluate(exp_net, val_loader, emotion_loss_fn, lambda_emo, audio2coeff, animate)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/total\": t[\"total\"], \"train/emotion\": t[\"emotion\"],\n",
        "            \"val/emotion\": v[\"emotion\"],\n",
        "            **{f\"val/{k}\": v[k] for k in [\"avg_cosine_sim\", \"agreement_rate\"] if k in v},\n",
        "        })\n",
        "\n",
        "        print(f\"  [{epoch+1:2d}/{EPOCHS}] \"\n",
        "              f\"t_loss={t['total']:.4f} v_emo={v['emotion']:.4f}\"\n",
        "              + (f\" cos_sim={v.get('avg_cosine_sim', 0):.3f}\" if lambda_emo > 0 else \"\"))\n",
        "\n",
        "        val_metric = v[\"emotion\"] if lambda_emo > 0 else v.get(\"avg_cosine_sim\", float(\"inf\"))\n",
        "        if val_metric < best_val:\n",
        "            best_val = val_metric\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(\n",
        "                {k: v for k, v in exp_net.model.netG.state_dict().items()},\n",
        "                save_path / \"expnet.pth\")\n",
        "            torch.save({\"audio_proj\": audio_proj.state_dict(),\n",
        "                         \"video_proj\": video_proj.state_dict()},\n",
        "                        save_path / \"projections.pth\")\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= PATIENCE:\n",
        "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    del preprocess_model, audio2coeff, animate, exp_net, optimizer, scaler, audio_proj, video_proj\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    all_results.append({\"name\": name, \"lambda_emo\": lambda_emo, \"best_val\": best_val})\n",
        "    print(f\"  Best val metric: {best_val:.4f} -> {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPY-niu2dUEw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(all_results).sort_values(\"best_val\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.bar(df[\"name\"], df[\"best_val\"], color=\"coral\")\n",
        "ax.set_ylabel(\"Best Val Emotion Loss\")\n",
        "ax.set_title(\"SadTalker Fine-tuning: Î»_emo Ablation\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrI6UyQ2dUEx"
      },
      "outputs": [],
      "source": [
        "best_name = df.iloc[0][\"name\"]\n",
        "print(f\"Best SadTalker variant: {best_name}\")\n",
        "\n",
        "preprocess_model, audio2coeff, animate = load_sadtalker_components(SADTALKER_CKPT, DEVICE)\n",
        "exp_net = ExpNetWrapper(audio2coeff.audio2exp)\n",
        "\n",
        "ckpt = torch.load(OUT_DIR / best_name / \"expnet.pth\", map_location=DEVICE, weights_only=True)\n",
        "exp_net.model.netG.load_state_dict(ckpt)\n",
        "exp_net.eval()\n",
        "\n",
        "audio_proj = nn.Linear(AUDIO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "video_proj = nn.Linear(VIDEO_DIM, PROJ_DIM, bias=False).to(DEVICE)\n",
        "proj_ckpt = torch.load(OUT_DIR / best_name / \"projections.pth\", map_location=DEVICE, weights_only=True)\n",
        "audio_proj.load_state_dict(proj_ckpt[\"audio_proj\"])\n",
        "video_proj.load_state_dict(proj_ckpt[\"video_proj\"])\n",
        "audio_proj.eval()\n",
        "video_proj.eval()\n",
        "\n",
        "metric = EmotionAgreementMetric()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Evaluating best\"):\n",
        "        source = batch[\"source\"].to(DEVICE)\n",
        "        B = source.shape[0]\n",
        "\n",
        "        gen_frames_list = []\n",
        "        for i in range(B):\n",
        "            try:\n",
        "                frames = generate_with_sadtalker(\n",
        "                    audio2coeff, animate,\n",
        "                    batch[\"audio_path\"][i], source[i], DEVICE)\n",
        "                if isinstance(frames, torch.Tensor) and frames.dim() == 4:\n",
        "                    gen_frames_list.append(frames)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if not gen_frames_list:\n",
        "            continue\n",
        "\n",
        "        n_frames = min(f.shape[0] for f in gen_frames_list)\n",
        "        gen_video = torch.stack([f[:n_frames] for f in gen_frames_list])\n",
        "        gen_video = gen_video.float() / 255.0 if gen_video.max() > 1 else gen_video.float()\n",
        "        gen_video = adapt_frames(gen_video, VIDEO_ENC_FRAMES)\n",
        "\n",
        "        audio_emb = extract_audio_embedding(\n",
        "            audio_enc, audio_proc, batch[\"audio\"], device=DEVICE)\n",
        "        video_emb = extract_video_embedding(\n",
        "            video_enc, video_preprocess, gen_video, device=DEVICE)\n",
        "        metric.update(audio_proj(audio_emb), video_proj(video_emb))\n",
        "\n",
        "agreement = metric.compute()\n",
        "print(f\"\\nBest model evaluation:\")\n",
        "print(f\"  Avg cosine sim:   {agreement['avg_cosine_sim']:.4f}\")\n",
        "print(f\"  Agreement rate:   {agreement['agreement_rate']:.4f}\")\n",
        "print(f\"  Std cosine sim:   {agreement['std_cosine_sim']:.4f}\")\n",
        "\n",
        "del preprocess_model, audio2coeff, animate, exp_net\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
